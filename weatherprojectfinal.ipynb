{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    Weather Forecast Model\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    Ⅰ.Based on the weather of the past ten days, predict the highest and lowest temperatures for the 11th day\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       date  cloud_cover  sunshine  global_radiation  max_temp  mean_temp  \\\n",
      "0  19790101          2.0       7.0              52.0       2.3       -4.1   \n",
      "1  19790102          6.0       1.7              27.0       1.6       -2.6   \n",
      "2  19790103          5.0       0.0              13.0       1.3       -2.8   \n",
      "3  19790104          8.0       0.0              13.0      -0.3       -2.6   \n",
      "4  19790105          6.0       2.0              29.0       5.6       -0.8   \n",
      "\n",
      "   min_temp  precipitation  pressure  snow_depth  \n",
      "0      -7.5            0.4  101900.0         9.0  \n",
      "1      -7.5            0.0  102530.0         8.0  \n",
      "2      -7.2            0.0  102050.0         4.0  \n",
      "3      -6.5            0.0  100840.0         2.0  \n",
      "4      -1.4            0.0  102250.0         1.0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_weather = pd.read_csv('data_weather.csv')\n",
    "print(df_weather.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\duplicate date:\n",
      "Empty DataFrame\n",
      "Columns: [date, cloud_cover, sunshine, global_radiation, max_temp, mean_temp, min_temp, precipitation, pressure, snow_depth]\n",
      "Index: []\n",
      "\\The data that precipitation is negative\n",
      "Empty DataFrame\n",
      "Columns: [date, cloud_cover, sunshine, global_radiation, max_temp, mean_temp, min_temp, precipitation, pressure, snow_depth]\n",
      "Index: []\n",
      "\\The data that snow depth is negative\n",
      "Empty DataFrame\n",
      "Columns: [date, cloud_cover, sunshine, global_radiation, max_temp, mean_temp, min_temp, precipitation, pressure, snow_depth]\n",
      "Index: []\n",
      "特征的3倍标准差阈值:\n",
      "cloud_cover: 3.0000977819788064\n",
      "sunshine: 3.0000977819788073\n",
      "global_radiation: 3.0000977819788073\n",
      "max_temp: 3.0000977819788073\n",
      "mean_temp: 3.000097781978808\n",
      "min_temp: 3.0000977819788073\n",
      "precipitation: 3.0000977819788073\n",
      "pressure: 3.000097781978808\n",
      "snow_depth: 3.000097781978808\n",
      "diff_temp: 3.0000977819788073\n",
      "       date  cloud_cover  sunshine  global_radiation  max_temp  mean_temp  \\\n",
      "0  19790101    -1.579618  0.657802         -0.752136 -1.996772  -2.718962   \n",
      "1  19790102     0.353314 -0.657920         -1.033264 -2.103549  -2.457114   \n",
      "2  19790103    -0.129919 -1.079944         -1.190696 -2.149310  -2.492027   \n",
      "\n",
      "   min_temp  precipitation  pressure    snow_depth  diff_temp  \n",
      "0 -2.827491      -0.339317  0.346207  5.557990e-18   0.512994  \n",
      "1 -2.827491      -0.446330  0.946409  5.557990e-18   0.330668  \n",
      "2 -2.771166      -0.446330  0.489112  5.557990e-18   0.174389  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Use linear interpolation to fill in missing values\n",
    "df_weather['date'] = df_weather['date'].interpolate(method='linear')\n",
    "df_weather['cloud_cover'] = df_weather['cloud_cover'].interpolate(method='linear')\n",
    "df_weather['sunshine'] = df_weather['sunshine'].interpolate(method='linear')\n",
    "df_weather['global_radiation'] = df_weather['global_radiation'].interpolate(method='linear')\n",
    "df_weather['max_temp'] = df_weather['max_temp'].interpolate(method='linear')\n",
    "df_weather['mean_temp'] = df_weather['mean_temp'].interpolate(method='linear')\n",
    "df_weather['min_temp'] = df_weather['min_temp'].interpolate(method='linear')\n",
    "df_weather['precipitation'] = df_weather['precipitation'].interpolate(method='linear')\n",
    "df_weather['pressure'] = df_weather['pressure'].interpolate(method='linear')\n",
    "df_weather['snow_depth'] = df_weather['snow_depth'].interpolate(method='linear')\n",
    "\n",
    "# Checking duplicate records - None\n",
    "duplicates = df_weather[df_weather.duplicated(subset=['date'])]\n",
    "print('\\duplicate date:')\n",
    "print(duplicates)\n",
    "\n",
    "# View and delete data with negative precipitation or snowfall - None\n",
    "negative_precipitation = df_weather[df_weather['precipitation']< 0]\n",
    "negative_snow_depth = df_weather[df_weather['snow_depth']< 0]\n",
    "print('\\The data that precipitation is negative')\n",
    "print(negative_precipitation)\n",
    "print('\\The data that snow depth is negative')\n",
    "print(negative_snow_depth)\n",
    "\n",
    "# Insert new column \"Temperature Difference\"\n",
    "df_weather['diff_temp'] = df_weather['max_temp'] - df_weather['min_temp']\n",
    "df_weather = df_weather\n",
    "\n",
    "# Z-score\n",
    "features = ['cloud_cover', 'sunshine', 'global_radiation', 'max_temp',\n",
    "             'mean_temp', 'min_temp','precipitation', 'pressure', \n",
    "             'snow_depth','diff_temp']\n",
    "scaler = StandardScaler()\n",
    "df_weather[features] = scaler.fit_transform(df_weather[features])  \n",
    "\n",
    "\n",
    "for feature in features:\n",
    "     mean = scaler.mean_[features.index(feature)]  \n",
    "     std = scaler.scale_[features.index(feature)]  \n",
    "     df_weather[feature + '_zscore'] = (df_weather[feature] - mean) / std \n",
    "\n",
    "\n",
    "thresholds = {}\n",
    "for feature in features:\n",
    "     std_3x = df_weather[feature].std() * 3\n",
    "     thresholds[feature] = std_3x\n",
    "\n",
    "print(\"特征的3倍标准差阈值:\")\n",
    "for feature, threshold in thresholds.items():\n",
    "     print(f\"{feature}: {threshold}\")\n",
    "\n",
    "for feature in features:\n",
    "     mean_value = df_weather[feature].mean()\n",
    "     df_weather.loc[df_weather[feature] > thresholds[feature], feature] = mean_value\n",
    "\n",
    "\n",
    "df_weather.drop(columns=[feature + '_zscore' for feature in features], inplace=True)\n",
    "\n",
    "print(df_weather.head(3)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date                0.089202\n",
      "cloud_cover         0.122408\n",
      "sunshine            0.337460\n",
      "global_radiation    0.593108\n",
      "max_temp            0.907082\n",
      "mean_temp           0.934198\n",
      "min_temp            0.907082\n",
      "precipitation       0.076482\n",
      "pressure            0.086690\n",
      "snow_depth          0.136514\n",
      "diff_temp           0.282762\n",
      "dtype: float64\n",
      "selected features based on combined correlation: \n",
      " ['global_radiation', 'max_temp', 'mean_temp', 'min_temp']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "pearson_corr_max_temp = df_weather.corr()['max_temp']\n",
    "pearson_corr_min_temp = df_weather.corr()['min_temp']\n",
    "\n",
    "\n",
    "spearman_corr_max_temp = df_weather.corr(method='spearman')['max_temp']\n",
    "spearman_corr_min_temp = df_weather.corr(method='spearman')['min_temp']\n",
    "\n",
    "# Consider the two coefficients together\n",
    "combined_corr = (pearson_corr_max_temp.abs() + pearson_corr_min_temp.abs()\n",
    "                 +spearman_corr_max_temp.abs() + spearman_corr_min_temp.abs()) /4\n",
    "\n",
    "print(combined_corr)\n",
    "\n",
    "threshold = 0.4\n",
    "selected_features = combined_corr[combined_corr > threshold].index.tolist()\n",
    "\n",
    "print('selected features based on combined correlation: \\n', selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Divide the data set by time sequence\n",
    "\n",
    "def split_data(x, y, test_size):\n",
    "    if not 0 <= test_size <= 1:\n",
    "        raise ValueError(\"test_size must be between 0 and 1.\")\n",
    "\n",
    "    \n",
    "    num_samples = len(x)\n",
    "    num_test_samples = int(num_samples * test_size)\n",
    "\n",
    "    # Divide the data set sequentially\n",
    "    x_train = x.iloc[:-num_test_samples]\n",
    "    x_test = x.iloc[-num_test_samples:]\n",
    "    y_train = y.iloc[:-num_test_samples]\n",
    "    y_test = y.iloc[-num_test_samples:]\n",
    "\n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the training set:  (9205, 4)\n",
      "Size of the validation set:  (3068, 4)\n",
      "Size of the testing set:  (3068, 4)\n",
      "Training set: \n",
      "       date  global_radiation  diff_temp  mean_temp  max_temp  min_temp\n",
      "0  20120808          0.529810   0.981831   1.348412  1.664123  1.340546\n",
      "1  20120809          1.631833   1.528808   1.453151  1.755645  1.058922\n",
      "2  20120810          1.519382   0.382761   1.645173  1.328541  1.359321\n",
      "3  20120811          1.440666   1.138111   1.383325  1.755645  1.340546\n",
      "4  20120812          1.103312   0.174389   1.680086  1.282779  1.453196\n",
      "\n",
      "Validation set:\n",
      "       date  global_radiation  diff_temp  mean_temp  max_temp  min_temp\n",
      "0  20040315         -0.785871   0.200436  -0.083024  0.306541  0.232825\n",
      "1  20040316          0.158720  -1.023751   0.458129 -0.105310  0.608324\n",
      "2  20040317         -0.684665  -0.216309  -0.117937 -0.227340 -0.123899\n",
      "3  20040318         -0.369801  -0.659100  -0.135394 -0.395131 -0.011250\n",
      "4  20040319         -0.819607  -0.268402  -0.222676 -0.166325 -0.011250\n",
      "\n",
      "Test set:\n",
      "       date  global_radiation  diff_temp  mean_temp  max_temp  min_temp\n",
      "0  20120808          0.529810   0.981831   1.348412  1.664123  1.340546\n",
      "1  20120809          1.631833   1.528808   1.453151  1.755645  1.058922\n",
      "2  20120810          1.519382   0.382761   1.645173  1.328541  1.359321\n",
      "3  20120811          1.440666   1.138111   1.383325  1.755645  1.340546\n",
      "4  20120812          1.103312   0.174389   1.680086  1.282779  1.453196\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df_weather[['date', 'global_radiation','diff_temp','mean_temp']]  \n",
    "y = df_weather[['max_temp', 'min_temp']]       \n",
    "\n",
    "\n",
    "X_train_val, X_test, y_train_val, y_test = split_data(X, y, test_size=0.2)\n",
    "X_train, X_val, y_train, y_val = split_data(X_train_val, y_train_val, test_size=0.25)\n",
    "\n",
    "\n",
    "print(\"Size of the training set: \", X_train.shape)\n",
    "print(\"Size of the validation set: \", X_val.shape)\n",
    "print(\"Size of the testing set: \", X_test.shape)\n",
    "\n",
    "\n",
    "train_data = pd.concat([X_train, y_train], axis=1)\n",
    "val_data = pd.concat([X_val, y_val], axis=1)\n",
    "test_data = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "train_data.to_csv('train.csv', index=False)\n",
    "val_data.to_csv('val.csv', index=False)\n",
    "test_data.to_csv('test.csv', index=False)\n",
    "\n",
    "\n",
    "train_df_weather = pd.read_csv('train.csv')\n",
    "val_df_weather = pd.read_csv('val.csv')\n",
    "test_df_weather = pd.read_csv('test.csv')\n",
    "\n",
    "print('Training set: ')\n",
    "print(test_df_weather.head())\n",
    "print('\\nValidation set:')\n",
    "print(val_df_weather.head())\n",
    "print('\\nTest set:')\n",
    "print(test_df_weather.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Baseline model - Moving average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(data, window_size):\n",
    "    averages = []\n",
    "    for i in range(len(data) - window_size + 1):\n",
    "        window = data[i:i + window_size]\n",
    "        average = np.mean(window)\n",
    "        averages.append(average)\n",
    "    return np.array(averages)\n",
    "\n",
    "window_size = 10\n",
    "\n",
    "# Calculate the moving average for each feature in the training set\n",
    "X_train_ma = np.zeros((X_train.shape[0] - window_size + 1, X_train.shape[1]))\n",
    "for i in range(X_train.shape[1]):\n",
    "    X_train_ma[:, i] = moving_average(X_train.iloc[:, i], window_size)\n",
    "\n",
    "y_train_max_ma = moving_average(y_train['max_temp'].values, window_size)\n",
    "y_train_min_ma = moving_average(y_train['min_temp'].values, window_size)\n",
    "\n",
    "# create model\n",
    "class MovingAverageModel:\n",
    "    def __init__(self):\n",
    "        self.window_size = None\n",
    "        self.coefficients = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.window_size = X.shape[1]\n",
    "        self.coefficients = np.linalg.lstsq(X, y, rcond=None)[0]\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.dot(X, self.coefficients)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and train model\n",
    "model_max_temp = MovingAverageModel()\n",
    "model_min_temp = MovingAverageModel()\n",
    "model_max_temp.fit(X_train_ma, y_train_max_ma)\n",
    "model_min_temp.fit(X_train_ma, y_train_min_ma)\n",
    "\n",
    "# evaluate model\n",
    "X_val_ma = np.zeros((X_val.shape[0] - window_size + 1, X_val.shape[1]))\n",
    "for i in range(X_val.shape[1]):\n",
    "    X_val_ma[:, i] = moving_average(X_val.iloc[:, i], window_size)\n",
    "\n",
    "y_val_pred_max = model_max_temp.predict(X_val_ma)\n",
    "y_val_pred_min = model_min_temp.predict(X_val_ma)\n",
    "\n",
    "# predict\n",
    "X_test_ma = np.zeros((X_test.shape[0] - window_size + 1, X_test.shape[1]))\n",
    "for i in range(X_test.shape[1]):\n",
    "    X_test_ma[:, i] = moving_average(X_test.iloc[:, i], window_size)\n",
    "\n",
    "y_test_pred_max_ma = model_max_temp.predict(X_test_ma)\n",
    "y_test_pred_min_ma = model_min_temp.predict(X_test_ma)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation results:\n",
      "Average MSE of 'max_temp': 0.00\n",
      "Average MSE of 'min_temp': 0.00\n",
      "Average RMSE of 'max_temp': 0.04\n",
      "Average RMSE of 'min_temp': 0.04\n",
      "Average MAE of 'max_temp': 0.03\n",
      "Average MAE of 'min_temp': 0.03\n",
      "Average R² of 'max_temp': 1.00\n",
      "Average R² of 'min_temp': 1.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Initializes the result store\n",
    "mse_scores_max = []\n",
    "mse_scores_min = []\n",
    "rmse_scores_max = []\n",
    "rmse_scores_min = []\n",
    "mae_scores_max = []\n",
    "mae_scores_min = []\n",
    "r2_scores_max = []\n",
    "r2_scores_min = []\n",
    "X_train = X_train.to_numpy()\n",
    "y_train = y_train.to_numpy()\n",
    "# TimeSeriesSplit\n",
    "for train_index, val_index in tscv.split(X_train):\n",
    "   \n",
    "    X_train_cv, X_val_cv = X_train[train_index], X_train[val_index]\n",
    "    y_train_max_cv, y_val_max_cv = y_train[train_index, 0], y_train[val_index, 0]\n",
    "    y_train_min_cv, y_val_min_cv = y_train[train_index, 1], y_train[val_index, 1]\n",
    "    \n",
    "   \n",
    "    X_train_ma = np.zeros((X_train_cv.shape[0] - window_size + 1, X_train_cv.shape[1]))\n",
    "    X_val_ma = np.zeros((X_val_cv.shape[0] - window_size + 1, X_val_cv.shape[1]))\n",
    "    for i in range(X_train_cv.shape[1]):\n",
    "        X_train_ma[:, i] = moving_average(X_train_cv[:, i], window_size)\n",
    "        X_val_ma[:, i] = moving_average(X_val_cv[:, i], window_size)\n",
    "    \n",
    "    y_train_max_ma = moving_average(y_train_max_cv, window_size)\n",
    "    y_train_min_ma = moving_average(y_train_min_cv, window_size)\n",
    "    \n",
    "    # Create the corresponding moving average for the validation set label\n",
    "    y_val_max_ma = moving_average(y_val_max_cv, window_size)\n",
    "    y_val_min_ma = moving_average(y_val_min_cv, window_size)\n",
    "    \n",
    "    # train\n",
    "    model_max_temp = MovingAverageModel()\n",
    "    model_min_temp = MovingAverageModel()\n",
    "    model_max_temp.fit(X_train_ma, y_train_max_ma)\n",
    "    model_min_temp.fit(X_train_ma, y_train_min_ma)\n",
    "    \n",
    "    # predict\n",
    "    y_val_pred_max = model_max_temp.predict(X_val_ma)\n",
    "    y_val_pred_min = model_min_temp.predict(X_val_ma)\n",
    "    \n",
    "    mse_max = mean_squared_error(y_val_max_ma, y_val_pred_max)\n",
    "    mse_min = mean_squared_error(y_val_min_ma, y_val_pred_min)\n",
    "    rmse_max = np.sqrt(mse_max)\n",
    "    rmse_min = np.sqrt(mse_min)\n",
    "    mae_max = mean_absolute_error(y_val_max_ma, y_val_pred_max)\n",
    "    mae_min = mean_absolute_error(y_val_min_ma, y_val_pred_min)\n",
    "    r2_max = r2_score(y_val_max_ma, y_val_pred_max)\n",
    "    r2_min = r2_score(y_val_min_ma, y_val_pred_min)\n",
    "    \n",
    "    mse_scores_max.append(mse_max)\n",
    "    mse_scores_min.append(mse_min)\n",
    "    rmse_scores_max.append(rmse_max)\n",
    "    rmse_scores_min.append(rmse_min)\n",
    "    mae_scores_max.append(mae_max)\n",
    "    mae_scores_min.append(mae_min)\n",
    "    r2_scores_max.append(r2_max)\n",
    "    r2_scores_min.append(r2_min)\n",
    "\n",
    "\n",
    "print(\"Cross-validation results:\")\n",
    "print(f\"Average MSE of 'max_temp': {np.mean(mse_scores_max):.2f}\")\n",
    "print(f\"Average MSE of 'min_temp': {np.mean(mse_scores_min):.2f}\")\n",
    "print(f\"Average RMSE of 'max_temp': {np.mean(rmse_scores_max):.2f}\")\n",
    "print(f\"Average RMSE of 'min_temp': {np.mean(rmse_scores_min):.2f}\")\n",
    "print(f\"Average MAE of 'max_temp': {np.mean(mae_scores_max):.2f}\")\n",
    "print(f\"Average MAE of 'min_temp': {np.mean(mae_scores_min):.2f}\")\n",
    "print(f\"Average R² of 'max_temp': {np.mean(r2_scores_max):.2f}\")\n",
    "print(f\"Average R² of 'min_temp': {np.mean(r2_scores_min):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE of 'max_temp' in test set: 0.46\n",
      "RMSE of 'min_temp' in test set: 0.52\n",
      "MSE of 'max_temp' in test set: 0.21\n",
      "MSE of 'min_temp' in test set: 0.27\n",
      "MAE of 'max_temp' in test set: 0.36\n",
      "MAE of 'min_temp' in test set: 0.41\n",
      "R² of 'max_temp' in test set: 0.79\n",
      "R² of 'min_temp' in test set: 0.72\n"
     ]
    }
   ],
   "source": [
    "# evaluate\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# RMSE\n",
    "mse_max_ma = mean_squared_error(y_test['max_temp'].values[window_size - 1:], y_test_pred_max_ma)\n",
    "mse_min_ma = mean_squared_error(y_test['min_temp'].values[window_size - 1:], y_test_pred_min_ma)\n",
    "\n",
    "# MSE\n",
    "mse_max_ma = mean_squared_error(y_test['max_temp'].values[window_size - 1:], y_test_pred_max_ma)\n",
    "mse_min_ma = mean_squared_error(y_test['min_temp'].values[window_size - 1:], y_test_pred_min_ma)\n",
    "\n",
    "# MAE\n",
    "mae_max_ma = mean_absolute_error(y_test['max_temp'].values[window_size - 1:], y_test_pred_max_ma)\n",
    "mae_min_ma = mean_absolute_error(y_test['min_temp'].values[window_size - 1:], y_test_pred_min_ma)\n",
    "\n",
    "# R2\n",
    "r2_max_ma = r2_score(y_test['max_temp'].values[window_size - 1:], y_test_pred_max_ma)\n",
    "r2_min_ma = r2_score(y_test['min_temp'].values[window_size - 1:], y_test_pred_min_ma)\n",
    "\n",
    "print(\"RMSE of 'max_temp' in test set: {:.2f}\".format(np.sqrt(mse_max_ma)))\n",
    "print(\"RMSE of 'min_temp' in test set: {:.2f}\".format(np.sqrt(mse_min_ma)))\n",
    "\n",
    "print(\"MSE of 'max_temp' in test set: {:.2f}\".format(mse_max_ma))\n",
    "print(\"MSE of 'min_temp' in test set: {:.2f}\".format(mse_min_ma))\n",
    "\n",
    "print(\"MAE of 'max_temp' in test set: {:.2f}\".format(mae_max_ma))\n",
    "print(\"MAE of 'min_temp' in test set: {:.2f}\".format(mae_min_ma))\n",
    "\n",
    "print(\"R² of 'max_temp' in test set: {:.2f}\".format(r2_max_ma))\n",
    "print(\"R² of 'min_temp' in test set: {:.2f}\".format(r2_min_ma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def create_features_and_target(df, n_days=10):\n",
    "    df = df.copy()\n",
    "    for i in range(1, n_days + 1):\n",
    "        df[f'global_radiation_{i}d'] = df['global_radiation'].shift(i)\n",
    "        df[f'diff_temp_{i}d'] = df['diff_temp'].shift(i)\n",
    "        df[f'mean_temp_{i}d'] = df['mean_temp'].shift(i)\n",
    "    \n",
    "    df = df.dropna().reset_index(drop=True)\n",
    "    X = df[[f'global_radiation_{i}d' for i in range(1, n_days + 1)] +\n",
    "           [f'diff_temp_{i}d' for i in range(1, n_days + 1)] +\n",
    "           [f'mean_temp_{i}d' for i in range(1, n_days + 1)]]\n",
    "    y_max_rf = df['max_temp']\n",
    "    y_min_rf = df['min_temp']\n",
    "    return X, y_max_rf, y_min_rf\n",
    "\n",
    "\n",
    "X_train_rf, y_train_max_rf, y_train_min_rf = create_features_and_target(train_df_weather)\n",
    "X_val_rf, y_val_max_rf, y_val_min_rf = create_features_and_target(val_df_weather)\n",
    "X_test_rf, y_test_max_rf, y_test_min_rf = create_features_and_target(test_df_weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and train\n",
    "rf_max = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_max.fit(X_train_rf, y_train_max_rf)\n",
    "\n",
    "rf_min = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_min.fit(X_train_rf, y_train_min_rf)\n",
    "\n",
    "# pred\n",
    "y_val_pred_max_rf = rf_max.predict(X_val_rf)\n",
    "y_val_pred_min_rf = rf_min.predict(X_val_rf)\n",
    "\n",
    "# pred\n",
    "y_test_pred_max_rf = rf_max.predict(X_test_rf)\n",
    "y_test_pred_min_rf = rf_min.predict(X_test_rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation results:\n",
      "Average MSE of 'max_temp': 0.15\n",
      "Average MSE of 'min_temp': 0.14\n",
      "Average RMSE of 'max_temp': 0.39\n",
      "Average RMSE of 'min_temp': 0.37\n",
      "Average MAE of 'max_temp': 0.30\n",
      "Average MAE of 'min_temp': 0.30\n",
      "Average R² of 'max_temp': 0.85\n",
      "Average R² of 'min_temp': 0.86\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# \n",
    "X, y_max, y_min = create_features_and_target(train_df_weather)\n",
    "\n",
    "# 5 fold\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "mse_scores_max = []\n",
    "mse_scores_min = []\n",
    "rmse_scores_max = []\n",
    "rmse_scores_min = []\n",
    "mae_scores_max = []\n",
    "mae_scores_min = []\n",
    "r2_scores_max = []\n",
    "r2_scores_min = []\n",
    "\n",
    "# initialize K-fold\n",
    "for train_index, val_index in kf.split(X):\n",
    "    X_train_cv, X_val_cv = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_train_max_cv, y_val_max_cv = y_max.iloc[train_index], y_max.iloc[val_index]\n",
    "    y_train_min_cv, y_val_min_cv = y_min.iloc[train_index], y_min.iloc[val_index]\n",
    "    \n",
    "  \n",
    "    rf_max = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    rf_max.fit(X_train_cv, y_train_max_cv)\n",
    "\n",
    "    rf_min = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    rf_min.fit(X_train_cv, y_train_min_cv)\n",
    "\n",
    "\n",
    "    y_val_pred_max_rf = rf_max.predict(X_val_cv)\n",
    "    y_val_pred_min_rf = rf_min.predict(X_val_cv)\n",
    "\n",
    "\n",
    "    mse_max = mean_squared_error(y_val_max_cv, y_val_pred_max_rf)\n",
    "    mse_min = mean_squared_error(y_val_min_cv, y_val_pred_min_rf)\n",
    "    rmse_max = np.sqrt(mse_max)\n",
    "    rmse_min = np.sqrt(mse_min)\n",
    "    \n",
    "    mse_scores_max.append(mse_max)\n",
    "    mse_scores_min.append(mse_min)\n",
    "    rmse_scores_max.append(rmse_max)\n",
    "    rmse_scores_min.append(rmse_min)\n",
    "    mae_scores_max.append(mean_absolute_error(y_val_max_cv, y_val_pred_max_rf))\n",
    "    mae_scores_min.append(mean_absolute_error(y_val_min_cv, y_val_pred_min_rf))\n",
    "    r2_scores_max.append(r2_score(y_val_max_cv, y_val_pred_max_rf))\n",
    "    r2_scores_min.append(r2_score(y_val_min_cv, y_val_pred_min_rf))\n",
    "\n",
    "\n",
    "print(\"Cross-validation results:\")\n",
    "print(f\"Average MSE of 'max_temp': {np.mean(mse_scores_max):.2f}\")\n",
    "print(f\"Average MSE of 'min_temp': {np.mean(mse_scores_min):.2f}\")\n",
    "print(f\"Average RMSE of 'max_temp': {np.mean(rmse_scores_max):.2f}\")\n",
    "print(f\"Average RMSE of 'min_temp': {np.mean(rmse_scores_min):.2f}\")\n",
    "print(f\"Average MAE of 'max_temp': {np.mean(mae_scores_max):.2f}\")\n",
    "print(f\"Average MAE of 'min_temp': {np.mean(mae_scores_min):.2f}\")\n",
    "print(f\"Average R² of 'max_temp': {np.mean(r2_scores_max):.2f}\")\n",
    "print(f\"Average R² of 'min_temp': {np.mean(r2_scores_min):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE of 'max_temp' in test set: 0.4041829141188286\n",
      "RMSE of 'min_temp' in test set: 0.39214930819146954\n",
      "MSE of 'max_temp' in test set: 0.1633638280655884\n",
      "MSE of 'min_temp' in test set: 0.15378107991504816\n",
      "MAE of 'max_temp' in test set: 0.3105157689668827\n",
      "MAE of 'min_temp' in test set: 0.3153303253808187\n",
      "R² of 'max_temp' in test set: 0.8352520531486846\n",
      "R² of 'min_temp' in test set: 0.837707867413086\n"
     ]
    }
   ],
   "source": [
    "# evaluate\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "# RMSE)\n",
    "rmse_max_rf = np.sqrt(mean_squared_error(y_test_max_rf, y_test_pred_max_rf))\n",
    "rmse_min_rf = np.sqrt(mean_squared_error(y_test_min_rf, y_test_pred_min_rf))\n",
    "\n",
    "# MSE\n",
    "mse_max_rf = mean_squared_error(y_test_max_rf, y_test_pred_max_rf)\n",
    "mse_min_rf = mean_squared_error(y_test_min_rf, y_test_pred_min_rf)\n",
    "\n",
    "# MAE\n",
    "mae_max_rf = mean_absolute_error(y_test_max_rf, y_test_pred_max_rf)\n",
    "mae_min_rf = mean_absolute_error(y_test_min_rf, y_test_pred_min_rf)\n",
    "\n",
    "# R²\n",
    "r2_max_rf = r2_score(y_test_max_rf, y_test_pred_max_rf)\n",
    "r2_min_rf = r2_score(y_test_min_rf, y_test_pred_min_rf)\n",
    "\n",
    "print(f\"RMSE of 'max_temp' in test set: {rmse_max_rf}\")\n",
    "print(f\"RMSE of 'min_temp' in test set: {rmse_min_rf}\")\n",
    "\n",
    "print(f\"MSE of 'max_temp' in test set: {mse_max_rf}\")\n",
    "print(f\"MSE of 'min_temp' in test set: {mse_min_rf}\")\n",
    "\n",
    "print(f\"MAE of 'max_temp' in test set: {mae_max_rf}\")\n",
    "print(f\"MAE of 'min_temp' in test set: {mae_min_rf}\")\n",
    "\n",
    "print(f\"R² of 'max_temp' in test set: {r2_max_rf}\")\n",
    "print(f\"R² of 'min_temp' in test set: {r2_min_rf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/opt/anaconda3/lib/python3.9/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "/opt/anaconda3/lib/python3.9/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/opt/anaconda3/lib/python3.9/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/opt/anaconda3/lib/python3.9/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/opt/anaconda3/lib/python3.9/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "/opt/anaconda3/lib/python3.9/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/opt/anaconda3/lib/python3.9/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "/opt/anaconda3/lib/python3.9/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "/opt/anaconda3/lib/python3.9/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "/opt/anaconda3/lib/python3.9/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/opt/anaconda3/lib/python3.9/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/opt/anaconda3/lib/python3.9/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "/opt/anaconda3/lib/python3.9/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/opt/anaconda3/lib/python3.9/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "/opt/anaconda3/lib/python3.9/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   1.8s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   1.9s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   1.9s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   1.9s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   2.0s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   3.6s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   3.6s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   3.6s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   3.5s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   3.5s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   1.8s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   1.8s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   1.9s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   1.8s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   1.8s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   7.0s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   7.0s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   7.2s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   7.0s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   3.5s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   7.1s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   3.5s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   3.7s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   3.7s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   3.7s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=   1.9s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=   1.9s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=   2.0s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=   1.8s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=   1.8s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   7.4s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   7.3s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   3.5s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   7.5s[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   7.5s\n",
      "\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   7.4s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   3.7s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   3.6s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   3.6s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   3.7s\n",
      "[CV] END max_depth=5, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   1.8s\n",
      "[CV] END max_depth=5, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   1.8s\n",
      "[CV] END max_depth=5, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   1.9s\n",
      "[CV] END max_depth=5, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   1.8s\n",
      "[CV] END max_depth=5, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   1.9s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   7.1s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   7.3s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   7.4s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   7.4s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   7.2s\n",
      "[CV] END max_depth=5, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   3.6s\n",
      "[CV] END max_depth=5, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   3.6s\n",
      "[CV] END max_depth=5, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   3.9s\n",
      "[CV] END max_depth=5, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   3.6s\n",
      "[CV] END max_depth=5, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   3.7s\n",
      "[CV] END max_depth=5, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=   1.9s\n",
      "[CV] END max_depth=5, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=   1.9s\n",
      "[CV] END max_depth=5, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=   1.9s\n",
      "[CV] END max_depth=5, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=   1.8s\n",
      "[CV] END max_depth=5, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=   1.8s\n",
      "[CV] END max_depth=5, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   7.4s\n",
      "[CV] END max_depth=5, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   7.4s\n",
      "[CV] END max_depth=5, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   7.3s\n",
      "[CV] END max_depth=5, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   7.4s\n",
      "[CV] END max_depth=5, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   7.6s\n",
      "[CV] END max_depth=5, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   3.7s\n",
      "[CV] END max_depth=5, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   3.7s\n",
      "[CV] END max_depth=5, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   3.6s\n",
      "[CV] END max_depth=5, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   3.7s\n",
      "[CV] END max_depth=5, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   3.7s\n",
      "[CV] END max_depth=5, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=   1.8s\n",
      "[CV] END max_depth=5, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=   1.8s\n",
      "[CV] END max_depth=5, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=   1.9s\n",
      "[CV] END max_depth=5, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=   1.9s\n",
      "[CV] END max_depth=5, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=   1.8s\n",
      "[CV] END max_depth=5, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   7.4s\n",
      "[CV] END max_depth=5, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   7.3s\n",
      "[CV] END max_depth=5, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   7.3s\n",
      "[CV] END max_depth=5, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   7.2s\n",
      "[CV] END max_depth=5, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   3.7s\n",
      "[CV] END max_depth=5, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   7.5s\n",
      "[CV] END max_depth=5, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   3.7s\n",
      "[CV] END max_depth=5, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   3.6s\n",
      "[CV] END max_depth=5, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   3.5s\n",
      "[CV] END max_depth=5, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   3.5s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=   1.8s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=   1.9s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=   1.8s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=   1.8s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=   1.9s\n",
      "[CV] END max_depth=5, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   7.2s\n",
      "[CV] END max_depth=5, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   7.2s\n",
      "[CV] END max_depth=5, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   7.2s\n",
      "[CV] END max_depth=5, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   7.4s\n",
      "[CV] END max_depth=5, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   7.3s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   3.6s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   3.6s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   3.6s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   3.6s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   3.7s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=   1.7s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=   1.7s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=   1.7s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=   1.9s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=   2.0s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   7.2s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   7.4s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   7.2s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   7.2s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   3.7s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   7.3s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   3.7s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   3.6s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   3.6s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   3.6s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=   1.9s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=   1.9s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=   1.8s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=   1.8s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=   1.8s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   7.2s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   7.3s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   7.2s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   7.2s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   3.5s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   7.2s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   3.8s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   3.6s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   3.6s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   3.6s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   3.5s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   3.5s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   3.5s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   7.2s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   7.2s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   7.1s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   7.1s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   6.9s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   3.6s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   3.5s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   7.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   7.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   7.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   7.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   7.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   3.4s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   3.5s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   3.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   3.4s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   3.4s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  14.0s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  14.0s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  14.0s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   7.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  14.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  14.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   7.4s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   7.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   7.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   7.4s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=   3.5s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=   3.5s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=   3.5s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=   3.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=   3.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  14.5s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  14.5s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   7.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  14.5s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  14.4s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  14.6s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   7.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   7.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   7.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   7.0s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   3.6s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   3.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   3.6s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   3.6s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   3.6s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=  14.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=  14.4s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=  14.4s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=  14.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=  14.5s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   7.4s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   7.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   7.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   7.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   7.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=   3.6s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=   3.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=   3.5s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=   3.6s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=   3.5s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=  14.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=  14.4s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=  14.4s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=  14.5s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=  14.5s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   7.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   7.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   7.4s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   7.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   7.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=   3.5s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=   3.5s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=   3.5s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=   3.6s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=   3.6s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  14.4s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  14.4s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  14.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  14.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  14.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   6.9s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   7.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   7.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   7.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   7.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=   3.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=   3.9s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=   3.9s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=   3.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=   3.5s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=  14.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=  14.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=  15.0s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=  14.9s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   7.4s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=  14.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   7.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   8.0s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   8.0s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   8.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=   4.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=   4.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=   4.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=   4.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=   4.0s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=  16.0s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=  16.0s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=  15.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=  16.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   7.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=  15.6s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   7.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   7.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   6.9s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   6.9s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=   3.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=   3.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=   3.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=   3.6s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=   3.6s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=  13.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=  13.9s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=  13.9s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=  13.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   7.0s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=  13.9s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   7.0s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   6.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   6.9s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   7.0s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   5.9s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   5.8s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   5.9s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=  13.5s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=  13.5s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=  13.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=  13.6s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   5.9s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=  13.5s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   5.8s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=  11.8s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=  11.7s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=  11.6s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=  11.9s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=  11.9s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   5.9s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   5.8s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   5.9s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   6.2s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   6.2s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  24.6s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  24.3s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  24.4s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=  12.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  25.0s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=  11.9s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=  12.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  24.8s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=  12.2s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=  12.0s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=   5.7s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=   5.6s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=   5.7s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=   5.6s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=   5.5s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  24.0s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  24.3s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=  11.7s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  24.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  23.3s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=  11.9s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  24.2s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=  11.7s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=  11.7s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=  11.5s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   5.9s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   5.8s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   5.9s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   6.0s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   5.7s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=  23.3s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=  23.2s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=  23.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=  23.3s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=  22.8s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=  11.8s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=  11.9s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=  11.8s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=  11.7s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=   5.9s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=  11.7s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=   5.8s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=   5.9s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=   5.9s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=   5.9s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=  23.8s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=  23.7s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=  23.7s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=  23.4s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=  23.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=  11.8s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=  11.4s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=  11.4s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=  11.4s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=  11.5s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=   5.7s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=   5.8s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=   5.9s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=   6.0s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=   5.7s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  23.4s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  23.4s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  23.3s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  23.5s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  23.6s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=  11.8s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=  11.5s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=  11.6s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=  11.7s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=  11.5s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=   5.5s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=   5.6s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=   5.6s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=   5.7s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=   5.7s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=  23.7s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=  23.5s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=  23.4s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=  23.7s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=  11.4s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=  23.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=  11.2s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=  11.2s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=  11.3s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=  10.8s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=   5.6s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=   5.4s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=   5.3s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=   5.3s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=   5.2s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=  21.9s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=  21.9s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=  21.8s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=  22.0s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=  11.0s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=  21.6s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=  10.9s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=  11.2s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=  11.2s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=  10.9s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=   5.8s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=   5.6s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=   5.5s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=   5.7s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=   5.5s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=  22.9s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=  22.5s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=  22.4s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=  22.4s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=  22.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=  11.2s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=  10.9s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=  11.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=  11.2s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=  10.8s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   7.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   6.8s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   6.9s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   7.3s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   7.2s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=  22.8s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=  22.7s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=  22.9s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=  23.0s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=  22.7s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=  14.7s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=  14.5s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=  14.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=  14.4s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=  14.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   6.4s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   6.5s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   6.8s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   6.3s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   6.4s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  28.3s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  28.0s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  27.9s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=  13.5s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  28.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  27.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=  13.4s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=  13.6s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=  13.6s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=  13.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=   6.5s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=   6.3s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=   6.4s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=   6.3s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=   6.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  27.8s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  26.3s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  26.7s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=  13.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=  12.7s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  26.0s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  26.9s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=  12.8s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=  12.9s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=  12.6s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   6.9s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   6.4s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   6.5s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   6.6s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   6.3s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=  25.4s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=  26.3s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=  25.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=  25.8s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=  26.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=  13.4s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=  13.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=  13.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=  12.9s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=  12.8s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=   6.4s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=   6.0s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=   6.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=   6.4s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=   6.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=  25.6s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=  25.7s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=  26.5s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=  26.0s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=  24.9s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=  13.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=  12.6s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=  12.5s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=  12.4s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=  12.3s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=   6.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=   5.9s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=   6.0s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=   6.0s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=   6.3s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  27.6s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  29.3s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  29.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  30.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  31.6s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=  20.5s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=  21.4s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=  24.9s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=  25.0s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=  24.3s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=  12.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=  11.4s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=  11.7s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=  11.6s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=  11.3s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=  48.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=  50.6s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=  49.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=  48.7s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=  23.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=  47.7s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=  23.0s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=  23.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=  23.3s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=  22.8s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=  11.9s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=  11.0s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=  11.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=  11.4s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=  11.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=  47.4s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=  45.7s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=  46.3s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=  46.3s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=  23.3s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=  44.0s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=  22.9s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=  22.9s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=  22.5s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=  23.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=  11.6s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=  10.9s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=  11.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=  11.5s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=  11.0s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=  46.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=  47.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=  46.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=  47.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=  45.0s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=  23.6s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=  23.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=  22.7s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=  22.3s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=  22.0s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=  42.6s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=  41.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=  40.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=  40.8s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=  37.8s\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   2.3s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   2.3s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   2.3s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   2.3s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   2.3s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   5.0s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   5.0s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   5.0s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   6.1s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   6.1s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   3.5s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   3.5s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   3.5s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   3.5s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   3.6s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  13.3s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  13.3s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  13.4s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   6.8s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   6.9s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  13.9s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  14.2s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   6.9s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   6.9s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   7.0s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=   3.7s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=   3.7s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=   3.6s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=   3.5s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=   3.4s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  14.2s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  13.9s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  14.2s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  14.1s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   6.9s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  14.1s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   6.8s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   6.8s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   6.9s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   6.8s\n",
      "[CV] END max_depth=5, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   3.4s\n",
      "[CV] END max_depth=5, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   3.4s\n",
      "[CV] END max_depth=5, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   3.7s\n",
      "[CV] END max_depth=5, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   3.8s\n",
      "[CV] END max_depth=5, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   3.8s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=  13.8s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=  14.0s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=  14.1s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=  14.0s\n",
      "[CV] END max_depth=5, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   6.7s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=  14.2s\n",
      "[CV] END max_depth=5, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   7.2s\n",
      "[CV] END max_depth=5, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   7.0s\n",
      "[CV] END max_depth=5, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   6.8s\n",
      "[CV] END max_depth=5, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   6.9s\n",
      "[CV] END max_depth=5, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=   3.5s\n",
      "[CV] END max_depth=5, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=   3.8s\n",
      "[CV] END max_depth=5, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=   3.9s\n",
      "[CV] END max_depth=5, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=   3.8s\n",
      "[CV] END max_depth=5, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=   3.4s\n",
      "[CV] END max_depth=5, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=  13.9s\n",
      "[CV] END max_depth=5, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=  14.5s\n",
      "[CV] END max_depth=5, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=  14.3s\n",
      "[CV] END max_depth=5, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   6.9s\n",
      "[CV] END max_depth=5, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=  14.3s\n",
      "[CV] END max_depth=5, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=  14.1s\n",
      "[CV] END max_depth=5, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   6.9s\n",
      "[CV] END max_depth=5, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   7.0s\n",
      "[CV] END max_depth=5, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   6.9s\n",
      "[CV] END max_depth=5, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   7.0s\n",
      "[CV] END max_depth=5, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=   3.4s\n",
      "[CV] END max_depth=5, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=   3.3s\n",
      "[CV] END max_depth=5, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=   3.5s\n",
      "[CV] END max_depth=5, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=   3.6s\n",
      "[CV] END max_depth=5, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=   3.7s\n",
      "[CV] END max_depth=5, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  13.9s\n",
      "[CV] END max_depth=5, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  14.4s\n",
      "[CV] END max_depth=5, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  14.3s\n",
      "[CV] END max_depth=5, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  13.9s\n",
      "[CV] END max_depth=5, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  14.3s\n",
      "[CV] END max_depth=5, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   7.3s\n",
      "[CV] END max_depth=5, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   7.2s\n",
      "[CV] END max_depth=5, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   7.4s\n",
      "[CV] END max_depth=5, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   6.8s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=   3.4s\n",
      "[CV] END max_depth=5, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   7.2s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=   3.4s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=   3.7s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=   3.8s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=   3.5s\n",
      "[CV] END max_depth=5, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=  14.1s\n",
      "[CV] END max_depth=5, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=  14.2s\n",
      "[CV] END max_depth=5, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=  14.6s\n",
      "[CV] END max_depth=5, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=  14.3s\n",
      "[CV] END max_depth=5, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=  14.0s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   7.0s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   6.7s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   6.9s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   6.8s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   7.1s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=   3.5s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=   3.6s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=   3.6s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=   3.5s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=   3.7s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=  13.9s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=  14.2s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=  13.9s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=  14.1s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=  14.2s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   7.1s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   6.9s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   7.1s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   6.9s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   7.0s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=   3.5s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=   3.5s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=   3.4s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=   3.6s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=   3.7s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=  14.2s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=  14.0s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=  13.9s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=  14.2s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   7.0s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=  13.9s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   7.0s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   7.1s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   7.2s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   7.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   7.0s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=  14.0s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   7.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   7.0s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=  14.1s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=  14.4s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=  14.2s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=  14.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   7.0s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   6.9s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=  13.6s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=  13.6s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=  13.6s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=  14.0s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=  13.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   6.5s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   6.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   6.5s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   6.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   6.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  26.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  27.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  27.6s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=  14.0s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  27.9s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  28.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=  14.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=  14.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=  14.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=  14.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=   7.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=   7.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=   7.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=   6.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=   6.6s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  27.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  27.9s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=  13.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  27.5s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  28.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  27.9s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=  13.6s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=  13.6s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=  13.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=  13.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   7.0s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   6.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   6.6s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   6.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   6.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=  27.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=  27.6s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=  27.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=  27.6s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=  27.4s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=  13.4s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=  13.5s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=  13.6s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=  13.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=   6.9s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=  13.9s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=   7.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=   7.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=   7.0s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=   6.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=  27.5s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=  27.5s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=  27.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=  27.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=  27.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=  13.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=  14.0s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=  13.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=  13.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=  13.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=   6.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=   6.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=   7.0s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=   7.0s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  27.5s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=   7.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  27.9s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  27.9s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  28.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=  14.9s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  29.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=  14.9s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=  14.4s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=  14.6s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=  14.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=   7.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=   7.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=   7.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=   7.0s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=   7.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=  28.9s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=  28.6s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=  13.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=  28.5s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=  28.5s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=  28.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=  13.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=  13.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=  13.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=  14.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=   6.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=   7.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=   6.9s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=   6.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=   6.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=  27.4s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=  27.6s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=  27.6s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=  26.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=  13.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=  13.6s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=  27.4s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=  13.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=  13.5s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=  13.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=   6.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=   6.6s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=   6.5s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=   6.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=   7.0s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=  27.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=  27.4s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=  26.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=  27.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=  27.5s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=  13.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=  13.5s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=  14.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=  13.5s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=  13.8s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=  11.7s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=  11.8s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=  12.0s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=  27.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=  27.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=  27.4s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=  27.5s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=  27.7s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=  12.3s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=  12.4s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=  24.3s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=  24.0s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=  23.8s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=  23.9s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=  23.8s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=  11.5s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=  11.3s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=  11.5s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=  11.6s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=  11.4s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  47.9s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  46.7s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  47.3s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=  23.2s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  47.2s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=  22.8s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=  23.3s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  47.3s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=  23.6s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=  23.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=  11.3s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=  10.8s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=  11.3s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=  11.5s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=  11.3s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  46.7s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  46.5s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=  22.5s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  45.8s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=  21.9s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  46.4s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  47.2s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=  22.0s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=  23.2s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=  22.7s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=  11.8s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=  11.2s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=  11.5s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=  11.8s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=  11.4s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=  46.2s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=  45.0s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=  44.8s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=  45.6s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=  45.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=  22.9s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=  22.7s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=  22.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=  11.3s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=  23.3s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=  22.4s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=  11.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=  11.2s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=  11.7s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=  11.2s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=  46.4s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=  44.8s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=  46.3s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=  45.9s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=  45.5s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=  22.9s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=  22.6s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=  22.9s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=  22.5s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=  23.3s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=  11.0s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=  11.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=  10.9s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=  11.0s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=  11.3s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  46.4s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  44.9s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  46.0s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  45.8s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  45.2s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=  22.6s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=  22.2s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=  22.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=  21.7s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=  22.3s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=  10.5s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=  10.5s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=  10.8s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=  10.7s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=  11.0s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=  45.5s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=  44.7s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=  44.3s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=  44.8s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=  22.5s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=  45.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=  23.0s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=  22.3s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=  23.0s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=  22.4s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=  11.0s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=  10.9s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=  10.8s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=  11.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=  10.8s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=  44.4s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=  44.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=  44.0s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=  44.3s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=  22.2s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=  44.6s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=  22.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=  22.3s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=  22.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=  22.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=  10.8s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=  10.5s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=  10.7s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=  10.5s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=  10.4s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=  44.2s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=  44.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=  43.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=  43.2s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=  21.3s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=  42.3s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=  21.2s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=  21.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=  21.6s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=  21.0s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=  13.5s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=  13.3s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=  13.5s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=  13.5s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=  43.0s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=  13.4s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=  41.9s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=  43.4s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=  43.5s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=  42.5s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=  27.5s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=  27.0s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=  26.5s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=  27.5s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=  27.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=  12.9s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=  12.5s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=  13.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=  13.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=  13.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  55.6s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  54.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  53.0s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=  26.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  54.9s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  53.5s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=  25.8s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=  25.3s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=  25.0s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=  25.9s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=  12.4s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=  12.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=  12.3s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=  12.4s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=  12.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  52.7s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  51.7s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=  24.5s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  50.6s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  51.9s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  50.8s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=  24.9s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=  24.4s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=  24.9s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=  24.4s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=  12.6s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=  12.0s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=  12.6s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=  13.0s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=  13.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=  49.3s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=  51.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=  48.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=  49.5s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=  49.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=  25.6s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=  25.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=  25.0s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=  25.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=  25.7s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=  12.3s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=  11.9s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=  12.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=  12.7s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=  12.6s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=  51.5s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=  49.6s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=  51.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=  50.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=  49.8s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=  26.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=  26.0s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=  26.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=  27.4s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=  26.3s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=  12.8s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=  12.3s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=  11.9s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=  12.0s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=  11.9s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  53.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  51.3s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  52.7s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  53.0s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  51.3s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=  25.0s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=  24.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=  24.3s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=  25.5s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=  24.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=  11.4s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=  11.6s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=  11.8s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=  11.7s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=  11.3s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=  50.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=  47.3s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=  48.3s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=  48.9s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=  23.6s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=  48.6s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=  23.0s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=  23.0s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=  24.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=  23.7s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=  12.6s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=  11.7s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=  11.7s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=  11.4s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=  11.3s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=  47.8s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=  47.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=  46.5s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=  45.8s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=  46.9s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=  23.3s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=  23.4s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=  22.4s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=  22.8s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=  23.7s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=  11.5s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=  11.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=  11.5s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=  11.6s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=  11.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=  47.0s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=  45.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=  46.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=  46.8s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=  23.3s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=  45.5s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=  23.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=  22.5s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=  22.6s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=  21.8s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=  42.9s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=  41.3s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=  40.6s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=  40.5s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=  38.1s\n",
      "Optimum parameter (max_temp): {'max_depth': 5, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "Optimum parameter (min_temp): {'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "#Select the best parameters and optimize the model\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [5, 10, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# GridSearchCV \n",
    "rf_max = RandomForestRegressor(random_state=42)\n",
    "grid_search_max = GridSearchCV(estimator=rf_max, param_grid=param_grid,\n",
    "                               cv=5, n_jobs=-1, verbose=2, scoring='neg_mean_squared_error')\n",
    "grid_search_max.fit(X_train_rf, y_train_max_rf)\n",
    "\n",
    "rf_min = RandomForestRegressor(random_state=42)\n",
    "grid_search_min = GridSearchCV(estimator=rf_min, param_grid=param_grid,\n",
    "                               cv=5, n_jobs=-1, verbose=2, scoring='neg_mean_squared_error')\n",
    "grid_search_min.fit(X_train_rf, y_train_min_rf)\n",
    "\n",
    "print(f\"Optimum parameter (max_temp): {grid_search_max.best_params_}\")\n",
    "print(f\"Optimum parameter (min_temp): {grid_search_min.best_params_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimum parameter (max_temp): {'max_depth': 5, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "Optimum parameter (min_temp): {'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"Optimum parameter (max_temp): {grid_search_max.best_params_}\")\n",
    "print(f\"Optimum parameter (min_temp): {grid_search_min.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation results with best parameters:\n",
      "Average MSE of 'max_temp': 0.19\n",
      "Average MSE of 'min_temp': 0.21\n",
      "Average RMSE of 'max_temp': 0.44\n",
      "Average RMSE of 'min_temp': 0.46\n",
      "Average MAE of 'max_temp': 0.34\n",
      "Average MAE of 'min_temp': 0.37\n",
      "Average R² of 'max_temp': 0.81\n",
      "Average R² of 'min_temp': 0.79\n"
     ]
    }
   ],
   "source": [
    "# cross validation\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# The best parameters from the previous GridSearchCV results\n",
    "best_params_max = grid_search_max.best_params_\n",
    "best_params_min = grid_search_min.best_params_\n",
    "\n",
    "X, y_max, y_min = create_features_and_target(train_df_weather)\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "mse_scores_max = []\n",
    "mse_scores_min = []\n",
    "rmse_scores_max = []\n",
    "rmse_scores_min = []\n",
    "mae_scores_max = []\n",
    "mae_scores_min = []\n",
    "r2_scores_max = []\n",
    "r2_scores_min = []\n",
    "\n",
    "for train_index, val_index in kf.split(X):\n",
    "    X_train_cv, X_val_cv = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_train_max_cv, y_val_max_cv = y_max.iloc[train_index], y_max.iloc[val_index]\n",
    "    y_train_min_cv, y_val_min_cv = y_min.iloc[train_index], y_min.iloc[val_index]\n",
    "    \n",
    "    rf_max = RandomForestRegressor(**best_params_max, random_state=42)\n",
    "    rf_max.fit(X_train_cv, y_train_max_cv)\n",
    "\n",
    "    rf_min = RandomForestRegressor(**best_params_min, random_state=42)\n",
    "    rf_min.fit(X_train_cv, y_train_min_cv)\n",
    "\n",
    "    y_val_pred_max_rf = rf_max.predict(X_val_cv)\n",
    "    y_val_pred_min_rf = rf_min.predict(X_val_cv)\n",
    "\n",
    "    mse_max = mean_squared_error(y_val_max_cv, y_val_pred_max_rf)\n",
    "    mse_min = mean_squared_error(y_val_min_cv, y_val_pred_min_rf)\n",
    "    rmse_max = np.sqrt(mse_max)\n",
    "    rmse_min = np.sqrt(mse_min)\n",
    "    \n",
    "    mse_scores_max.append(mse_max)\n",
    "    mse_scores_min.append(mse_min)\n",
    "    rmse_scores_max.append(rmse_max)\n",
    "    rmse_scores_min.append(rmse_min)\n",
    "    mae_scores_max.append(mean_absolute_error(y_val_max_cv, y_val_pred_max_rf))\n",
    "    mae_scores_min.append(mean_absolute_error(y_val_min_cv, y_val_pred_min_rf))\n",
    "    r2_scores_max.append(r2_score(y_val_max_cv, y_val_pred_max_rf))\n",
    "    r2_scores_min.append(r2_score(y_val_min_cv, y_val_pred_min_rf))\n",
    "\n",
    "print(\"Cross-validation results with best parameters:\")\n",
    "print(f\"Average MSE of 'max_temp': {np.mean(mse_scores_max):.2f}\")\n",
    "print(f\"Average MSE of 'min_temp': {np.mean(mse_scores_min):.2f}\")\n",
    "print(f\"Average RMSE of 'max_temp': {np.mean(rmse_scores_max):.2f}\")\n",
    "print(f\"Average RMSE of 'min_temp': {np.mean(rmse_scores_min):.2f}\")\n",
    "print(f\"Average MAE of 'max_temp': {np.mean(mae_scores_max):.2f}\")\n",
    "print(f\"Average MAE of 'min_temp': {np.mean(mae_scores_min):.2f}\")\n",
    "print(f\"Average R² of 'max_temp': {np.mean(r2_scores_max):.2f}\")\n",
    "print(f\"Average R² of 'min_temp': {np.mean(r2_scores_min):.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the training set: (9195, 10, 3) (9195, 2)\n",
      "Size of the validation set: (3058, 10, 3) (3058, 2)\n",
      "Size of the testing set: (3058, 10, 3) (3058, 2)\n"
     ]
    }
   ],
   "source": [
    "train_df_weather = train_df_weather.sort_values('date')\n",
    "val_df_weather = val_df_weather.sort_values('date')\n",
    "test_df_weather = test_df_weather.sort_values('date')\n",
    "\n",
    "features = ['global_radiation', 'diff_temp', 'mean_temp']\n",
    "target = ['max_temp', 'min_temp']\n",
    "\n",
    "train_X_rnn = train_df_weather[features].values\n",
    "train_y_rnn = train_df_weather[target].values\n",
    "\n",
    "val_X_rnn = val_df_weather[features].values\n",
    "val_y_rnn = val_df_weather[target].values\n",
    "\n",
    "test_X_rnn = test_df_weather[features].values\n",
    "test_y_rnn = test_df_weather[target].values\n",
    "\n",
    "# Create a time window function\n",
    "def create_time_series(X, y, time_steps=10):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        Xs.append(X[i:(i + time_steps)])\n",
    "        ys.append(y[i + time_steps])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "time_steps = 10\n",
    "X_train_rnn, y_train_rnn = create_time_series(train_X_rnn, train_y_rnn, time_steps)\n",
    "X_val_rnn, y_val_rnn = create_time_series(val_X_rnn, val_y_rnn, time_steps)\n",
    "X_test_rnn, y_test_rnn = create_time_series(test_X_rnn, test_y_rnn, time_steps)\n",
    "\n",
    "print(\"Size of the training set:\", X_train_rnn.shape, y_train_rnn.shape)\n",
    "print(\"Size of the validation set:\", X_val_rnn.shape, y_val_rnn.shape)\n",
    "print(\"Size of the testing set:\", X_test_rnn.shape, y_test_rnn.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_21\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_21\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">10,800</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_121 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">102</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_21 (\u001b[38;5;33mLSTM\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │        \u001b[38;5;34m10,800\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_121 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │           \u001b[38;5;34m102\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,902</span> (42.59 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m10,902\u001b[0m (42.59 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,902</span> (42.59 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m10,902\u001b[0m (42.59 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "\n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, activation='relu', input_shape=(time_steps, len(features))))\n",
    "model.add(Dense(2))  \n",
    "\n",
    "model.compile(optimizer='adam', loss='mse', run_eagerly=True)\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 36ms/step - loss: 0.3555 - val_loss: 0.1457\n",
      "Epoch 2/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 35ms/step - loss: 0.1445 - val_loss: 0.1354\n",
      "Epoch 3/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 35ms/step - loss: 0.1376 - val_loss: 0.1335\n",
      "Epoch 4/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 34ms/step - loss: 0.1382 - val_loss: 0.1318\n",
      "Epoch 5/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 35ms/step - loss: 0.1345 - val_loss: 0.1298\n",
      "Epoch 6/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 34ms/step - loss: 0.1332 - val_loss: 0.1327\n",
      "Epoch 7/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 35ms/step - loss: 0.1344 - val_loss: 0.1336\n",
      "Epoch 8/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 34ms/step - loss: 0.1310 - val_loss: 0.1316\n",
      "Epoch 9/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 34ms/step - loss: 0.1358 - val_loss: 0.1292\n",
      "Epoch 10/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 35ms/step - loss: 0.1336 - val_loss: 0.1302\n",
      "Epoch 11/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 35ms/step - loss: 0.1282 - val_loss: 0.1367\n",
      "Epoch 12/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 34ms/step - loss: 0.1306 - val_loss: 0.1285\n",
      "Epoch 13/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 35ms/step - loss: 0.1334 - val_loss: 0.1297\n",
      "Epoch 14/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 35ms/step - loss: 0.1302 - val_loss: 0.1306\n",
      "Epoch 15/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 35ms/step - loss: 0.1292 - val_loss: 0.1291\n",
      "Epoch 16/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 35ms/step - loss: 0.1310 - val_loss: 0.1305\n",
      "Epoch 17/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 35ms/step - loss: 0.1274 - val_loss: 0.1322\n",
      "Epoch 18/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 35ms/step - loss: 0.1286 - val_loss: 0.1306\n",
      "Epoch 19/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 35ms/step - loss: 0.1285 - val_loss: 0.1340\n",
      "Epoch 20/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 34ms/step - loss: 0.1308 - val_loss: 0.1324\n",
      "Epoch 21/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 35ms/step - loss: 0.1258 - val_loss: 0.1322\n",
      "Epoch 22/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 34ms/step - loss: 0.1251 - val_loss: 0.1326\n",
      "Epoch 23/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 35ms/step - loss: 0.1269 - val_loss: 0.1312\n",
      "Epoch 24/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 35ms/step - loss: 0.1259 - val_loss: 0.1309\n",
      "Epoch 25/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 34ms/step - loss: 0.1269 - val_loss: 0.1356\n",
      "Epoch 26/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 35ms/step - loss: 0.1288 - val_loss: 0.1381\n",
      "Epoch 27/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 34ms/step - loss: 0.1272 - val_loss: 0.1345\n",
      "Epoch 28/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 35ms/step - loss: 0.1253 - val_loss: 0.1329\n",
      "Epoch 29/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 35ms/step - loss: 0.1274 - val_loss: 0.1374\n",
      "Epoch 30/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 34ms/step - loss: 0.1261 - val_loss: 0.1382\n",
      "Epoch 31/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 35ms/step - loss: 0.1253 - val_loss: 0.1384\n",
      "Epoch 32/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 35ms/step - loss: 0.1242 - val_loss: 0.1373\n",
      "Epoch 33/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 34ms/step - loss: 0.1241 - val_loss: 0.1409\n",
      "Epoch 34/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 35ms/step - loss: 0.1231 - val_loss: 0.1416\n",
      "Epoch 35/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 35ms/step - loss: 0.1238 - val_loss: 0.1394\n",
      "Epoch 36/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 35ms/step - loss: 0.1244 - val_loss: 0.1499\n",
      "Epoch 37/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 34ms/step - loss: 0.1242 - val_loss: 0.1425\n",
      "Epoch 38/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 35ms/step - loss: 0.1232 - val_loss: 0.1404\n",
      "Epoch 39/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 35ms/step - loss: 0.1245 - val_loss: 0.1388\n",
      "Epoch 40/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 35ms/step - loss: 0.1256 - val_loss: 0.1440\n",
      "Epoch 41/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 34ms/step - loss: 0.1254 - val_loss: 0.1425\n",
      "Epoch 42/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 35ms/step - loss: 0.1231 - val_loss: 0.1449\n",
      "Epoch 43/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 35ms/step - loss: 0.1205 - val_loss: 0.1450\n",
      "Epoch 44/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 35ms/step - loss: 0.1243 - val_loss: 0.1431\n",
      "Epoch 45/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 35ms/step - loss: 0.1210 - val_loss: 0.1438\n",
      "Epoch 46/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 34ms/step - loss: 0.1215 - val_loss: 0.1481\n",
      "Epoch 47/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 35ms/step - loss: 0.1200 - val_loss: 0.1456\n",
      "Epoch 48/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 35ms/step - loss: 0.1230 - val_loss: 0.1509\n",
      "Epoch 49/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 35ms/step - loss: 0.1222 - val_loss: 0.1508\n",
      "Epoch 50/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 35ms/step - loss: 0.1231 - val_loss: 0.1488\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "history = model.fit(X_train_rnn, y_train_rnn, epochs=50, batch_size=32, validation_data=(X_val_rnn, y_val_rnn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# save model\n",
    "model.save('rnn_weather_model.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.1381\n",
      "The loss in testing set: 0.16117988526821136\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# evaludate\n",
    "loss = model.evaluate(X_test_rnn, y_test_rnn)\n",
    "print('The loss in testing set:', loss)\n",
    "\n",
    "# loss function\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n",
      "RMSE of 'max_temp' in test set: 0.4010163099390085\n",
      "RMSE of 'min_temp' in test set: 0.40192748090429864\n",
      "MSE of 'max_temp' in test set: 0.16081408083709892\n",
      "MSE of 'min_temp' in test set: 0.16154569990607534\n",
      "MAE of 'max_temp' in test set: 0.30369115058371127\n",
      "MAE of 'min_temp' in test set: 0.3181266719998786\n",
      "R² of 'max_temp' in test set: 0.8378234034032515\n",
      "R² of 'min_temp' in test set: 0.8295135125693893\n"
     ]
    }
   ],
   "source": [
    "# evaluate\n",
    "y_val_pred_rnn = model.predict(X_val_rnn)\n",
    "y_test_pred_rnn = model.predict(X_test_rnn)\n",
    "\n",
    "# RMSE\n",
    "rmse_max_rnn = np.sqrt(mean_squared_error(y_test_rnn[:, 0], y_test_pred_rnn[:, 0]))\n",
    "rmse_min_rnn = np.sqrt(mean_squared_error(y_test_rnn[:, 1], y_test_pred_rnn[:, 1]))\n",
    "\n",
    "# MSE\n",
    "mse_max_rnn = mean_squared_error(y_test_rnn[:, 0], y_test_pred_rnn[:, 0])\n",
    "mse_min_rnn = mean_squared_error(y_test_rnn[:, 1], y_test_pred_rnn[:, 1])\n",
    "\n",
    "# MAE\n",
    "mae_max_rnn = mean_absolute_error(y_test_rnn[:, 0], y_test_pred_rnn[:, 0])\n",
    "mae_min_rnn = mean_absolute_error(y_test_rnn[:, 1], y_test_pred_rnn[:, 1])\n",
    "\n",
    "# R²\n",
    "r2_max_rnn = r2_score(y_test_rnn[:, 0], y_test_pred_rnn[:, 0])\n",
    "r2_min_rnn = r2_score(y_test_rnn[:, 1], y_test_pred_rnn[:, 1])\n",
    "\n",
    "print(f\"RMSE of 'max_temp' in test set: {rmse_max_rnn}\")\n",
    "print(f\"RMSE of 'min_temp' in test set: {rmse_min_rnn}\")\n",
    "\n",
    "print(f\"MSE of 'max_temp' in test set: {mse_max_rnn}\")\n",
    "print(f\"MSE of 'min_temp' in test set: {mse_min_rnn}\")\n",
    "\n",
    "print(f\"MAE of 'max_temp' in test set: {mae_max_rnn}\")\n",
    "print(f\"MAE of 'min_temp' in test set: {mae_min_rnn}\")\n",
    "\n",
    "print(f\"R² of 'max_temp' in test set: {r2_max_rnn}\")\n",
    "print(f\"R² of 'min_temp' in test set: {r2_min_rnn}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permutation Feature Importance\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "X_test_rnn_reshaped = X_test_rnn.reshape(X_test_rnn.shape[0], -1)\n",
    "\n",
    "def custom_scoring(estimator, X, y):\n",
    "    X_reshaped7 = X.reshape(-1, time_steps, len(features))\n",
    "    y_pred7 = estimator.predict(X_reshaped7)\n",
    "    return -np.mean((y_pred7 - y) ** 2)\n",
    "\n",
    "# use scikit-learn  permutation_importance\n",
    "# result_rnn = permutation_importance(estimator=model, X=X_test_rnn_reshaped, y=y_test_rnn, n_repeats=10, random_state=42, scoring=custom_scoring, n_jobs=-1)\n",
    "\n",
    "# # show \n",
    "# for i in range(len(features)):\n",
    "#     print(f\"Feature: {features[i]}, Importance: {result_rnn.importances_mean[i]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Fold 1, Loss: 0.13885636627674103, RMSE Max: 0.3754215137386663, RMSE Min: 0.36982617017735475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Fold 2, Loss: 0.13154767453670502, RMSE Max: 0.3736972104564844, RMSE Min: 0.35134840267764916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Fold 3, Loss: 0.1388428658246994, RMSE Max: 0.38979825589716455, RMSE Min: 0.35460278307847803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Fold 4, Loss: 0.1300431191921234, RMSE Max: 0.3595533040383151, RMSE Min: 0.3616734033677247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Fold 5, Loss: 0.16811782121658325, RMSE Max: 0.4009343299207795, RMSE Min: 0.41891197416440723\n",
      "Average Loss: 0.14148156940937043\n",
      "Average RMSE of max_temp: 0.37988092281028196\n",
      "Average RMSE of min_temp: 0.3712725466931227\n",
      "Average MSE of max_temp: 0.14451210274703088\n",
      "Average MSE of min_temp: 0.13845102455622027\n",
      "Average MAE of max_temp: 0.2945731504685038\n",
      "Average MAE of min_temp: 0.2968711330170186\n",
      "Average R² of max_temp: 0.8522027597069375\n",
      "Average R² of min_temp: 0.8566586006539417\n"
     ]
    }
   ],
   "source": [
    "# cross validation\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "features = ['global_radiation', 'diff_temp', 'mean_temp']\n",
    "target = ['max_temp', 'min_temp']\n",
    "X = df_weather[features].values  \n",
    "y = df_weather[target].values  \n",
    "\n",
    "\n",
    "def create_lstm_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, activation='relu', input_shape=input_shape))\n",
    "    model.add(Dense(2))  \n",
    "    model.compile(optimizer=Adam(), loss='mse')\n",
    "    return model\n",
    "\n",
    "# TimeSeriesSplit\n",
    "n_splits = 5\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "results = {\n",
    "    'loss': [],\n",
    "    'rmse_max': [],\n",
    "    'rmse_min': [],\n",
    "    'mse_max': [],\n",
    "    'mse_min': [],\n",
    "    'mae_max': [],\n",
    "    'mae_min': [],\n",
    "    'r2_max': [],\n",
    "    'r2_min': []\n",
    "}\n",
    "\n",
    "for train_index, val_index in tscv.split(X):\n",
    "    X_train, X_val = X[train_index], X[val_index]\n",
    "    y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "    time_steps = 10\n",
    "    X_train_rnn, y_train_rnn = create_time_series(X_train, y_train, time_steps)\n",
    "    X_val_rnn, y_val_rnn = create_time_series(X_val, y_val, time_steps)\n",
    "\n",
    "    model = create_lstm_model((time_steps, len(features)))\n",
    "    history = model.fit(X_train_rnn, y_train_rnn, epochs=50, batch_size=32, validation_data=(X_val_rnn, y_val_rnn), verbose=0)\n",
    "\n",
    "    loss = model.evaluate(X_val_rnn, y_val_rnn, verbose=0)\n",
    "    y_val_pred_rnn = model.predict(X_val_rnn)\n",
    "\n",
    "    mse_max = mean_squared_error(y_val_rnn[:, 0], y_val_pred_rnn[:, 0])\n",
    "    mse_min = mean_squared_error(y_val_rnn[:, 1], y_val_pred_rnn[:, 1])\n",
    "    rmse_max = np.sqrt(mse_max)\n",
    "    rmse_min = np.sqrt(mse_min)\n",
    "    mae_max = mean_absolute_error(y_val_rnn[:, 0], y_val_pred_rnn[:, 0])\n",
    "    mae_min = mean_absolute_error(y_val_rnn[:, 1], y_val_pred_rnn[:, 1])\n",
    "    r2_max = r2_score(y_val_rnn[:, 0], y_val_pred_rnn[:, 0])\n",
    "    r2_min = r2_score(y_val_rnn[:, 1], y_val_pred_rnn[:, 1])\n",
    "\n",
    "    results['loss'].append(loss)\n",
    "    results['rmse_max'].append(rmse_max)\n",
    "    results['rmse_min'].append(rmse_min)\n",
    "    results['mse_max'].append(mse_max)\n",
    "    results['mse_min'].append(mse_min)\n",
    "    results['mae_max'].append(mae_max)\n",
    "    results['mae_min'].append(mae_min)\n",
    "    results['r2_max'].append(r2_max)\n",
    "    results['r2_min'].append(r2_min)\n",
    "    print(f'Fold {len(results[\"loss\"])}, Loss: {loss}, RMSE Max: {rmse_max}, RMSE Min: {rmse_min}')\n",
    "\n",
    "print(f'Average Loss: {np.mean(results[\"loss\"])}')\n",
    "print(f'Average RMSE of max_temp: {np.mean(results[\"rmse_max\"])}')\n",
    "print(f'Average RMSE of min_temp: {np.mean(results[\"rmse_min\"])}')\n",
    "print(f'Average MSE of max_temp: {np.mean(results[\"mse_max\"])}')\n",
    "print(f'Average MSE of min_temp: {np.mean(results[\"mse_min\"])}')\n",
    "print(f'Average MAE of max_temp: {np.mean(results[\"mae_max\"])}')\n",
    "print(f'Average MAE of min_temp: {np.mean(results[\"mae_min\"])}')\n",
    "print(f'Average R² of max_temp: {np.mean(results[\"r2_max\"])}')\n",
    "print(f'Average R² of min_temp: {np.mean(results[\"r2_min\"])}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Input, LayerNormalization, MultiHeadAttention, Dropout, GlobalAveragePooling1D, Add\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    # Normalization and Attention\n",
    "    x = LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    x = MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(x, x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    res = Add()([x, inputs])\n",
    "\n",
    "    # Feed Forward Part\n",
    "    x = LayerNormalization(epsilon=1e-6)(res)\n",
    "    x = Dense(ff_dim, activation=\"relu\")(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = Dense(inputs.shape[-1])(x)\n",
    "    return Add()([x, res])\n",
    "\n",
    "def build_model(input_shape, head_size, num_heads, ff_dim, num_transformer_blocks, mlp_units, dropout=0, mlp_dropout=0):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = inputs\n",
    "    for _ in range(num_transformer_blocks):\n",
    "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
    "\n",
    "    x = GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
    "    for dim in mlp_units:\n",
    "        x = Dense(dim, activation=\"relu\")(x)\n",
    "        x = Dropout(mlp_dropout)(x)\n",
    "    outputs = Dense(2)(x)  \n",
    "    return Model(inputs, outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_54\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_54\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_32      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span> │ input_layer_32[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">15,363</span> │ layer_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_131         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_atten… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_80 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_131[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                     │                   │            │ input_layer_32[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span> │ add_80[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_122 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span> │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_132         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_122[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_123 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span> │ dropout_132[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_81 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_123[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│                     │                   │            │ add_80[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span> │ add_81[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">15,363</span> │ layer_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_134         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_atten… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_82 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_134[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                     │                   │            │ add_81[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span> │ add_82[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_124 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span> │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_135         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_124[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_125 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span> │ dropout_135[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_83 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_125[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│                     │                   │            │ add_82[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span> │ add_83[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">15,363</span> │ layer_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_137         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_atten… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_84 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_137[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                     │                   │            │ add_83[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span> │ add_84[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_126 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span> │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_138         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_126[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_127 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span> │ dropout_138[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_85 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_127[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│                     │                   │            │ add_84[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span> │ add_85[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">15,363</span> │ layer_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_140         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_atten… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_86 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_140[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                     │                   │            │ add_85[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span> │ add_86[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_128 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span> │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_141         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_128[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_129 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span> │ dropout_141[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_87 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_129[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│                     │                   │            │ add_86[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_87[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_130 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,408</span> │ global_average_p… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_142         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_130[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_131 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">258</span> │ dropout_142[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_32      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │          \u001b[38;5;34m6\u001b[0m │ input_layer_32[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │     \u001b[38;5;34m15,363\u001b[0m │ layer_normalizat… │\n",
       "│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_131         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ multi_head_atten… │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_80 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ dropout_131[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
       "│                     │                   │            │ input_layer_32[\u001b[38;5;34m0\u001b[0m… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │          \u001b[38;5;34m6\u001b[0m │ add_80[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_122 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m4\u001b[0m)     │         \u001b[38;5;34m16\u001b[0m │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_132         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m4\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ dense_122[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_123 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │         \u001b[38;5;34m15\u001b[0m │ dropout_132[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_81 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ dense_123[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│                     │                   │            │ add_80[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │          \u001b[38;5;34m6\u001b[0m │ add_81[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │     \u001b[38;5;34m15,363\u001b[0m │ layer_normalizat… │\n",
       "│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_134         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ multi_head_atten… │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_82 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ dropout_134[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
       "│                     │                   │            │ add_81[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │          \u001b[38;5;34m6\u001b[0m │ add_82[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_124 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m4\u001b[0m)     │         \u001b[38;5;34m16\u001b[0m │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_135         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m4\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ dense_124[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_125 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │         \u001b[38;5;34m15\u001b[0m │ dropout_135[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_83 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ dense_125[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│                     │                   │            │ add_82[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │          \u001b[38;5;34m6\u001b[0m │ add_83[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │     \u001b[38;5;34m15,363\u001b[0m │ layer_normalizat… │\n",
       "│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_137         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ multi_head_atten… │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_84 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ dropout_137[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
       "│                     │                   │            │ add_83[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │          \u001b[38;5;34m6\u001b[0m │ add_84[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_126 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m4\u001b[0m)     │         \u001b[38;5;34m16\u001b[0m │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_138         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m4\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ dense_126[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_127 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │         \u001b[38;5;34m15\u001b[0m │ dropout_138[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_85 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ dense_127[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│                     │                   │            │ add_84[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │          \u001b[38;5;34m6\u001b[0m │ add_85[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │     \u001b[38;5;34m15,363\u001b[0m │ layer_normalizat… │\n",
       "│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_140         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ multi_head_atten… │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_86 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ dropout_140[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
       "│                     │                   │            │ add_85[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │          \u001b[38;5;34m6\u001b[0m │ add_86[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_128 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m4\u001b[0m)     │         \u001b[38;5;34m16\u001b[0m │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_141         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m4\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ dense_128[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_129 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │         \u001b[38;5;34m15\u001b[0m │ dropout_141[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_87 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ dense_129[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│                     │                   │            │ add_86[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ add_87[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_130 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │      \u001b[38;5;34m1,408\u001b[0m │ global_average_p… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_142         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_130[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_131 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)         │        \u001b[38;5;34m258\u001b[0m │ dropout_142[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">63,290</span> (247.23 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m63,290\u001b[0m (247.23 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">63,290</span> (247.23 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m63,290\u001b[0m (247.23 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_shape = (time_steps, len(features))\n",
    "model1 = build_model(\n",
    "    input_shape,\n",
    "    head_size=256,\n",
    "    num_heads=4,\n",
    "    ff_dim=4,\n",
    "    num_transformer_blocks=4,\n",
    "    mlp_units=[128],\n",
    "    mlp_dropout=0.4,\n",
    "    dropout=0.25,\n",
    ")\n",
    "\n",
    "model1.compile(\n",
    "    loss=\"mean_squared_error\",\n",
    "    optimizer=Adam(learning_rate=1e-4),\n",
    ")\n",
    "\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 28ms/step - loss: 1.2715 - val_loss: 0.3940\n",
      "Epoch 2/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 33ms/step - loss: 0.5235 - val_loss: 0.3054\n",
      "Epoch 3/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 37ms/step - loss: 0.4168 - val_loss: 0.2857\n",
      "Epoch 4/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 36ms/step - loss: 0.3855 - val_loss: 0.2628\n",
      "Epoch 5/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 36ms/step - loss: 0.3627 - val_loss: 0.2500\n",
      "Epoch 6/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 35ms/step - loss: 0.3399 - val_loss: 0.2453\n",
      "Epoch 7/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 34ms/step - loss: 0.3282 - val_loss: 0.2411\n",
      "Epoch 8/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 34ms/step - loss: 0.3115 - val_loss: 0.2320\n",
      "Epoch 9/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 33ms/step - loss: 0.3019 - val_loss: 0.2260\n",
      "Epoch 10/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 33ms/step - loss: 0.2885 - val_loss: 0.2225\n",
      "Epoch 11/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 33ms/step - loss: 0.2788 - val_loss: 0.2173\n",
      "Epoch 12/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 32ms/step - loss: 0.2733 - val_loss: 0.2124\n",
      "Epoch 13/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 33ms/step - loss: 0.2592 - val_loss: 0.2045\n",
      "Epoch 14/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 33ms/step - loss: 0.2618 - val_loss: 0.2014\n",
      "Epoch 15/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 32ms/step - loss: 0.2496 - val_loss: 0.1926\n",
      "Epoch 16/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 33ms/step - loss: 0.2489 - val_loss: 0.1867\n",
      "Epoch 17/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 32ms/step - loss: 0.2414 - val_loss: 0.1903\n",
      "Epoch 18/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 34ms/step - loss: 0.2380 - val_loss: 0.1824\n",
      "Epoch 19/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 33ms/step - loss: 0.2269 - val_loss: 0.1822\n",
      "Epoch 20/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 32ms/step - loss: 0.2303 - val_loss: 0.1813\n",
      "Epoch 21/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 33ms/step - loss: 0.2219 - val_loss: 0.1789\n",
      "Epoch 22/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 32ms/step - loss: 0.2206 - val_loss: 0.1834\n",
      "Epoch 23/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 33ms/step - loss: 0.2219 - val_loss: 0.1836\n",
      "Epoch 24/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 33ms/step - loss: 0.2168 - val_loss: 0.1766\n",
      "Epoch 25/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 33ms/step - loss: 0.2178 - val_loss: 0.1747\n",
      "Epoch 26/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 34ms/step - loss: 0.2173 - val_loss: 0.1786\n",
      "Epoch 27/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 33ms/step - loss: 0.2154 - val_loss: 0.1817\n",
      "Epoch 28/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 34ms/step - loss: 0.2124 - val_loss: 0.1789\n",
      "Epoch 29/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 33ms/step - loss: 0.2095 - val_loss: 0.1793\n",
      "Epoch 30/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 33ms/step - loss: 0.2098 - val_loss: 0.1766\n",
      "Epoch 31/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 33ms/step - loss: 0.2099 - val_loss: 0.1706\n",
      "Epoch 32/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 32ms/step - loss: 0.2053 - val_loss: 0.1697\n",
      "Epoch 33/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 33ms/step - loss: 0.2063 - val_loss: 0.1710\n",
      "Epoch 34/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 33ms/step - loss: 0.2048 - val_loss: 0.1746\n",
      "Epoch 35/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 33ms/step - loss: 0.2116 - val_loss: 0.1730\n",
      "Epoch 36/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 34ms/step - loss: 0.2034 - val_loss: 0.1719\n",
      "Epoch 37/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 33ms/step - loss: 0.2056 - val_loss: 0.1719\n",
      "Epoch 38/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 34ms/step - loss: 0.1982 - val_loss: 0.1706\n",
      "Epoch 39/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 34ms/step - loss: 0.2096 - val_loss: 0.1706\n",
      "Epoch 40/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 33ms/step - loss: 0.1996 - val_loss: 0.1685\n",
      "Epoch 41/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 34ms/step - loss: 0.2009 - val_loss: 0.1678\n",
      "Epoch 42/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 33ms/step - loss: 0.1986 - val_loss: 0.1692\n",
      "Epoch 43/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 33ms/step - loss: 0.1952 - val_loss: 0.1681\n",
      "Epoch 44/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 33ms/step - loss: 0.1991 - val_loss: 0.1689\n",
      "Epoch 45/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 33ms/step - loss: 0.1950 - val_loss: 0.1725\n",
      "Epoch 46/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 34ms/step - loss: 0.1985 - val_loss: 0.1713\n",
      "Epoch 47/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 33ms/step - loss: 0.2026 - val_loss: 0.1742\n",
      "Epoch 48/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 33ms/step - loss: 0.1944 - val_loss: 0.1702\n",
      "Epoch 49/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 33ms/step - loss: 0.1965 - val_loss: 0.1688\n",
      "Epoch 50/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 34ms/step - loss: 0.1976 - val_loss: 0.1706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "history1 = model1.fit(\n",
    "    X_train_rnn, y_train_rnn,\n",
    "    validation_data=(X_val_rnn, y_val_rnn),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    ")\n",
    "\n",
    "model1.save('transformer_weather_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step\n",
      "Fold 1: MSE max_temp=0.1827615128405319, min_temp=0.21031718496411161\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n",
      "Fold 2: MSE max_temp=0.17088210596416128, min_temp=0.17190250803680887\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n",
      "Fold 3: MSE max_temp=0.21241883358609057, min_temp=0.1652696181228999\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step\n",
      "Fold 4: MSE max_temp=0.1450262004862647, min_temp=0.19370179342977126\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n",
      "Fold 5: MSE max_temp=0.17397028232237421, min_temp=0.1992945693497248\n",
      "Average MSE max_temp: 0.17701178703988454\n",
      "Average MSE min_temp: 0.18809713478066328\n",
      "Average RMSE max_temp: 0.4199389455304131\n",
      "Average RMSE min_temp: 0.43325766274814415\n",
      "Average MAE max_temp: 0.32972836484047174\n",
      "Average MAE min_temp: 0.35247189859461725\n",
      "Average R² max_temp: 0.8190828364478548\n",
      "Average R² min_temp: 0.8057290180747019\n"
     ]
    }
   ],
   "source": [
    "# cross validation\n",
    "n_splits = 5\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "results = {'mse_max': [], 'mse_min': [], 'rmse_max': [], 'rmse_min': [], 'mae_max': [], 'mae_min': [], 'r2_max': [], 'r2_min': []}\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(tscv.split(X)):\n",
    "    X_train, X_val = X[train_index], X[val_index]\n",
    "    y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "    time_steps = 10\n",
    "    X_train_rnn, y_train_rnn = create_time_series(X_train, y_train, time_steps)\n",
    "    X_val_rnn, y_val_rnn = create_time_series(X_val, y_val, time_steps)\n",
    "\n",
    "    model = build_model(\n",
    "        input_shape=(time_steps, X_train_rnn.shape[2]),\n",
    "        head_size=256,\n",
    "        num_heads=4,\n",
    "        ff_dim=4,\n",
    "        num_transformer_blocks=4,\n",
    "        mlp_units=[128],\n",
    "        mlp_dropout=0.4,\n",
    "        dropout=0.25,\n",
    "    )\n",
    "    model.compile(\n",
    "        loss=\"mean_squared_error\",\n",
    "        optimizer=Adam(learning_rate=1e-4),\n",
    "    )\n",
    "    model.fit(X_train_rnn, y_train_rnn, epochs=50, batch_size=32, validation_data=(X_val_rnn, y_val_rnn), verbose=0)\n",
    "\n",
    "    y_val_pred = model.predict(X_val_rnn)\n",
    "    \n",
    "    mse_max = mean_squared_error(y_val_rnn[:, 0], y_val_pred[:, 0])\n",
    "    mse_min = mean_squared_error(y_val_rnn[:, 1], y_val_pred[:, 1])\n",
    "    rmse_max = np.sqrt(mse_max)\n",
    "    rmse_min = np.sqrt(mse_min)\n",
    "    mae_max = mean_absolute_error(y_val_rnn[:, 0], y_val_pred[:, 0])\n",
    "    mae_min = mean_absolute_error(y_val_rnn[:, 1], y_val_pred[:, 1])\n",
    "    r2_max = r2_score(y_val_rnn[:, 0], y_val_pred[:, 0])\n",
    "    r2_min = r2_score(y_val_rnn[:, 1], y_val_pred[:, 1])\n",
    "    \n",
    "    results['mse_max'].append(mse_max)\n",
    "    results['mse_min'].append(mse_min)\n",
    "    results['rmse_max'].append(rmse_max)\n",
    "    results['rmse_min'].append(rmse_min)\n",
    "    results['mae_max'].append(mae_max)\n",
    "    results['mae_min'].append(mae_min)\n",
    "    results['r2_max'].append(r2_max)\n",
    "    results['r2_min'].append(r2_min)\n",
    "\n",
    "print(f'Average MSE max_temp: {np.mean(results[\"mse_max\"])}')\n",
    "print(f'Average MSE min_temp: {np.mean(results[\"mse_min\"])}')\n",
    "print(f'Average RMSE max_temp: {np.mean(results[\"rmse_max\"])}')\n",
    "print(f'Average RMSE min_temp: {np.mean(results[\"rmse_min\"])}')\n",
    "print(f'Average MAE max_temp: {np.mean(results[\"mae_max\"])}')\n",
    "print(f'Average MAE min_temp: {np.mean(results[\"mae_min\"])}')\n",
    "print(f'Average R² max_temp: {np.mean(results[\"r2_max\"])}')\n",
    "print(f'Average R² min_temp: {np.mean(results[\"r2_min\"])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step\n",
      "RMSE of 'max_temp' in test set: 0.4010163099390085\n",
      "RMSE of 'min_temp' in test set: 0.40192748090429864\n",
      "MSE of 'max_temp' in test set: 0.16081408083709892\n",
      "MSE of 'min_temp' in test set: 0.16154569990607534\n",
      "MAE of 'max_temp' in test set: 0.30369115058371127\n",
      "MAE of 'min_temp' in test set: 0.3181266719998786\n",
      "R² of 'max_temp' in test set: 0.8378234034032515\n",
      "R² of 'min_temp' in test set: 0.8295135125693893\n"
     ]
    }
   ],
   "source": [
    "# evaludate\n",
    "y_val_pred_t1 = model1.predict(X_val_rnn)\n",
    "y_test_pred_t1 = model1.predict(X_test_rnn)\n",
    "\n",
    "# RMSE\n",
    "rmse_max_t1 = np.sqrt(mean_squared_error(y_test_rnn[:, 0], y_test_pred_rnn[:, 0]))\n",
    "rmse_min_t1 = np.sqrt(mean_squared_error(y_test_rnn[:, 1], y_test_pred_rnn[:, 1]))\n",
    "\n",
    "# MSE\n",
    "mse_max_t1 = mean_squared_error(y_test_rnn[:, 0], y_test_pred_rnn[:, 0])\n",
    "mse_min_t1 = mean_squared_error(y_test_rnn[:, 1], y_test_pred_rnn[:, 1])\n",
    "\n",
    "# MAE\n",
    "mae_max_t1 = mean_absolute_error(y_test_rnn[:, 0], y_test_pred_rnn[:, 0])\n",
    "mae_min_t1 = mean_absolute_error(y_test_rnn[:, 1], y_test_pred_rnn[:, 1])\n",
    "\n",
    "# R²\n",
    "r2_max_t1 = r2_score(y_test_rnn[:, 0], y_test_pred_rnn[:, 0])\n",
    "r2_min_t1 = r2_score(y_test_rnn[:, 1], y_test_pred_rnn[:, 1])\n",
    "\n",
    "print(f\"RMSE of 'max_temp' in test set: {rmse_max_t1}\")\n",
    "print(f\"RMSE of 'min_temp' in test set: {rmse_min_t1}\")\n",
    "\n",
    "print(f\"MSE of 'max_temp' in test set: {mse_max_t1}\")\n",
    "print(f\"MSE of 'min_temp' in test set: {mse_min_t1}\")\n",
    "\n",
    "print(f\"MAE of 'max_temp' in test set: {mae_max_t1}\")\n",
    "print(f\"MAE of 'min_temp' in test set: {mae_min_t1}\")\n",
    "\n",
    "print(f\"R² of 'max_temp' in test set: {r2_max_t1}\")\n",
    "print(f\"R² of 'min_temp' in test set: {r2_min_t1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    Ⅱ.Based on the weather of the past ten days, predict the highest and lowest temperatures for the next three days\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Baseline model - Moving average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ma_forecast_10(history_10, forecast_horizon_10):\n",
    "    max_temp_forecast_10 = history_10['max_temp'].rolling(window=10).mean().iloc[-1]\n",
    "    min_temp_forecast_10 = history_10['min_temp'].rolling(window=10).mean().iloc[-1]\n",
    "    \n",
    "    return [max_temp_forecast_10] * forecast_horizon_10, [min_temp_forecast_10] * forecast_horizon_10\n",
    "\n",
    "def ma_forecast_10test(test_data, forecast_horizon_10):\n",
    "    history_10 = test_data[['max_temp', 'min_temp']].copy()\n",
    "    max_temp_forecast_10 = history_10['max_temp'].rolling(window=10).mean().iloc[-1]\n",
    "    min_temp_forecast_10 = history_10['min_temp'].rolling(window=10).mean().iloc[-1]\n",
    "    return [max_temp_forecast_10] * forecast_horizon_10, [min_temp_forecast_10] * forecast_horizon_10\n",
    "\n",
    "test_history = test_df_weather[['max_temp', 'min_temp']]\n",
    "\n",
    "# Forecast maximum and minimum temperatures for the next three days\n",
    "forecast_horizon_10 = 3\n",
    "max_temp_forecast_10, min_temp_forecast_10 = ma_forecast_10test(test_history, forecast_horizon_10)\n",
    "\n",
    "# Expanded predicted value\n",
    "predicted_max_temp_10ma = np.array(max_temp_forecast_10).flatten()[:len(test_history)]\n",
    "predicted_min_temp_10ma = np.array(min_temp_forecast_10).flatten()[:len(test_history)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Temperature - RMSE: 2.888288310378546 MSE: 8.342209363869356 R²: -246.45368865030724 MAE: 2.882446384468892\n",
      "Min Temperature - RMSE: 2.062940241862688 MSE: 4.255722441496485 R²: -224.42989626555956 MAE: 2.058359586969246\n"
     ]
    }
   ],
   "source": [
    "# Ensure consistent length\n",
    "if len(predicted_max_temp_10ma) != len(test_history) or len(predicted_min_temp_10ma) != len(test_history):\n",
    "    min_length = min(len(predicted_max_temp_10ma), len(predicted_min_temp_10ma), len(test_history))\n",
    "    predicted_max_temp_10ma = predicted_max_temp_10ma[:min_length]\n",
    "    predicted_min_temp_10ma = predicted_min_temp_10ma[:min_length]\n",
    "    y_test_actual_10ma = test_df_weather[['max_temp', 'min_temp']].iloc[:min_length]\n",
    "\n",
    "# evaludate\n",
    "y_test_actual_10ma = y_test_actual_10ma.copy()\n",
    "\n",
    "# Calculation index\n",
    "def calculate_metrics(true_values_10ma, predicted_values_10ma):\n",
    "    mse = mean_squared_error(true_values_10ma, predicted_values_10ma)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(true_values_10ma, predicted_values_10ma)\n",
    "    r2 = r2_score(true_values_10ma, predicted_values_10ma)\n",
    "    return rmse, mse, r2, mae\n",
    "\n",
    "rmse_max_temp_10ma, mse_max_temp_10ma, r2_max_temp_10ma, mae_max_temp_10ma = calculate_metrics(y_test_actual_10ma['max_temp'], predicted_max_temp_10ma)\n",
    "rmse_min_temp_10ma, mse_min_temp_10ma, r2_min_temp_10ma, mae_min_temp_10ma = calculate_metrics(y_test_actual_10ma['min_temp'], predicted_min_temp_10ma)\n",
    "\n",
    "print(\"Max Temperature - RMSE:\", rmse_max_temp_10ma, \"MSE:\", mse_max_temp_10ma, \"R²:\", r2_max_temp_10ma, \"MAE:\", mae_max_temp_10ma)\n",
    "print(\"Min Temperature - RMSE:\", rmse_min_temp_10ma, \"MSE:\", mse_min_temp_10ma, \"R²:\", r2_min_temp_10ma, \"MAE:\", mae_min_temp_10ma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation results:\n",
      "Average MSE of 'max_temp': 2.57\n",
      "Average MSE of 'min_temp': 2.47\n",
      "Average RMSE of 'max_temp': 1.58\n",
      "Average RMSE of 'min_temp': 1.54\n",
      "Average MAE of 'max_temp': 1.32\n",
      "Average MAE of 'min_temp': 1.31\n",
      "Average R² of 'max_temp': -1.63\n",
      "Average R² of 'min_temp': -1.55\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "def ma_forecast_10(history_10, forecast_horizon_10=3):\n",
    "    max_temp_forecast_10 = history_10['max_temp'].rolling(window=10).mean().iloc[-1]\n",
    "    min_temp_forecast_10 = history_10['min_temp'].rolling(window=10).mean().iloc[-1]\n",
    "    return [max_temp_forecast_10] * forecast_horizon_10, [min_temp_forecast_10] * forecast_horizon_10\n",
    "\n",
    "# TimeSeriesSplit cross validation\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "mse_scores_max = []\n",
    "mse_scores_min = []\n",
    "rmse_scores_max = []\n",
    "rmse_scores_min = []\n",
    "mae_scores_max = []\n",
    "mae_scores_min = []\n",
    "r2_scores_max = []\n",
    "r2_scores_min = []\n",
    "\n",
    "for train_index, val_index in tscv.split(df_weather):\n",
    "\n",
    "    train_df = df_weather.iloc[train_index]\n",
    "    val_df = df_weather.iloc[val_index]\n",
    "    \n",
    "    # If the number of predicted days is fixed at 3, the data effect is very poor, and the effect is not good now\n",
    "    forecast_horizon = len(val_df)\n",
    "    \n",
    "    max_temp_pred, min_temp_pred = ma_forecast_10(train_df, forecast_horizon)\n",
    "    \n",
    "    # Get the true value of the validation set, only the data from the previous 3 days\n",
    "    y_val_max = val_df['max_temp'].iloc[:forecast_horizon].values\n",
    "    y_val_min = val_df['min_temp'].iloc[:forecast_horizon].values\n",
    "    \n",
    "    # Calculate and store each score indicator\n",
    "    mse_max = mean_squared_error(y_val_max, max_temp_pred)\n",
    "    mse_min = mean_squared_error(y_val_min, min_temp_pred)\n",
    "    rmse_max = np.sqrt(mse_max)\n",
    "    rmse_min = np.sqrt(mse_min)\n",
    "    mae_max = mean_absolute_error(y_val_max, max_temp_pred)\n",
    "    mae_min = mean_absolute_error(y_val_min, min_temp_pred)\n",
    "    r2_max = r2_score(y_val_max, max_temp_pred)\n",
    "    r2_min = r2_score(y_val_min, min_temp_pred)\n",
    "    \n",
    "    mse_scores_max.append(mse_max)\n",
    "    mse_scores_min.append(mse_min)\n",
    "    rmse_scores_max.append(rmse_max)\n",
    "    rmse_scores_min.append(rmse_min)\n",
    "    mae_scores_max.append(mae_max)\n",
    "    mae_scores_min.append(mae_min)\n",
    "    r2_scores_max.append(r2_max)\n",
    "    r2_scores_min.append(r2_min)\n",
    "\n",
    "print(\"Cross-validation results:\")\n",
    "print(f\"Average MSE of 'max_temp': {np.mean(mse_scores_max):.2f}\")\n",
    "print(f\"Average MSE of 'min_temp': {np.mean(mse_scores_min):.2f}\")\n",
    "print(f\"Average RMSE of 'max_temp': {np.mean(rmse_scores_max):.2f}\")\n",
    "print(f\"Average RMSE of 'min_temp': {np.mean(rmse_scores_min):.2f}\")\n",
    "print(f\"Average MAE of 'max_temp': {np.mean(mae_scores_max):.2f}\")\n",
    "print(f\"Average MAE of 'min_temp': {np.mean(mae_scores_min):.2f}\")\n",
    "print(f\"Average R² of 'max_temp': {np.mean(r2_scores_max):.2f}\")\n",
    "print(f\"Average R² of 'min_temp': {np.mean(r2_scores_min):.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Random Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features_and_target_10(df, n_days=10, forecast_days=3):\n",
    "    df = df.copy()\n",
    "    for i in range(1, n_days + 1):\n",
    "        df[f'global_radiation_{i}d'] = df['global_radiation'].shift(i)\n",
    "        df[f'diff_temp_{i}d'] = df['diff_temp'].shift(i)\n",
    "        df[f'mean_temp_{i}d'] = df['mean_temp'].shift(i)\n",
    "    \n",
    "    df = df.dropna().reset_index(drop=True)\n",
    "    X = df[[f'global_radiation_{i}d' for i in range(1, n_days + 1)] +\n",
    "           [f'diff_temp_{i}d' for i in range(1, n_days + 1)] +\n",
    "           [f'mean_temp_{i}d' for i in range(1, n_days + 1)]]\n",
    "    y_max = []\n",
    "    y_min = []\n",
    "    for i in range(len(df) - forecast_days + 1):\n",
    "        y_max.append(df['max_temp'].iloc[i:i+forecast_days].values)\n",
    "        y_min.append(df['min_temp'].iloc[i:i+forecast_days].values)\n",
    "    \n",
    "    return X[:-forecast_days + 1], np.array(y_max), np.array(y_min)\n",
    "\n",
    "X_train_rf10, y_train_max_rf10, y_train_min_rf10 = create_features_and_target(train_df_weather)\n",
    "X_val_rf10, y_val_max_rf10, y_val_min_rf10 = create_features_and_target(val_df_weather)\n",
    "X_test_rf10, y_test_max_rf10, y_test_min_rf10 = create_features_and_target(test_df_weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_max10 = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_max10.fit(X_train_rf10, y_train_max_rf10)\n",
    "\n",
    "rf_min10 = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_min10.fit(X_train_rf10, y_train_min_rf10)\n",
    "\n",
    "y_val_pred_max_rf10 = rf_max10.predict(X_val_rf10)\n",
    "y_val_pred_min_rf10 = rf_min10.predict(X_val_rf10)\n",
    "\n",
    "y_test_pred_max_rf10 = rf_max10.predict(X_test_rf10)\n",
    "y_test_pred_min_rf10 = rf_min10.predict(X_test_rf10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9n/bcfhtg5541d9rk5jnjx5j2w80000gn/T/ipykernel_94287/1401294683.py:2: FutureWarning: Series.ravel is deprecated. The underlying array is already 1D, so ravel is not necessary.  Use `to_numpy()` for conversion to a numpy array instead.\n",
      "  y_test_max_flat = y_test_max_rf10.ravel()\n",
      "/var/folders/9n/bcfhtg5541d9rk5jnjx5j2w80000gn/T/ipykernel_94287/1401294683.py:3: FutureWarning: Series.ravel is deprecated. The underlying array is already 1D, so ravel is not necessary.  Use `to_numpy()` for conversion to a numpy array instead.\n",
      "  y_test_min_flat = y_test_min_rf10.ravel()\n"
     ]
    }
   ],
   "source": [
    "# The forecast results are flattened into a one-dimensional array\n",
    "y_test_max_flat = y_test_max_rf10.ravel()\n",
    "y_test_min_flat = y_test_min_rf10.ravel()\n",
    "y_test_pred_max_flat = y_test_pred_max_rf10.ravel()\n",
    "y_test_pred_min_flat = y_test_pred_min_rf10.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE of 'max_temp' in test set: 0.4041829141188286\n",
      "RMSE of 'min_temp' in test set: 0.39214930819146954\n",
      "MSE of 'max_temp' in test set: 0.1633638280655884\n",
      "MSE of 'min_temp' in test set: 0.15378107991504816\n",
      "MAE of 'max_temp' in test set: 0.3105157689668827\n",
      "MAE of 'min_temp' in test set: 0.3153303253808187\n",
      "R² of 'max_temp' in test set: 0.8352520531486846\n",
      "R² of 'min_temp' in test set: 0.837707867413086\n"
     ]
    }
   ],
   "source": [
    "# evaluate\n",
    "\n",
    "# RMSE\n",
    "rmse_max_rf10 = np.sqrt(mean_squared_error(y_test_max_rf10, y_test_pred_max_rf10))\n",
    "rmse_min_rf10 = np.sqrt(mean_squared_error(y_test_min_rf10, y_test_pred_min_rf10))\n",
    "\n",
    "# MSE\n",
    "mse_max_rf10 = mean_squared_error(y_test_max_rf10, y_test_pred_max_rf10)\n",
    "mse_min_rf10 = mean_squared_error(y_test_min_rf10, y_test_pred_min_rf10)\n",
    "\n",
    "# MAE\n",
    "mae_max_rf10 = mean_absolute_error(y_test_max_rf10, y_test_pred_max_rf10)\n",
    "mae_min_rf10 = mean_absolute_error(y_test_min_rf10, y_test_pred_min_rf10)\n",
    "\n",
    "# R²\n",
    "r2_max_rf10 = r2_score(y_test_max_rf10, y_test_pred_max_rf10)\n",
    "r2_min_rf10 = r2_score(y_test_min_rf10, y_test_pred_min_rf10)\n",
    "\n",
    "print(f\"RMSE of 'max_temp' in test set: {rmse_max_rf10}\")\n",
    "print(f\"RMSE of 'min_temp' in test set: {rmse_min_rf10}\")\n",
    "\n",
    "print(f\"MSE of 'max_temp' in test set: {mse_max_rf10}\")\n",
    "print(f\"MSE of 'min_temp' in test set: {mse_min_rf10}\")\n",
    "\n",
    "print(f\"MAE of 'max_temp' in test set: {mae_max_rf10}\")\n",
    "print(f\"MAE of 'min_temp' in test set: {mae_min_rf10}\")\n",
    "\n",
    "print(f\"R² of 'max_temp' in test set: {r2_max_rf10}\")\n",
    "print(f\"R² of 'min_temp' in test set: {r2_min_rf10}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation results:\n",
      "Average MSE of 'max_temp': 0.20\n",
      "Average MSE of 'min_temp': 0.21\n",
      "Average RMSE of 'max_temp': 0.45\n",
      "Average RMSE of 'min_temp': 0.46\n",
      "Average MAE of 'max_temp': 0.35\n",
      "Average MAE of 'min_temp': 0.37\n",
      "Average R² of 'max_temp': 0.79\n",
      "Average R² of 'min_temp': 0.78\n"
     ]
    }
   ],
   "source": [
    "# cross validation\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "X, y_max, y_min = create_features_and_target_10(df_weather)\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "mse_scores_max = []\n",
    "mse_scores_min = []\n",
    "rmse_scores_max = []\n",
    "rmse_scores_min = []\n",
    "mae_scores_max = []\n",
    "mae_scores_min = []\n",
    "r2_scores_max = []\n",
    "r2_scores_min = []\n",
    "\n",
    "\n",
    "for train_index, val_index in tscv.split(X):\n",
    "\n",
    "    X_train_cv, X_val_cv = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_train_max_cv, y_val_max_cv = y_max[train_index], y_max[val_index]\n",
    "    y_train_min_cv, y_val_min_cv = y_min[train_index], y_min[val_index]\n",
    "    \n",
    "    rf_max = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    rf_max.fit(X_train_cv, y_train_max_cv)\n",
    "    \n",
    "    rf_min = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    rf_min.fit(X_train_cv, y_train_min_cv)\n",
    "    \n",
    "    y_val_pred_max_rf = rf_max.predict(X_val_cv)\n",
    "    y_val_pred_min_rf = rf_min.predict(X_val_cv)\n",
    "    \n",
    "    mse_max = mean_squared_error(y_val_max_cv, y_val_pred_max_rf)\n",
    "    mse_min = mean_squared_error(y_val_min_cv, y_val_pred_min_rf)\n",
    "    rmse_max = np.sqrt(mse_max)\n",
    "    rmse_min = np.sqrt(mse_min)\n",
    "    mae_max = mean_absolute_error(y_val_max_cv, y_val_pred_max_rf)\n",
    "    mae_min = mean_absolute_error(y_val_min_cv, y_val_pred_min_rf)\n",
    "    r2_max = r2_score(y_val_max_cv, y_val_pred_max_rf)\n",
    "    r2_min = r2_score(y_val_min_cv, y_val_pred_min_rf)\n",
    "    \n",
    "    mse_scores_max.append(mse_max)\n",
    "    mse_scores_min.append(mse_min)\n",
    "    rmse_scores_max.append(rmse_max)\n",
    "    rmse_scores_min.append(rmse_min)\n",
    "    mae_scores_max.append(mae_max)\n",
    "    mae_scores_min.append(mae_min)\n",
    "    r2_scores_max.append(r2_max)\n",
    "    r2_scores_min.append(r2_min)\n",
    "\n",
    "print(\"Cross-validation results:\")\n",
    "print(f\"Average MSE of 'max_temp': {np.mean(mse_scores_max):.2f}\")\n",
    "print(f\"Average MSE of 'min_temp': {np.mean(mse_scores_min):.2f}\")\n",
    "print(f\"Average RMSE of 'max_temp': {np.mean(rmse_scores_max):.2f}\")\n",
    "print(f\"Average RMSE of 'min_temp': {np.mean(rmse_scores_min):.2f}\")\n",
    "print(f\"Average MAE of 'max_temp': {np.mean(mae_scores_max):.2f}\")\n",
    "print(f\"Average MAE of 'min_temp': {np.mean(mae_scores_min):.2f}\")\n",
    "print(f\"Average R² of 'max_temp': {np.mean(r2_scores_max):.2f}\")\n",
    "print(f\"Average R² of 'min_temp': {np.mean(r2_scores_min):.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the training set: (9193, 10, 3) (9193, 3, 2)\n",
      "Size of the validation set: (3056, 10, 3) (3056, 3, 2)\n",
      "Size of the testing set: (3056, 10, 3) (3056, 3, 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def create_time_series10(X, y, input_steps=10, output_steps=3):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - input_steps - output_steps + 1):\n",
    "        Xs.append(X[i:(i + input_steps)])\n",
    "        ys.append(y[(i + input_steps):(i + input_steps + output_steps)])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "  \n",
    "input_steps = 10\n",
    "output_steps = 3\n",
    "\n",
    "X_train_rnn10, y_train_rnn10 = create_time_series10(train_X_rnn, train_y_rnn, input_steps, output_steps)\n",
    "X_val_rnn10, y_val_rnn10 = create_time_series10(val_X_rnn, val_y_rnn, input_steps, output_steps)\n",
    "X_test_rnn10, y_test_rnn10 = create_time_series10(test_X_rnn, test_y_rnn, input_steps, output_steps)\n",
    "\n",
    "print(\"Size of the training set:\", X_train_rnn10.shape, y_train_rnn10.shape)\n",
    "print(\"Size of the validation set:\", X_val_rnn10.shape, y_val_rnn10.shape)\n",
    "print(\"Size of the testing set:\", X_test_rnn10.shape, y_test_rnn10.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_22\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_22\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">10,800</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_132 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">306</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_22 (\u001b[38;5;33mLSTM\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │        \u001b[38;5;34m10,800\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_132 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)              │           \u001b[38;5;34m306\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">11,106</span> (43.38 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m11,106\u001b[0m (43.38 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">11,106</span> (43.38 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m11,106\u001b[0m (43.38 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model_rnn10 = Sequential()\n",
    "model_rnn10.add(LSTM(50, input_shape=(input_steps, len(features)), return_sequences=False))\n",
    "model_rnn10.add(Dense(output_steps * len(target)))\n",
    "model_rnn10.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "\n",
    "model_rnn10.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.3645 - val_loss: 0.2218\n",
      "Epoch 2/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2162 - val_loss: 0.2087\n",
      "Epoch 3/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2012 - val_loss: 0.2009\n",
      "Epoch 4/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1946 - val_loss: 0.1959\n",
      "Epoch 5/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1907 - val_loss: 0.1962\n",
      "Epoch 6/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1886 - val_loss: 0.1949\n",
      "Epoch 7/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1872 - val_loss: 0.1962\n",
      "Epoch 8/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1872 - val_loss: 0.1973\n",
      "Epoch 9/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1845 - val_loss: 0.2015\n",
      "Epoch 10/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1844 - val_loss: 0.2013\n",
      "Epoch 11/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1830 - val_loss: 0.1993\n",
      "Epoch 12/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1831 - val_loss: 0.2027\n",
      "Epoch 13/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1860 - val_loss: 0.2032\n",
      "Epoch 14/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1823 - val_loss: 0.2073\n",
      "Epoch 15/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1830 - val_loss: 0.2064\n",
      "Epoch 16/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1875 - val_loss: 0.2103\n",
      "Epoch 17/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1832 - val_loss: 0.2053\n",
      "Epoch 18/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1836 - val_loss: 0.2112\n",
      "Epoch 19/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1832 - val_loss: 0.2085\n",
      "Epoch 20/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1826 - val_loss: 0.2085\n",
      "Epoch 21/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1833 - val_loss: 0.2100\n",
      "Epoch 22/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1797 - val_loss: 0.2095\n",
      "Epoch 23/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1812 - val_loss: 0.2133\n",
      "Epoch 24/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1824 - val_loss: 0.2091\n",
      "Epoch 25/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1833 - val_loss: 0.2102\n",
      "Epoch 26/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1813 - val_loss: 0.2144\n",
      "Epoch 27/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1843 - val_loss: 0.2116\n",
      "Epoch 28/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1810 - val_loss: 0.2133\n",
      "Epoch 29/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1804 - val_loss: 0.2110\n",
      "Epoch 30/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1805 - val_loss: 0.2147\n",
      "Epoch 31/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1833 - val_loss: 0.2133\n",
      "Epoch 32/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1780 - val_loss: 0.2160\n",
      "Epoch 33/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1812 - val_loss: 0.2184\n",
      "Epoch 34/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1792 - val_loss: 0.2158\n",
      "Epoch 35/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1802 - val_loss: 0.2179\n",
      "Epoch 36/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1787 - val_loss: 0.2207\n",
      "Epoch 37/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1785 - val_loss: 0.2167\n",
      "Epoch 38/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1784 - val_loss: 0.2168\n",
      "Epoch 39/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1783 - val_loss: 0.2195\n",
      "Epoch 40/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1783 - val_loss: 0.2213\n",
      "Epoch 41/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1775 - val_loss: 0.2211\n",
      "Epoch 42/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1794 - val_loss: 0.2189\n",
      "Epoch 43/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1768 - val_loss: 0.2159\n",
      "Epoch 44/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1780 - val_loss: 0.2203\n",
      "Epoch 45/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1766 - val_loss: 0.2238\n",
      "Epoch 46/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1777 - val_loss: 0.2205\n",
      "Epoch 47/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1753 - val_loss: 0.2197\n",
      "Epoch 48/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1763 - val_loss: 0.2202\n",
      "Epoch 49/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1755 - val_loss: 0.2194\n",
      "Epoch 50/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1743 - val_loss: 0.2230\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "history10 = model_rnn10.fit(X_train_rnn10, y_train_rnn10.reshape((y_train_rnn10.shape[0], output_steps * len(target))),\n",
    "                    epochs=50,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(X_val_rnn10, y_val_rnn10.reshape((y_val_rnn10.shape[0], output_steps * len(target)))),\n",
    "                    verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 983us/step - loss: 0.1857\n",
      "The loss in testing set:  0.2147047072649002\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# evaluate\n",
    "loss_rnn10 = model_rnn10.evaluate(X_test_rnn10, y_test_rnn10.reshape((y_test_rnn10.shape[0], output_steps * len(target))))\n",
    "print(\"The loss in testing set: \", loss_rnn10)\n",
    "\n",
    "plt.plot(history10.history['loss'], label='train loss')\n",
    "plt.plot(history.history['val_loss'], label='val loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "y_test_rnn10 shape: (3056, 3, 2)\n",
      "y_test_pred_rnn10 shape: (3056, 6)\n",
      "y_test_rnn10_flat shape: (3056, 6)\n"
     ]
    }
   ],
   "source": [
    "# evaluate\n",
    "y_val_pred_rnn10 = model_rnn10.predict(X_val_rnn10)\n",
    "y_test_pred_rnn10 = model_rnn10.predict(X_test_rnn10)\n",
    "\n",
    "print(f'y_test_rnn10 shape: {y_test_rnn10.shape}')\n",
    "print(f'y_test_pred_rnn10 shape: {y_test_pred_rnn10.shape}')\n",
    "\n",
    "# Flattening y_test_rnn10 to match the shape of y_test_pred_rnn10\n",
    "# the shape of y_test_rnn10 from (3056, 3, 2) to (3056, 6)\n",
    "y_test_rnn10_flat = y_test_rnn10.reshape(3056, -1)\n",
    "\n",
    "# make sure y_test_rnn10_flat and y_test_pred_rnn10 have same shape\n",
    "assert y_test_rnn10_flat.shape == y_test_pred_rnn10.shape, \"Shapes do not match!\"\n",
    "\n",
    "print(f'y_test_rnn10_flat shape: {y_test_rnn10_flat.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE of 'max_temp' in test set: 0.40297232113327\n",
      "RMSE of 'min_temp' in test set: 0.3887022280456923\n",
      "MSE of 'max_temp' in test set: 0.16238669159953525\n",
      "MSE of 'min_temp' in test set: 0.1510894220876854\n",
      "MAE of 'max_temp' in test set: 0.30529344984909623\n",
      "MAE of 'min_temp' in test set: 0.31001912385337244\n",
      "R² of 'max_temp' in test set: 0.8359384549668885\n",
      "R² of 'min_temp' in test set: 0.8402762425696243\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# MSE\n",
    "mse_max_rnn10 = mean_squared_error(y_test_rnn10_flat[:, 0], y_test_pred_rnn10[:, 0])\n",
    "mse_min_rnn10 = mean_squared_error(y_test_rnn10_flat[:, 1].flatten(), y_test_pred_rnn10[:, 1].flatten())\n",
    "\n",
    "# MAE\n",
    "mae_max_rnn10 = mean_absolute_error(y_test_rnn10_flat[:, 0].flatten(), y_test_pred_rnn10[:, 0].flatten())\n",
    "mae_min_rnn10 = mean_absolute_error(y_test_rnn10_flat[:, 1].flatten(), y_test_pred_rnn10[:, 1].flatten())\n",
    "\n",
    "# R²\n",
    "r2_max_rnn10 = r2_score(y_test_rnn10_flat[:, 0].flatten(), y_test_pred_rnn10[:, 0].flatten())\n",
    "r2_min_rnn10 = r2_score(y_test_rnn10_flat[:, 1].flatten(), y_test_pred_rnn10[:, 1].flatten())\n",
    "\n",
    "# RMSE\n",
    "rmse_max_rnn10 = np.sqrt(mse_max_rnn10)\n",
    "rmse_min_rnn10 = np.sqrt(mse_min_rnn10)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"RMSE of 'max_temp' in test set: {rmse_max_rnn10}\")\n",
    "print(f\"RMSE of 'min_temp' in test set: {rmse_min_rnn10}\")\n",
    "\n",
    "print(f\"MSE of 'max_temp' in test set: {mse_max_rnn10}\")\n",
    "print(f\"MSE of 'min_temp' in test set: {mse_min_rnn10}\")\n",
    "\n",
    "print(f\"MAE of 'max_temp' in test set: {mae_max_rnn10}\")\n",
    "print(f\"MAE of 'min_temp' in test set: {mae_min_rnn10}\")\n",
    "\n",
    "print(f\"R² of 'max_temp' in test set: {r2_max_rnn10}\")\n",
    "print(f\"R² of 'min_temp' in test set: {r2_min_rnn10}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 903us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/opt/anaconda3/lib/python3.9/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/opt/anaconda3/lib/python3.9/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/opt/anaconda3/lib/python3.9/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/opt/anaconda3/lib/python3.9/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/opt/anaconda3/lib/python3.9/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/opt/anaconda3/lib/python3.9/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "/opt/anaconda3/lib/python3.9/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "/opt/anaconda3/lib/python3.9/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "/opt/anaconda3/lib/python3.9/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "/opt/anaconda3/lib/python3.9/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "/opt/anaconda3/lib/python3.9/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "/opt/anaconda3/lib/python3.9/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/opt/anaconda3/lib/python3.9/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "/opt/anaconda3/lib/python3.9/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/opt/anaconda3/lib/python3.9/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step \n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step \n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step  \n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step  \n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step  \n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step \n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step  \n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step   \n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step   \n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 977us/step\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 976us/step\n",
      "Feature: global_radiation, Importance: 0.0036230334292143896\n",
      "Feature: diff_temp, Importance: 0.0008676244876378913\n",
      "Feature: mean_temp, Importance: 0.0028886890251751786\n"
     ]
    }
   ],
   "source": [
    "# Permutation Feature Importance\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "X_test_rnn10_reshaped = X_test_rnn10.reshape(X_test_rnn10.shape[0], -1)\n",
    "\n",
    "def custom_scoring(estimator, X, y):\n",
    "    X_reshaped = X.reshape(-1, input_steps, len(features))\n",
    "    y_pred = estimator.predict(X_reshaped)\n",
    "    return -np.mean((y_pred - y) ** 2)\n",
    "\n",
    "# scikit-learn  permutation_importance\n",
    "result_rnn10 = permutation_importance(estimator=model_rnn10, X=X_test_rnn10_reshaped, y=y_test_rnn10.reshape((y_test_rnn10.shape[0], output_steps * len(target))), n_repeats=10, random_state=42, scoring=custom_scoring, n_jobs=-1)\n",
    "\n",
    "for i in range(len(features)):\n",
    "    print(f\"Feature: {features[i]}, Importance: {result_rnn10.importances_mean[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.6072 - val_loss: 0.2697\n",
      "Epoch 2/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2781 - val_loss: 0.2378\n",
      "Epoch 3/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2569 - val_loss: 0.2255\n",
      "Epoch 4/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2424 - val_loss: 0.2166\n",
      "Epoch 5/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2261 - val_loss: 0.2109\n",
      "Epoch 6/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2273 - val_loss: 0.2066\n",
      "Epoch 7/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2204 - val_loss: 0.2043\n",
      "Epoch 8/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2096 - val_loss: 0.2004\n",
      "Epoch 9/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2070 - val_loss: 0.1981\n",
      "Epoch 10/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2048 - val_loss: 0.1971\n",
      "Epoch 11/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2048 - val_loss: 0.1958\n",
      "Epoch 12/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2089 - val_loss: 0.1954\n",
      "Epoch 13/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1991 - val_loss: 0.1982\n",
      "Epoch 14/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1965 - val_loss: 0.1923\n",
      "Epoch 15/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1955 - val_loss: 0.1920\n",
      "Epoch 16/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1968 - val_loss: 0.1910\n",
      "Epoch 17/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1966 - val_loss: 0.1931\n",
      "Epoch 18/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1885 - val_loss: 0.1948\n",
      "Epoch 19/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1994 - val_loss: 0.1909\n",
      "Epoch 20/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1962 - val_loss: 0.1910\n",
      "Epoch 21/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1961 - val_loss: 0.1894\n",
      "Epoch 22/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1942 - val_loss: 0.1904\n",
      "Epoch 23/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1865 - val_loss: 0.1907\n",
      "Epoch 24/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1951 - val_loss: 0.1900\n",
      "Epoch 25/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1928 - val_loss: 0.1909\n",
      "Epoch 26/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1946 - val_loss: 0.1914\n",
      "Epoch 27/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1886 - val_loss: 0.1908\n",
      "Epoch 28/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1880 - val_loss: 0.1893\n",
      "Epoch 29/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1906 - val_loss: 0.1894\n",
      "Epoch 30/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1865 - val_loss: 0.1928\n",
      "Epoch 31/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1851 - val_loss: 0.1906\n",
      "Epoch 32/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1877 - val_loss: 0.1935\n",
      "Epoch 33/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1905 - val_loss: 0.1921\n",
      "Epoch 34/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1843 - val_loss: 0.1902\n",
      "Epoch 35/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1861 - val_loss: 0.1899\n",
      "Epoch 36/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1865 - val_loss: 0.1910\n",
      "Epoch 37/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1803 - val_loss: 0.1942\n",
      "Epoch 38/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1847 - val_loss: 0.1947\n",
      "Epoch 39/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1846 - val_loss: 0.1921\n",
      "Epoch 40/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1816 - val_loss: 0.1930\n",
      "Epoch 41/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1858 - val_loss: 0.1949\n",
      "Epoch 42/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1834 - val_loss: 0.1953\n",
      "Epoch 43/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1841 - val_loss: 0.1916\n",
      "Epoch 44/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1845 - val_loss: 0.1925\n",
      "Epoch 45/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1823 - val_loss: 0.1946\n",
      "Epoch 46/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1848 - val_loss: 0.1957\n",
      "Epoch 47/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1792 - val_loss: 0.1936\n",
      "Epoch 48/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1812 - val_loss: 0.1945\n",
      "Epoch 49/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1814 - val_loss: 0.1975\n",
      "Epoch 50/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1789 - val_loss: 0.1964\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Fold 1, Loss: 0.19639289379119873, RMSE Max: 0.44376013590150626, RMSE Min: 0.44256378968027543\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.5493 - val_loss: 0.2666\n",
      "Epoch 2/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2499 - val_loss: 0.2334\n",
      "Epoch 3/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2183 - val_loss: 0.2072\n",
      "Epoch 4/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2074 - val_loss: 0.2004\n",
      "Epoch 5/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1993 - val_loss: 0.1945\n",
      "Epoch 6/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1980 - val_loss: 0.1943\n",
      "Epoch 7/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1952 - val_loss: 0.1917\n",
      "Epoch 8/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1951 - val_loss: 0.1922\n",
      "Epoch 9/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1932 - val_loss: 0.1916\n",
      "Epoch 10/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1934 - val_loss: 0.1915\n",
      "Epoch 11/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1903 - val_loss: 0.1889\n",
      "Epoch 12/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1945 - val_loss: 0.1891\n",
      "Epoch 13/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1882 - val_loss: 0.1879\n",
      "Epoch 14/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1942 - val_loss: 0.1903\n",
      "Epoch 15/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1878 - val_loss: 0.1894\n",
      "Epoch 16/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1894 - val_loss: 0.1878\n",
      "Epoch 17/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1873 - val_loss: 0.1877\n",
      "Epoch 18/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1846 - val_loss: 0.1870\n",
      "Epoch 19/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1894 - val_loss: 0.1879\n",
      "Epoch 20/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1859 - val_loss: 0.1894\n",
      "Epoch 21/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1919 - val_loss: 0.1888\n",
      "Epoch 22/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1890 - val_loss: 0.1901\n",
      "Epoch 23/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1856 - val_loss: 0.1886\n",
      "Epoch 24/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1869 - val_loss: 0.1874\n",
      "Epoch 25/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1845 - val_loss: 0.1877\n",
      "Epoch 26/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1849 - val_loss: 0.1888\n",
      "Epoch 27/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1829 - val_loss: 0.1903\n",
      "Epoch 28/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1842 - val_loss: 0.1884\n",
      "Epoch 29/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1855 - val_loss: 0.1903\n",
      "Epoch 30/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1850 - val_loss: 0.1893\n",
      "Epoch 31/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1855 - val_loss: 0.1897\n",
      "Epoch 32/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1851 - val_loss: 0.1893\n",
      "Epoch 33/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1852 - val_loss: 0.1886\n",
      "Epoch 34/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1839 - val_loss: 0.1877\n",
      "Epoch 35/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1834 - val_loss: 0.1884\n",
      "Epoch 36/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1829 - val_loss: 0.1907\n",
      "Epoch 37/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1828 - val_loss: 0.1915\n",
      "Epoch 38/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1847 - val_loss: 0.1891\n",
      "Epoch 39/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1812 - val_loss: 0.1881\n",
      "Epoch 40/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1803 - val_loss: 0.1879\n",
      "Epoch 41/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1802 - val_loss: 0.1905\n",
      "Epoch 42/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1802 - val_loss: 0.1903\n",
      "Epoch 43/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1835 - val_loss: 0.1900\n",
      "Epoch 44/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1808 - val_loss: 0.1891\n",
      "Epoch 45/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1800 - val_loss: 0.1908\n",
      "Epoch 46/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1828 - val_loss: 0.1916\n",
      "Epoch 47/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1770 - val_loss: 0.1907\n",
      "Epoch 48/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1798 - val_loss: 0.1919\n",
      "Epoch 49/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1789 - val_loss: 0.1904\n",
      "Epoch 50/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1774 - val_loss: 0.1909\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Fold 2, Loss: 0.1909276694059372, RMSE Max: 0.4436630776435029, RMSE Min: 0.4301376634019081\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.4763 - val_loss: 0.2336\n",
      "Epoch 2/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2327 - val_loss: 0.2080\n",
      "Epoch 3/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2133 - val_loss: 0.1969\n",
      "Epoch 4/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1994 - val_loss: 0.1936\n",
      "Epoch 5/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1990 - val_loss: 0.1933\n",
      "Epoch 6/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1919 - val_loss: 0.1917\n",
      "Epoch 7/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1902 - val_loss: 0.1920\n",
      "Epoch 8/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1905 - val_loss: 0.1905\n",
      "Epoch 9/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1892 - val_loss: 0.1909\n",
      "Epoch 10/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1905 - val_loss: 0.1952\n",
      "Epoch 11/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1887 - val_loss: 0.1926\n",
      "Epoch 12/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1921 - val_loss: 0.1929\n",
      "Epoch 13/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1853 - val_loss: 0.1913\n",
      "Epoch 14/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1892 - val_loss: 0.1949\n",
      "Epoch 15/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1914 - val_loss: 0.1967\n",
      "Epoch 16/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1864 - val_loss: 0.1919\n",
      "Epoch 17/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1898 - val_loss: 0.1956\n",
      "Epoch 18/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1857 - val_loss: 0.1932\n",
      "Epoch 19/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1861 - val_loss: 0.1960\n",
      "Epoch 20/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1851 - val_loss: 0.1971\n",
      "Epoch 21/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1850 - val_loss: 0.1944\n",
      "Epoch 22/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1842 - val_loss: 0.1993\n",
      "Epoch 23/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1861 - val_loss: 0.1978\n",
      "Epoch 24/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1850 - val_loss: 0.1950\n",
      "Epoch 25/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1874 - val_loss: 0.1939\n",
      "Epoch 26/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1845 - val_loss: 0.1948\n",
      "Epoch 27/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1855 - val_loss: 0.1957\n",
      "Epoch 28/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1843 - val_loss: 0.2026\n",
      "Epoch 29/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1827 - val_loss: 0.1955\n",
      "Epoch 30/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1830 - val_loss: 0.1972\n",
      "Epoch 31/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1833 - val_loss: 0.1947\n",
      "Epoch 32/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1840 - val_loss: 0.1952\n",
      "Epoch 33/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1824 - val_loss: 0.1979\n",
      "Epoch 34/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1831 - val_loss: 0.1989\n",
      "Epoch 35/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1838 - val_loss: 0.1972\n",
      "Epoch 36/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1831 - val_loss: 0.1967\n",
      "Epoch 37/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1837 - val_loss: 0.1975\n",
      "Epoch 38/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1823 - val_loss: 0.1974\n",
      "Epoch 39/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1783 - val_loss: 0.1976\n",
      "Epoch 40/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1816 - val_loss: 0.1987\n",
      "Epoch 41/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1829 - val_loss: 0.2005\n",
      "Epoch 42/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1802 - val_loss: 0.1949\n",
      "Epoch 43/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1798 - val_loss: 0.2005\n",
      "Epoch 44/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1766 - val_loss: 0.2007\n",
      "Epoch 45/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1823 - val_loss: 0.2022\n",
      "Epoch 46/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1762 - val_loss: 0.1992\n",
      "Epoch 47/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1789 - val_loss: 0.2006\n",
      "Epoch 48/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1783 - val_loss: 0.2040\n",
      "Epoch 49/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1767 - val_loss: 0.1989\n",
      "Epoch 50/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1775 - val_loss: 0.2030\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Fold 3, Loss: 0.20301920175552368, RMSE Max: 0.4469483273799469, RMSE Min: 0.45417573062519023\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.4292 - val_loss: 0.2206\n",
      "Epoch 2/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2147 - val_loss: 0.1984\n",
      "Epoch 3/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1991 - val_loss: 0.1928\n",
      "Epoch 4/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1952 - val_loss: 0.1916\n",
      "Epoch 5/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1933 - val_loss: 0.1922\n",
      "Epoch 6/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1900 - val_loss: 0.1879\n",
      "Epoch 7/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1893 - val_loss: 0.1878\n",
      "Epoch 8/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1912 - val_loss: 0.1875\n",
      "Epoch 9/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1889 - val_loss: 0.1892\n",
      "Epoch 10/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1883 - val_loss: 0.1888\n",
      "Epoch 11/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1900 - val_loss: 0.1879\n",
      "Epoch 12/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1873 - val_loss: 0.1888\n",
      "Epoch 13/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1862 - val_loss: 0.1892\n",
      "Epoch 14/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1876 - val_loss: 0.1894\n",
      "Epoch 15/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1868 - val_loss: 0.1889\n",
      "Epoch 16/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1868 - val_loss: 0.1894\n",
      "Epoch 17/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1857 - val_loss: 0.1879\n",
      "Epoch 18/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1868 - val_loss: 0.1890\n",
      "Epoch 19/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1907 - val_loss: 0.1896\n",
      "Epoch 20/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1849 - val_loss: 0.1894\n",
      "Epoch 21/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1848 - val_loss: 0.1905\n",
      "Epoch 22/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1855 - val_loss: 0.1886\n",
      "Epoch 23/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1859 - val_loss: 0.1878\n",
      "Epoch 24/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1850 - val_loss: 0.1911\n",
      "Epoch 25/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1839 - val_loss: 0.1890\n",
      "Epoch 26/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1855 - val_loss: 0.1920\n",
      "Epoch 27/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1855 - val_loss: 0.1894\n",
      "Epoch 28/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1843 - val_loss: 0.1897\n",
      "Epoch 29/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1835 - val_loss: 0.1910\n",
      "Epoch 30/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1843 - val_loss: 0.1914\n",
      "Epoch 31/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1840 - val_loss: 0.1910\n",
      "Epoch 32/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1874 - val_loss: 0.1900\n",
      "Epoch 33/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1842 - val_loss: 0.1909\n",
      "Epoch 34/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1859 - val_loss: 0.1928\n",
      "Epoch 35/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1819 - val_loss: 0.1912\n",
      "Epoch 36/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1824 - val_loss: 0.1904\n",
      "Epoch 37/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1835 - val_loss: 0.1909\n",
      "Epoch 38/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1812 - val_loss: 0.1919\n",
      "Epoch 39/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1801 - val_loss: 0.1908\n",
      "Epoch 40/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1797 - val_loss: 0.1915\n",
      "Epoch 41/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1811 - val_loss: 0.1918\n",
      "Epoch 42/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1823 - val_loss: 0.1926\n",
      "Epoch 43/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1787 - val_loss: 0.1933\n",
      "Epoch 44/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1784 - val_loss: 0.1922\n",
      "Epoch 45/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1787 - val_loss: 0.1937\n",
      "Epoch 46/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1792 - val_loss: 0.1927\n",
      "Epoch 47/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1791 - val_loss: 0.1934\n",
      "Epoch 48/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1782 - val_loss: 0.1920\n",
      "Epoch 49/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1789 - val_loss: 0.1929\n",
      "Epoch 50/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1748 - val_loss: 0.1939\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Fold 4, Loss: 0.19389548897743225, RMSE Max: 0.4335176725628513, RMSE Min: 0.4470496938645162\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.3901 - val_loss: 0.2318\n",
      "Epoch 2/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2099 - val_loss: 0.2185\n",
      "Epoch 3/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1992 - val_loss: 0.2224\n",
      "Epoch 4/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1923 - val_loss: 0.2136\n",
      "Epoch 5/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1904 - val_loss: 0.2112\n",
      "Epoch 6/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1896 - val_loss: 0.2168\n",
      "Epoch 7/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1909 - val_loss: 0.2115\n",
      "Epoch 8/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1868 - val_loss: 0.2114\n",
      "Epoch 9/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1878 - val_loss: 0.2139\n",
      "Epoch 10/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1906 - val_loss: 0.2114\n",
      "Epoch 11/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1889 - val_loss: 0.2097\n",
      "Epoch 12/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1877 - val_loss: 0.2136\n",
      "Epoch 13/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1854 - val_loss: 0.2124\n",
      "Epoch 14/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1859 - val_loss: 0.2145\n",
      "Epoch 15/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1895 - val_loss: 0.2127\n",
      "Epoch 16/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1856 - val_loss: 0.2132\n",
      "Epoch 17/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1862 - val_loss: 0.2101\n",
      "Epoch 18/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1882 - val_loss: 0.2117\n",
      "Epoch 19/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1840 - val_loss: 0.2124\n",
      "Epoch 20/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1862 - val_loss: 0.2113\n",
      "Epoch 21/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1858 - val_loss: 0.2125\n",
      "Epoch 22/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1840 - val_loss: 0.2141\n",
      "Epoch 23/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1850 - val_loss: 0.2106\n",
      "Epoch 24/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1846 - val_loss: 0.2096\n",
      "Epoch 25/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1843 - val_loss: 0.2126\n",
      "Epoch 26/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1831 - val_loss: 0.2109\n",
      "Epoch 27/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1818 - val_loss: 0.2128\n",
      "Epoch 28/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1857 - val_loss: 0.2132\n",
      "Epoch 29/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1831 - val_loss: 0.2124\n",
      "Epoch 30/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1853 - val_loss: 0.2120\n",
      "Epoch 31/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1844 - val_loss: 0.2112\n",
      "Epoch 32/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1829 - val_loss: 0.2133\n",
      "Epoch 33/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1817 - val_loss: 0.2135\n",
      "Epoch 34/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1805 - val_loss: 0.2143\n",
      "Epoch 35/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1823 - val_loss: 0.2165\n",
      "Epoch 36/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1821 - val_loss: 0.2142\n",
      "Epoch 37/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1805 - val_loss: 0.2153\n",
      "Epoch 38/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1811 - val_loss: 0.2148\n",
      "Epoch 39/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1793 - val_loss: 0.2176\n",
      "Epoch 40/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1796 - val_loss: 0.2168\n",
      "Epoch 41/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1800 - val_loss: 0.2176\n",
      "Epoch 42/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1788 - val_loss: 0.2164\n",
      "Epoch 43/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1776 - val_loss: 0.2181\n",
      "Epoch 44/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1771 - val_loss: 0.2153\n",
      "Epoch 45/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1769 - val_loss: 0.2180\n",
      "Epoch 46/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1791 - val_loss: 0.2170\n",
      "Epoch 47/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1745 - val_loss: 0.2212\n",
      "Epoch 48/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1750 - val_loss: 0.2171\n",
      "Epoch 49/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1749 - val_loss: 0.2186\n",
      "Epoch 50/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1743 - val_loss: 0.2198\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Fold 5, Loss: 0.21981048583984375, RMSE Max: 0.46744424653042915, RMSE Min: 0.47023064765600275\n",
      "Average Loss: 0.20080914795398713\n",
      "Average RMSE of max_temp: 0.44706669200364735\n",
      "Average RMSE of min_temp: 0.44883150504557856\n",
      "Average MSE of max_temp: 0.19999289761317451\n",
      "Average MSE of min_temp: 0.20162540049625743\n",
      "Average MAE of max_temp: 0.3466039440625104\n",
      "Average MAE of min_temp: 0.3580280398125199\n",
      "Average R² of max_temp: 0.7954230643683501\n",
      "Average R² of min_temp: 0.7915317120828492\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Create a time window function\n",
    "def create_time_series10(X, y, input_steps=10, output_steps=3):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - input_steps - output_steps + 1):\n",
    "        Xs.append(X[i:(i + input_steps)])\n",
    "        ys.append(y[(i + input_steps):(i + input_steps + output_steps)])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "# create LSTM\n",
    "def create_lstm_model(input_shape, output_steps, num_targets):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, input_shape=input_shape, return_sequences=False))\n",
    "    model.add(Dense(output_steps * num_targets))  # output_steps * num_targets\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# cross-va\n",
    "n_splits = 5\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "input_steps = 10\n",
    "output_steps = 3\n",
    "\n",
    "results = {\n",
    "    'loss': [],\n",
    "    'rmse_max': [],\n",
    "    'rmse_min': [],\n",
    "    'mse_max': [],\n",
    "    'mse_min': [],\n",
    "    'mae_max': [],\n",
    "    'mae_min': [],\n",
    "    'r2_max': [],\n",
    "    'r2_min': []\n",
    "}\n",
    "\n",
    "# Converts feature and target variables to numpy arrays\n",
    "X = df_weather[['global_radiation', 'diff_temp', 'mean_temp']].values\n",
    "y = df_weather[['max_temp', 'min_temp']].values\n",
    "\n",
    "# Check the shape of y\n",
    "num_targets = y.shape[1]  \n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(tscv.split(X)):\n",
    "\n",
    "    X_train, X_val = X[train_index], X[val_index]\n",
    "    y_train, y_val = y[train_index], y[val_index]\n",
    "    \n",
    "    X_train_rnn, y_train_rnn = create_time_series10(X_train, y_train, input_steps, output_steps)\n",
    "    X_val_rnn, y_val_rnn = create_time_series10(X_val, y_val, input_steps, output_steps)\n",
    "    \n",
    "\n",
    "    model = create_lstm_model((input_steps, X_train_rnn.shape[2]), output_steps, num_targets)\n",
    "    history = model.fit(X_train_rnn, y_train_rnn.reshape((y_train_rnn.shape[0], output_steps * num_targets)),\n",
    "                        epochs=50,\n",
    "                        batch_size=32,\n",
    "                        validation_data=(X_val_rnn, y_val_rnn.reshape((y_val_rnn.shape[0], output_steps * num_targets))),\n",
    "                        verbose=1)\n",
    "    \n",
    "\n",
    "    loss = model.evaluate(X_val_rnn, y_val_rnn.reshape((y_val_rnn.shape[0], output_steps * num_targets)), verbose=0)\n",
    "    y_val_pred_rnn = model.predict(X_val_rnn)\n",
    "\n",
    "    # Flatters y_val_rnn and y_val_pred_rnn into a 2D array to make sure the shapes match\n",
    "    y_val_rnn_flat = y_val_rnn.reshape(-1, num_targets)\n",
    "    y_val_pred_rnn_flat = y_val_pred_rnn.reshape(-1, num_targets)\n",
    "\n",
    "    mse_max = mean_squared_error(y_val_rnn_flat[:, 0], y_val_pred_rnn_flat[:, 0])\n",
    "    mse_min = mean_squared_error(y_val_rnn_flat[:, 1], y_val_pred_rnn_flat[:, 1])\n",
    "    rmse_max = np.sqrt(mse_max)\n",
    "    rmse_min = np.sqrt(mse_min)\n",
    "    mae_max = mean_absolute_error(y_val_rnn_flat[:, 0], y_val_pred_rnn_flat[:, 0])\n",
    "    mae_min = mean_absolute_error(y_val_rnn_flat[:, 1], y_val_pred_rnn_flat[:, 1])\n",
    "    r2_max = r2_score(y_val_rnn_flat[:, 0], y_val_pred_rnn_flat[:, 0])\n",
    "    r2_min = r2_score(y_val_rnn_flat[:, 1], y_val_pred_rnn_flat[:, 1])\n",
    "\n",
    "    results['loss'].append(loss)\n",
    "    results['rmse_max'].append(rmse_max)\n",
    "    results['rmse_min'].append(rmse_min)\n",
    "    results['mse_max'].append(mse_max)\n",
    "    results['mse_min'].append(mse_min)\n",
    "    results['mae_max'].append(mae_max)\n",
    "    results['mae_min'].append(mae_min)\n",
    "    results['r2_max'].append(r2_max)\n",
    "    results['r2_min'].append(r2_min)\n",
    "    print(f'Fold {len(results[\"loss\"])}, Loss: {loss}, RMSE Max: {rmse_max}, RMSE Min: {rmse_min}')\n",
    "\n",
    "print(f'Average Loss: {np.mean(results[\"loss\"])}')\n",
    "print(f'Average RMSE of max_temp: {np.mean(results[\"rmse_max\"])}')\n",
    "print(f'Average RMSE of min_temp: {np.mean(results[\"rmse_min\"])}')\n",
    "print(f'Average MSE of max_temp: {np.mean(results[\"mse_max\"])}')\n",
    "print(f'Average MSE of min_temp: {np.mean(results[\"mse_min\"])}')\n",
    "print(f'Average MAE of max_temp: {np.mean(results[\"mae_max\"])}')\n",
    "print(f'Average MAE of min_temp: {np.mean(results[\"mae_min\"])}')\n",
    "print(f'Average R² of max_temp: {np.mean(results[\"r2_max\"])}')\n",
    "print(f'Average R² of min_temp: {np.mean(results[\"r2_min\"])}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_rnn101 = y_train_rnn10.reshape((y_train_rnn10.shape[0], output_steps * len(target)))\n",
    "y_val_rnn101 = y_val_rnn10.reshape((y_val_rnn10.shape[0], output_steps * len(target)))\n",
    "y_test_rnn101 = y_test_rnn10.reshape((y_test_rnn10.shape[0], output_steps * len(target)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model2(input_shape, head_size, num_heads, ff_dim, num_transformer_blocks, mlp_units, dropout=0, mlp_dropout=0):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = inputs\n",
    "    for _ in range(num_transformer_blocks):\n",
    "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
    "\n",
    "    x = GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
    "    for dim in mlp_units:\n",
    "        x = Dense(dim, activation=\"relu\")(x)\n",
    "        x = Dropout(mlp_dropout)(x)\n",
    "    x = Dense(output_steps * len(target))(x)  # 输出是 6 个神经元，对应 3 天的 max_temp 和 min_temp\n",
    "    return Model(inputs, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Reshape\n",
    "\n",
    "input_shape2 = (input_steps, len(features))\n",
    "model2 = build_model2(\n",
    "    input_shape2,\n",
    "    head_size=256,\n",
    "    num_heads=4,\n",
    "    ff_dim=4,\n",
    "    num_transformer_blocks=4,\n",
    "    mlp_units=[128],\n",
    "    mlp_dropout=0.4,\n",
    "    dropout=0.25,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_57\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_57\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_34      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span> │ input_layer_34[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">15,363</span> │ layer_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_144         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_atten… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_88 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_144[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                     │                   │            │ input_layer_34[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span> │ add_88[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_133 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span> │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_145         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_133[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_134 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span> │ dropout_145[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_89 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_134[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│                     │                   │            │ add_88[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span> │ add_89[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">15,363</span> │ layer_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_147         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_atten… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_90 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_147[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                     │                   │            │ add_89[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span> │ add_90[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_135 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span> │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_148         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_135[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_136 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span> │ dropout_148[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_91 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_136[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│                     │                   │            │ add_90[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span> │ add_91[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">15,363</span> │ layer_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_150         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_atten… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_92 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_150[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                     │                   │            │ add_91[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span> │ add_92[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_137 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span> │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_151         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_137[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_138 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span> │ dropout_151[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_93 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_138[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│                     │                   │            │ add_92[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span> │ add_93[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">15,363</span> │ layer_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_153         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_atten… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_94 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_153[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                     │                   │            │ add_93[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span> │ add_94[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_139 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span> │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_154         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_139[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_140 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span> │ dropout_154[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_95 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_140[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│                     │                   │            │ add_94[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_95[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_141 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,408</span> │ global_average_p… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_155         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_141[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_142 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">774</span> │ dropout_155[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_34      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │          \u001b[38;5;34m6\u001b[0m │ input_layer_34[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │     \u001b[38;5;34m15,363\u001b[0m │ layer_normalizat… │\n",
       "│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_144         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ multi_head_atten… │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_88 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ dropout_144[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
       "│                     │                   │            │ input_layer_34[\u001b[38;5;34m0\u001b[0m… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │          \u001b[38;5;34m6\u001b[0m │ add_88[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_133 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m4\u001b[0m)     │         \u001b[38;5;34m16\u001b[0m │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_145         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m4\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ dense_133[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_134 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │         \u001b[38;5;34m15\u001b[0m │ dropout_145[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_89 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ dense_134[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│                     │                   │            │ add_88[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │          \u001b[38;5;34m6\u001b[0m │ add_89[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │     \u001b[38;5;34m15,363\u001b[0m │ layer_normalizat… │\n",
       "│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_147         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ multi_head_atten… │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_90 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ dropout_147[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
       "│                     │                   │            │ add_89[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │          \u001b[38;5;34m6\u001b[0m │ add_90[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_135 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m4\u001b[0m)     │         \u001b[38;5;34m16\u001b[0m │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_148         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m4\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ dense_135[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_136 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │         \u001b[38;5;34m15\u001b[0m │ dropout_148[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_91 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ dense_136[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│                     │                   │            │ add_90[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │          \u001b[38;5;34m6\u001b[0m │ add_91[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │     \u001b[38;5;34m15,363\u001b[0m │ layer_normalizat… │\n",
       "│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_150         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ multi_head_atten… │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_92 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ dropout_150[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
       "│                     │                   │            │ add_91[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │          \u001b[38;5;34m6\u001b[0m │ add_92[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_137 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m4\u001b[0m)     │         \u001b[38;5;34m16\u001b[0m │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_151         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m4\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ dense_137[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_138 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │         \u001b[38;5;34m15\u001b[0m │ dropout_151[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_93 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ dense_138[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│                     │                   │            │ add_92[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │          \u001b[38;5;34m6\u001b[0m │ add_93[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │     \u001b[38;5;34m15,363\u001b[0m │ layer_normalizat… │\n",
       "│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_153         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ multi_head_atten… │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_94 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ dropout_153[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
       "│                     │                   │            │ add_93[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │          \u001b[38;5;34m6\u001b[0m │ add_94[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_139 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m4\u001b[0m)     │         \u001b[38;5;34m16\u001b[0m │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_154         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m4\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ dense_139[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_140 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │         \u001b[38;5;34m15\u001b[0m │ dropout_154[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_95 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ dense_140[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│                     │                   │            │ add_94[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ add_95[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_141 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │      \u001b[38;5;34m1,408\u001b[0m │ global_average_p… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_155         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_141[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_142 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)         │        \u001b[38;5;34m774\u001b[0m │ dropout_155[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">63,806</span> (249.24 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m63,806\u001b[0m (249.24 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">63,806</span> (249.24 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m63,806\u001b[0m (249.24 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model2.compile(\n",
    "    loss=\"mean_squared_error\",\n",
    "    optimizer=Adam(learning_rate=1e-4),\n",
    ")\n",
    "\n",
    "model2.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 28ms/step - loss: 1.1740 - val_loss: 0.3992\n",
      "Epoch 2/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 34ms/step - loss: 0.5313 - val_loss: 0.3146\n",
      "Epoch 3/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 36ms/step - loss: 0.4206 - val_loss: 0.2904\n",
      "Epoch 4/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 34ms/step - loss: 0.3882 - val_loss: 0.2837\n",
      "Epoch 5/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 32ms/step - loss: 0.3652 - val_loss: 0.2817\n",
      "Epoch 6/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 33ms/step - loss: 0.3472 - val_loss: 0.2644\n",
      "Epoch 7/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 34ms/step - loss: 0.3340 - val_loss: 0.2620\n",
      "Epoch 8/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 33ms/step - loss: 0.3251 - val_loss: 0.2592\n",
      "Epoch 9/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 35ms/step - loss: 0.3187 - val_loss: 0.2556\n",
      "Epoch 10/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 32ms/step - loss: 0.3119 - val_loss: 0.2492\n",
      "Epoch 11/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 34ms/step - loss: 0.3073 - val_loss: 0.2446\n",
      "Epoch 12/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 33ms/step - loss: 0.2987 - val_loss: 0.2432\n",
      "Epoch 13/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 33ms/step - loss: 0.2962 - val_loss: 0.2513\n",
      "Epoch 14/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 34ms/step - loss: 0.2935 - val_loss: 0.2424\n",
      "Epoch 15/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 33ms/step - loss: 0.2936 - val_loss: 0.2386\n",
      "Epoch 16/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 34ms/step - loss: 0.2853 - val_loss: 0.2374\n",
      "Epoch 17/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 34ms/step - loss: 0.2874 - val_loss: 0.2363\n",
      "Epoch 18/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 34ms/step - loss: 0.2888 - val_loss: 0.2357\n",
      "Epoch 19/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 34ms/step - loss: 0.2816 - val_loss: 0.2359\n",
      "Epoch 20/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 33ms/step - loss: 0.2803 - val_loss: 0.2310\n",
      "Epoch 21/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 35ms/step - loss: 0.2761 - val_loss: 0.2340\n",
      "Epoch 22/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 34ms/step - loss: 0.2773 - val_loss: 0.2317\n",
      "Epoch 23/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 33ms/step - loss: 0.2726 - val_loss: 0.2320\n",
      "Epoch 24/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 38ms/step - loss: 0.2739 - val_loss: 0.2330\n",
      "Epoch 25/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 35ms/step - loss: 0.2740 - val_loss: 0.2331\n",
      "Epoch 26/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 33ms/step - loss: 0.2766 - val_loss: 0.2361\n",
      "Epoch 27/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 34ms/step - loss: 0.2731 - val_loss: 0.2346\n",
      "Epoch 28/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 33ms/step - loss: 0.2786 - val_loss: 0.2315\n",
      "Epoch 29/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 34ms/step - loss: 0.2800 - val_loss: 0.2324\n",
      "Epoch 30/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 34ms/step - loss: 0.2787 - val_loss: 0.2312\n",
      "Epoch 31/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 34ms/step - loss: 0.2764 - val_loss: 0.2357\n",
      "Epoch 32/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 34ms/step - loss: 0.2753 - val_loss: 0.2297\n",
      "Epoch 33/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 33ms/step - loss: 0.2662 - val_loss: 0.2279\n",
      "Epoch 34/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 34ms/step - loss: 0.2596 - val_loss: 0.2265\n",
      "Epoch 35/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 35ms/step - loss: 0.2647 - val_loss: 0.2319\n",
      "Epoch 36/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 36ms/step - loss: 0.2704 - val_loss: 0.2286\n",
      "Epoch 37/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 34ms/step - loss: 0.2621 - val_loss: 0.2277\n",
      "Epoch 38/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 35ms/step - loss: 0.2618 - val_loss: 0.2298\n",
      "Epoch 39/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 34ms/step - loss: 0.2651 - val_loss: 0.2279\n",
      "Epoch 40/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 34ms/step - loss: 0.2607 - val_loss: 0.2284\n",
      "Epoch 41/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 34ms/step - loss: 0.2557 - val_loss: 0.2291\n",
      "Epoch 42/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 34ms/step - loss: 0.2582 - val_loss: 0.2279\n",
      "Epoch 43/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 34ms/step - loss: 0.2536 - val_loss: 0.2305\n",
      "Epoch 44/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 34ms/step - loss: 0.2534 - val_loss: 0.2294\n",
      "Epoch 45/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 35ms/step - loss: 0.2520 - val_loss: 0.2262\n",
      "Epoch 46/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 36ms/step - loss: 0.2572 - val_loss: 0.2236\n",
      "Epoch 47/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 35ms/step - loss: 0.2529 - val_loss: 0.2275\n",
      "Epoch 48/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 34ms/step - loss: 0.2551 - val_loss: 0.2253\n",
      "Epoch 49/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 34ms/step - loss: 0.2547 - val_loss: 0.2268\n",
      "Epoch 50/50\n",
      "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 35ms/step - loss: 0.2538 - val_loss: 0.2281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "history2 = model2.fit(\n",
    "    X_train_rnn10, y_train_rnn101,\n",
    "    validation_data=(X_val_rnn10, y_val_rnn101),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    ")\n",
    "\n",
    "model2.save('transformer_weather_model2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step\n",
      "RMSE of 'max_temp' in test set: 0.41213728636805025\n",
      "RMSE of 'min_temp' in test set: 0.48201765031550925\n",
      "MSE of 'max_temp' in test set: 0.16985714281482026\n",
      "MSE of 'min_temp' in test set: 0.23234101521568454\n",
      "MAE of 'max_temp' in test set: 0.3165091763238219\n",
      "MAE of 'min_temp' in test set: 0.3904010183093992\n",
      "R² of 'max_temp' in test set: 0.8283909536513456\n",
      "R² of 'min_temp' in test set: 0.7543813495169764\n"
     ]
    }
   ],
   "source": [
    "# evaluate\n",
    "y_pred_test2 = model2.predict(X_test_rnn10)\n",
    "\n",
    "# Reshape the predicted and actual results into a flat array to calculate metrics\n",
    "y_test_actual = y_test_rnn10.reshape(-1, output_steps * len(target))\n",
    "y_pred_test_flat = y_pred_test2.reshape(-1, output_steps * len(target))\n",
    "\n",
    "\n",
    "\n",
    "# RMSE\n",
    "rmse_max_t2 = np.sqrt(mean_squared_error(y_test_actual[:, 0], y_pred_test_flat[:, 0]))\n",
    "rmse_min_t2 = np.sqrt(mean_squared_error(y_test_actual[:, 1], y_pred_test_flat[:, 1]))\n",
    "\n",
    "# MSE\n",
    "mse_max_t2 = mean_squared_error(y_test_actual[:, 0], y_pred_test_flat[:, 0])\n",
    "mse_min_t2 = mean_squared_error(y_test_actual[:, 1], y_pred_test_flat[:, 1])\n",
    "\n",
    "# MAE\n",
    "mae_max_t2 = mean_absolute_error(y_test_actual[:, 0], y_pred_test_flat[:, 0])\n",
    "mae_min_t2 = mean_absolute_error(y_test_actual[:, 1], y_pred_test_flat[:, 1])\n",
    "\n",
    "# R²\n",
    "r2_max_t2 = r2_score(y_test_actual[:, 0], y_pred_test_flat[:, 0])\n",
    "r2_min_t2 = r2_score(y_test_actual[:, 1], y_pred_test_flat[:, 1])\n",
    "\n",
    "print(f\"RMSE of 'max_temp' in test set: {rmse_max_t2}\")\n",
    "print(f\"RMSE of 'min_temp' in test set: {rmse_min_t2}\")\n",
    "\n",
    "print(f\"MSE of 'max_temp' in test set: {mse_max_t2}\")\n",
    "print(f\"MSE of 'min_temp' in test set: {mse_min_t2}\")\n",
    "\n",
    "print(f\"MAE of 'max_temp' in test set: {mae_max_t2}\")\n",
    "print(f\"MAE of 'min_temp' in test set: {mae_min_t2}\")\n",
    "\n",
    "print(f\"R² of 'max_temp' in test set: {r2_max_t2}\")\n",
    "print(f\"R² of 'min_temp' in test set: {r2_min_t2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 33ms/step - loss: 1.5790 - val_loss: 0.8096\n",
      "Epoch 2/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - loss: 0.9262 - val_loss: 0.5394\n",
      "Epoch 3/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 41ms/step - loss: 0.7372 - val_loss: 0.4287\n",
      "Epoch 4/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 40ms/step - loss: 0.6308 - val_loss: 0.3777\n",
      "Epoch 5/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - loss: 0.5638 - val_loss: 0.3487\n",
      "Epoch 6/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - loss: 0.5335 - val_loss: 0.3293\n",
      "Epoch 7/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - loss: 0.5081 - val_loss: 0.3226\n",
      "Epoch 8/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 43ms/step - loss: 0.4867 - val_loss: 0.3124\n",
      "Epoch 9/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 40ms/step - loss: 0.4560 - val_loss: 0.3038\n",
      "Epoch 10/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - loss: 0.4613 - val_loss: 0.3016\n",
      "Epoch 11/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 40ms/step - loss: 0.4474 - val_loss: 0.2978\n",
      "Epoch 12/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 41ms/step - loss: 0.4361 - val_loss: 0.2991\n",
      "Epoch 13/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 41ms/step - loss: 0.4416 - val_loss: 0.2941\n",
      "Epoch 14/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 40ms/step - loss: 0.4162 - val_loss: 0.2930\n",
      "Epoch 15/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 42ms/step - loss: 0.4086 - val_loss: 0.3018\n",
      "Epoch 16/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 40ms/step - loss: 0.4103 - val_loss: 0.2796\n",
      "Epoch 17/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 40ms/step - loss: 0.4031 - val_loss: 0.2807\n",
      "Epoch 18/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 45ms/step - loss: 0.3972 - val_loss: 0.2747\n",
      "Epoch 19/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 41ms/step - loss: 0.3871 - val_loss: 0.2804\n",
      "Epoch 20/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 40ms/step - loss: 0.3921 - val_loss: 0.2880\n",
      "Epoch 21/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 42ms/step - loss: 0.3719 - val_loss: 0.2704\n",
      "Epoch 22/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 41ms/step - loss: 0.3681 - val_loss: 0.2671\n",
      "Epoch 23/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 42ms/step - loss: 0.3635 - val_loss: 0.2689\n",
      "Epoch 24/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 41ms/step - loss: 0.3564 - val_loss: 0.2673\n",
      "Epoch 25/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 42ms/step - loss: 0.3700 - val_loss: 0.2708\n",
      "Epoch 26/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 44ms/step - loss: 0.3599 - val_loss: 0.2615\n",
      "Epoch 27/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 42ms/step - loss: 0.3625 - val_loss: 0.2634\n",
      "Epoch 28/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 46ms/step - loss: 0.3619 - val_loss: 0.2628\n",
      "Epoch 29/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 41ms/step - loss: 0.3385 - val_loss: 0.2590\n",
      "Epoch 30/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 40ms/step - loss: 0.3476 - val_loss: 0.2570\n",
      "Epoch 31/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 41ms/step - loss: 0.3576 - val_loss: 0.2561\n",
      "Epoch 32/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 44ms/step - loss: 0.3493 - val_loss: 0.2546\n",
      "Epoch 33/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 42ms/step - loss: 0.3468 - val_loss: 0.2560\n",
      "Epoch 34/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 41ms/step - loss: 0.3281 - val_loss: 0.2552\n",
      "Epoch 35/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 42ms/step - loss: 0.3330 - val_loss: 0.2514\n",
      "Epoch 36/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 44ms/step - loss: 0.3483 - val_loss: 0.2574\n",
      "Epoch 37/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 44ms/step - loss: 0.3425 - val_loss: 0.2504\n",
      "Epoch 38/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 42ms/step - loss: 0.3235 - val_loss: 0.2495\n",
      "Epoch 39/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 41ms/step - loss: 0.3258 - val_loss: 0.2503\n",
      "Epoch 40/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 42ms/step - loss: 0.3303 - val_loss: 0.2524\n",
      "Epoch 41/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 42ms/step - loss: 0.3165 - val_loss: 0.2490\n",
      "Epoch 42/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 48ms/step - loss: 0.3259 - val_loss: 0.2501\n",
      "Epoch 43/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 43ms/step - loss: 0.3129 - val_loss: 0.2478\n",
      "Epoch 44/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 41ms/step - loss: 0.3303 - val_loss: 0.2476\n",
      "Epoch 45/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 43ms/step - loss: 0.3129 - val_loss: 0.2510\n",
      "Epoch 46/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 46ms/step - loss: 0.3114 - val_loss: 0.2452\n",
      "Epoch 47/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 45ms/step - loss: 0.3083 - val_loss: 0.2463\n",
      "Epoch 48/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 41ms/step - loss: 0.3085 - val_loss: 0.2455\n",
      "Epoch 49/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 42ms/step - loss: 0.3038 - val_loss: 0.2415\n",
      "Epoch 50/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 41ms/step - loss: 0.3033 - val_loss: 0.2428\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step\n",
      "Fold 1, Loss: 0.24275651574134827, RMSE Max: 0.4728973697784774, RMSE Min: 0.5117432232694085\n",
      "Epoch 1/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 29ms/step - loss: 1.0122 - val_loss: 0.5071\n",
      "Epoch 2/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 32ms/step - loss: 0.5806 - val_loss: 0.3372\n",
      "Epoch 3/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 36ms/step - loss: 0.4438 - val_loss: 0.3070\n",
      "Epoch 4/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 36ms/step - loss: 0.4153 - val_loss: 0.2954\n",
      "Epoch 5/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 36ms/step - loss: 0.3893 - val_loss: 0.2893\n",
      "Epoch 6/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 35ms/step - loss: 0.3674 - val_loss: 0.2871\n",
      "Epoch 7/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 35ms/step - loss: 0.3606 - val_loss: 0.2866\n",
      "Epoch 8/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 37ms/step - loss: 0.3543 - val_loss: 0.2770\n",
      "Epoch 9/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 35ms/step - loss: 0.3323 - val_loss: 0.2728\n",
      "Epoch 10/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 35ms/step - loss: 0.3404 - val_loss: 0.2698\n",
      "Epoch 11/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 36ms/step - loss: 0.3197 - val_loss: 0.2667\n",
      "Epoch 12/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 35ms/step - loss: 0.3298 - val_loss: 0.2631\n",
      "Epoch 13/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 36ms/step - loss: 0.3133 - val_loss: 0.2605\n",
      "Epoch 14/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 37ms/step - loss: 0.3106 - val_loss: 0.2558\n",
      "Epoch 15/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 36ms/step - loss: 0.3144 - val_loss: 0.2527\n",
      "Epoch 16/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 37ms/step - loss: 0.3055 - val_loss: 0.2509\n",
      "Epoch 17/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 37ms/step - loss: 0.2996 - val_loss: 0.2489\n",
      "Epoch 18/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 36ms/step - loss: 0.2962 - val_loss: 0.2469\n",
      "Epoch 19/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 39ms/step - loss: 0.2933 - val_loss: 0.2445\n",
      "Epoch 20/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 37ms/step - loss: 0.2906 - val_loss: 0.2440\n",
      "Epoch 21/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 37ms/step - loss: 0.2946 - val_loss: 0.2420\n",
      "Epoch 22/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 39ms/step - loss: 0.2868 - val_loss: 0.2416\n",
      "Epoch 23/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 38ms/step - loss: 0.2810 - val_loss: 0.2411\n",
      "Epoch 24/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 39ms/step - loss: 0.2816 - val_loss: 0.2387\n",
      "Epoch 25/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 38ms/step - loss: 0.2842 - val_loss: 0.2386\n",
      "Epoch 26/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 37ms/step - loss: 0.2768 - val_loss: 0.2377\n",
      "Epoch 27/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 39ms/step - loss: 0.2858 - val_loss: 0.2365\n",
      "Epoch 28/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 36ms/step - loss: 0.2745 - val_loss: 0.2348\n",
      "Epoch 29/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 37ms/step - loss: 0.2694 - val_loss: 0.2344\n",
      "Epoch 30/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 38ms/step - loss: 0.2705 - val_loss: 0.2311\n",
      "Epoch 31/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 37ms/step - loss: 0.2673 - val_loss: 0.2327\n",
      "Epoch 32/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 38ms/step - loss: 0.2661 - val_loss: 0.2306\n",
      "Epoch 33/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 37ms/step - loss: 0.2697 - val_loss: 0.2280\n",
      "Epoch 34/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 36ms/step - loss: 0.2644 - val_loss: 0.2285\n",
      "Epoch 35/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 38ms/step - loss: 0.2617 - val_loss: 0.2256\n",
      "Epoch 36/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 37ms/step - loss: 0.2685 - val_loss: 0.2252\n",
      "Epoch 37/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 40ms/step - loss: 0.2612 - val_loss: 0.2287\n",
      "Epoch 38/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 40ms/step - loss: 0.2595 - val_loss: 0.2240\n",
      "Epoch 39/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 38ms/step - loss: 0.2627 - val_loss: 0.2236\n",
      "Epoch 40/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 38ms/step - loss: 0.2596 - val_loss: 0.2226\n",
      "Epoch 41/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 37ms/step - loss: 0.2576 - val_loss: 0.2216\n",
      "Epoch 42/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 37ms/step - loss: 0.2587 - val_loss: 0.2232\n",
      "Epoch 43/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 39ms/step - loss: 0.2570 - val_loss: 0.2212\n",
      "Epoch 44/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 37ms/step - loss: 0.2523 - val_loss: 0.2239\n",
      "Epoch 45/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 39ms/step - loss: 0.2587 - val_loss: 0.2215\n",
      "Epoch 46/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 37ms/step - loss: 0.2596 - val_loss: 0.2216\n",
      "Epoch 47/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 38ms/step - loss: 0.2571 - val_loss: 0.2207\n",
      "Epoch 48/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 39ms/step - loss: 0.2556 - val_loss: 0.2196\n",
      "Epoch 49/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 38ms/step - loss: 0.2500 - val_loss: 0.2193\n",
      "Epoch 50/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 39ms/step - loss: 0.2510 - val_loss: 0.2178\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step\n",
      "Fold 2, Loss: 0.2178162932395935, RMSE Max: 0.45991458094137927, RMSE Min: 0.4734037874187873\n",
      "Epoch 1/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 30ms/step - loss: 0.9442 - val_loss: 0.3958\n",
      "Epoch 2/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 34ms/step - loss: 0.5115 - val_loss: 0.3302\n",
      "Epoch 3/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - loss: 0.4388 - val_loss: 0.3112\n",
      "Epoch 4/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - loss: 0.4271 - val_loss: 0.3090\n",
      "Epoch 5/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 37ms/step - loss: 0.3951 - val_loss: 0.2991\n",
      "Epoch 6/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - loss: 0.3858 - val_loss: 0.2965\n",
      "Epoch 7/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 38ms/step - loss: 0.3631 - val_loss: 0.2833\n",
      "Epoch 8/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - loss: 0.3572 - val_loss: 0.2807\n",
      "Epoch 9/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 38ms/step - loss: 0.3458 - val_loss: 0.2753\n",
      "Epoch 10/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 34ms/step - loss: 0.3381 - val_loss: 0.2690\n",
      "Epoch 11/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - loss: 0.3317 - val_loss: 0.2601\n",
      "Epoch 12/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 35ms/step - loss: 0.3293 - val_loss: 0.2574\n",
      "Epoch 13/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - loss: 0.3207 - val_loss: 0.2541\n",
      "Epoch 14/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 35ms/step - loss: 0.3168 - val_loss: 0.2533\n",
      "Epoch 15/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 37ms/step - loss: 0.3097 - val_loss: 0.2462\n",
      "Epoch 16/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 37ms/step - loss: 0.2994 - val_loss: 0.2427\n",
      "Epoch 17/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 37ms/step - loss: 0.3029 - val_loss: 0.2444\n",
      "Epoch 18/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 37ms/step - loss: 0.2905 - val_loss: 0.2378\n",
      "Epoch 19/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - loss: 0.2883 - val_loss: 0.2370\n",
      "Epoch 20/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 37ms/step - loss: 0.2838 - val_loss: 0.2373\n",
      "Epoch 21/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 34ms/step - loss: 0.2829 - val_loss: 0.2315\n",
      "Epoch 22/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 35ms/step - loss: 0.2811 - val_loss: 0.2310\n",
      "Epoch 23/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 34ms/step - loss: 0.2815 - val_loss: 0.2327\n",
      "Epoch 24/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - loss: 0.2739 - val_loss: 0.2323\n",
      "Epoch 25/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - loss: 0.2738 - val_loss: 0.2289\n",
      "Epoch 26/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - loss: 0.2682 - val_loss: 0.2268\n",
      "Epoch 27/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 35ms/step - loss: 0.2721 - val_loss: 0.2256\n",
      "Epoch 28/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 37ms/step - loss: 0.2664 - val_loss: 0.2270\n",
      "Epoch 29/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 37ms/step - loss: 0.2642 - val_loss: 0.2261\n",
      "Epoch 30/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - loss: 0.2639 - val_loss: 0.2245\n",
      "Epoch 31/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 35ms/step - loss: 0.2584 - val_loss: 0.2202\n",
      "Epoch 32/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 34ms/step - loss: 0.2589 - val_loss: 0.2220\n",
      "Epoch 33/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 35ms/step - loss: 0.2548 - val_loss: 0.2199\n",
      "Epoch 34/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 34ms/step - loss: 0.2549 - val_loss: 0.2201\n",
      "Epoch 35/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - loss: 0.2542 - val_loss: 0.2209\n",
      "Epoch 36/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 35ms/step - loss: 0.2602 - val_loss: 0.2181\n",
      "Epoch 37/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - loss: 0.2556 - val_loss: 0.2182\n",
      "Epoch 38/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 35ms/step - loss: 0.2538 - val_loss: 0.2176\n",
      "Epoch 39/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - loss: 0.2488 - val_loss: 0.2185\n",
      "Epoch 40/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 35ms/step - loss: 0.2519 - val_loss: 0.2163\n",
      "Epoch 41/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 35ms/step - loss: 0.2452 - val_loss: 0.2144\n",
      "Epoch 42/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 33ms/step - loss: 0.2461 - val_loss: 0.2163\n",
      "Epoch 43/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - loss: 0.2430 - val_loss: 0.2149\n",
      "Epoch 44/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 34ms/step - loss: 0.2462 - val_loss: 0.2152\n",
      "Epoch 45/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - loss: 0.2487 - val_loss: 0.2162\n",
      "Epoch 46/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - loss: 0.2395 - val_loss: 0.2128\n",
      "Epoch 47/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - loss: 0.2394 - val_loss: 0.2126\n",
      "Epoch 48/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 37ms/step - loss: 0.2422 - val_loss: 0.2129\n",
      "Epoch 49/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - loss: 0.2390 - val_loss: 0.2155\n",
      "Epoch 50/50\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - loss: 0.2418 - val_loss: 0.2160\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n",
      "Fold 3, Loss: 0.21597076952457428, RMSE Max: 0.4512046483586094, RMSE Min: 0.4778660498050247\n",
      "Epoch 1/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 29ms/step - loss: 0.9857 - val_loss: 0.3643\n",
      "Epoch 2/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 34ms/step - loss: 0.4655 - val_loss: 0.3034\n",
      "Epoch 3/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 34ms/step - loss: 0.3926 - val_loss: 0.2860\n",
      "Epoch 4/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 34ms/step - loss: 0.3699 - val_loss: 0.2751\n",
      "Epoch 5/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 34ms/step - loss: 0.3442 - val_loss: 0.2657\n",
      "Epoch 6/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 33ms/step - loss: 0.3402 - val_loss: 0.2629\n",
      "Epoch 7/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 34ms/step - loss: 0.3283 - val_loss: 0.2582\n",
      "Epoch 8/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 34ms/step - loss: 0.3133 - val_loss: 0.2493\n",
      "Epoch 9/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 33ms/step - loss: 0.3060 - val_loss: 0.2493\n",
      "Epoch 10/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 34ms/step - loss: 0.2996 - val_loss: 0.2454\n",
      "Epoch 11/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 34ms/step - loss: 0.2933 - val_loss: 0.2464\n",
      "Epoch 12/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 33ms/step - loss: 0.2877 - val_loss: 0.2344\n",
      "Epoch 13/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 34ms/step - loss: 0.2905 - val_loss: 0.2421\n",
      "Epoch 14/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 34ms/step - loss: 0.2835 - val_loss: 0.2315\n",
      "Epoch 15/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 33ms/step - loss: 0.2773 - val_loss: 0.2343\n",
      "Epoch 16/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 34ms/step - loss: 0.2806 - val_loss: 0.2299\n",
      "Epoch 17/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 34ms/step - loss: 0.2719 - val_loss: 0.2270\n",
      "Epoch 18/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 34ms/step - loss: 0.2775 - val_loss: 0.2292\n",
      "Epoch 19/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 34ms/step - loss: 0.2685 - val_loss: 0.2287\n",
      "Epoch 20/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 34ms/step - loss: 0.2683 - val_loss: 0.2272\n",
      "Epoch 21/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 33ms/step - loss: 0.2724 - val_loss: 0.2256\n",
      "Epoch 22/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 34ms/step - loss: 0.2631 - val_loss: 0.2228\n",
      "Epoch 23/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 34ms/step - loss: 0.2668 - val_loss: 0.2220\n",
      "Epoch 24/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 34ms/step - loss: 0.2616 - val_loss: 0.2198\n",
      "Epoch 25/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 34ms/step - loss: 0.2615 - val_loss: 0.2219\n",
      "Epoch 26/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 34ms/step - loss: 0.2532 - val_loss: 0.2198\n",
      "Epoch 27/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 34ms/step - loss: 0.2499 - val_loss: 0.2191\n",
      "Epoch 28/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 35ms/step - loss: 0.2549 - val_loss: 0.2182\n",
      "Epoch 29/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 35ms/step - loss: 0.2596 - val_loss: 0.2174\n",
      "Epoch 30/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 33ms/step - loss: 0.2547 - val_loss: 0.2197\n",
      "Epoch 31/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 35ms/step - loss: 0.2548 - val_loss: 0.2180\n",
      "Epoch 32/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 34ms/step - loss: 0.2541 - val_loss: 0.2174\n",
      "Epoch 33/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 35ms/step - loss: 0.2534 - val_loss: 0.2195\n",
      "Epoch 34/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 35ms/step - loss: 0.2499 - val_loss: 0.2169\n",
      "Epoch 35/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - loss: 0.2509 - val_loss: 0.2202\n",
      "Epoch 36/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 35ms/step - loss: 0.2558 - val_loss: 0.2149\n",
      "Epoch 37/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 34ms/step - loss: 0.2434 - val_loss: 0.2151\n",
      "Epoch 38/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 35ms/step - loss: 0.2475 - val_loss: 0.2129\n",
      "Epoch 39/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 34ms/step - loss: 0.2416 - val_loss: 0.2182\n",
      "Epoch 40/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 34ms/step - loss: 0.2463 - val_loss: 0.2170\n",
      "Epoch 41/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 35ms/step - loss: 0.2431 - val_loss: 0.2201\n",
      "Epoch 42/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 35ms/step - loss: 0.2497 - val_loss: 0.2186\n",
      "Epoch 43/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 34ms/step - loss: 0.2429 - val_loss: 0.2139\n",
      "Epoch 44/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 34ms/step - loss: 0.2475 - val_loss: 0.2169\n",
      "Epoch 45/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 34ms/step - loss: 0.2404 - val_loss: 0.2143\n",
      "Epoch 46/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 34ms/step - loss: 0.2455 - val_loss: 0.2169\n",
      "Epoch 47/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 34ms/step - loss: 0.2466 - val_loss: 0.2163\n",
      "Epoch 48/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 34ms/step - loss: 0.2456 - val_loss: 0.2126\n",
      "Epoch 49/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 35ms/step - loss: 0.2440 - val_loss: 0.2188\n",
      "Epoch 50/50\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 34ms/step - loss: 0.2416 - val_loss: 0.2120\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n",
      "Fold 4, Loss: 0.21200938522815704, RMSE Max: 0.44360899472610793, RMSE Min: 0.47668624586333164\n",
      "Epoch 1/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 28ms/step - loss: 0.9549 - val_loss: 0.3531\n",
      "Epoch 2/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 33ms/step - loss: 0.4232 - val_loss: 0.3023\n",
      "Epoch 3/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 34ms/step - loss: 0.3783 - val_loss: 0.2880\n",
      "Epoch 4/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 34ms/step - loss: 0.3486 - val_loss: 0.2811\n",
      "Epoch 5/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 34ms/step - loss: 0.3284 - val_loss: 0.2669\n",
      "Epoch 6/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 34ms/step - loss: 0.3146 - val_loss: 0.2567\n",
      "Epoch 7/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 34ms/step - loss: 0.2949 - val_loss: 0.2522\n",
      "Epoch 8/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 34ms/step - loss: 0.2896 - val_loss: 0.2491\n",
      "Epoch 9/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 34ms/step - loss: 0.2768 - val_loss: 0.2454\n",
      "Epoch 10/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 34ms/step - loss: 0.2682 - val_loss: 0.2421\n",
      "Epoch 11/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 34ms/step - loss: 0.2692 - val_loss: 0.2407\n",
      "Epoch 12/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 34ms/step - loss: 0.2631 - val_loss: 0.2394\n",
      "Epoch 13/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 30ms/step - loss: 0.2611 - val_loss: 0.2395\n",
      "Epoch 14/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 30ms/step - loss: 0.2567 - val_loss: 0.2372\n",
      "Epoch 15/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 31ms/step - loss: 0.2525 - val_loss: 0.2357\n",
      "Epoch 16/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 30ms/step - loss: 0.2487 - val_loss: 0.2363\n",
      "Epoch 17/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 30ms/step - loss: 0.2474 - val_loss: 0.2350\n",
      "Epoch 18/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 31ms/step - loss: 0.2546 - val_loss: 0.2345\n",
      "Epoch 19/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 30ms/step - loss: 0.2493 - val_loss: 0.2340\n",
      "Epoch 20/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 30ms/step - loss: 0.2427 - val_loss: 0.2362\n",
      "Epoch 21/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 30ms/step - loss: 0.2430 - val_loss: 0.2335\n",
      "Epoch 22/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 29ms/step - loss: 0.2454 - val_loss: 0.2322\n",
      "Epoch 23/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 29ms/step - loss: 0.2448 - val_loss: 0.2328\n",
      "Epoch 24/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 30ms/step - loss: 0.2430 - val_loss: 0.2365\n",
      "Epoch 25/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 31ms/step - loss: 0.2427 - val_loss: 0.2322\n",
      "Epoch 26/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 29ms/step - loss: 0.2426 - val_loss: 0.2323\n",
      "Epoch 27/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 37ms/step - loss: 0.2404 - val_loss: 0.2325\n",
      "Epoch 28/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 34ms/step - loss: 0.2368 - val_loss: 0.2333\n",
      "Epoch 29/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 34ms/step - loss: 0.2388 - val_loss: 0.2365\n",
      "Epoch 30/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 34ms/step - loss: 0.2364 - val_loss: 0.2310\n",
      "Epoch 31/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 34ms/step - loss: 0.2342 - val_loss: 0.2308\n",
      "Epoch 32/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 34ms/step - loss: 0.2378 - val_loss: 0.2312\n",
      "Epoch 33/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 34ms/step - loss: 0.2358 - val_loss: 0.2352\n",
      "Epoch 34/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 34ms/step - loss: 0.2334 - val_loss: 0.2325\n",
      "Epoch 35/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 34ms/step - loss: 0.2358 - val_loss: 0.2319\n",
      "Epoch 36/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 34ms/step - loss: 0.2333 - val_loss: 0.2312\n",
      "Epoch 37/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 34ms/step - loss: 0.2343 - val_loss: 0.2309\n",
      "Epoch 38/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 33ms/step - loss: 0.2363 - val_loss: 0.2313\n",
      "Epoch 39/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 33ms/step - loss: 0.2334 - val_loss: 0.2308\n",
      "Epoch 40/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 34ms/step - loss: 0.2350 - val_loss: 0.2314\n",
      "Epoch 41/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 34ms/step - loss: 0.2279 - val_loss: 0.2309\n",
      "Epoch 42/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 34ms/step - loss: 0.2310 - val_loss: 0.2300\n",
      "Epoch 43/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 35ms/step - loss: 0.2286 - val_loss: 0.2290\n",
      "Epoch 44/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 34ms/step - loss: 0.2300 - val_loss: 0.2309\n",
      "Epoch 45/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 35ms/step - loss: 0.2319 - val_loss: 0.2325\n",
      "Epoch 46/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 33ms/step - loss: 0.2304 - val_loss: 0.2299\n",
      "Epoch 47/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 34ms/step - loss: 0.2336 - val_loss: 0.2340\n",
      "Epoch 48/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 41ms/step - loss: 0.2310 - val_loss: 0.2306\n",
      "Epoch 49/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 34ms/step - loss: 0.2303 - val_loss: 0.2312\n",
      "Epoch 50/50\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 34ms/step - loss: 0.2320 - val_loss: 0.2318\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n",
      "Fold 5, Loss: 0.23178541660308838, RMSE Max: 0.4738048241434873, RMSE Min: 0.48895788640000354\n",
      "Average Loss: 0.22406767606735228\n",
      "Average RMSE of max_temp: 0.46028608358961226\n",
      "Average RMSE of min_temp: 0.48573143855131107\n",
      "Average MSE of max_temp: 0.21200378607797035\n",
      "Average MSE of min_temp: 0.23613156514578196\n",
      "Average MAE of max_temp: 0.3565548326651914\n",
      "Average MAE of min_temp: 0.3925267222006118\n",
      "Average R² of max_temp: 0.7831549395741428\n",
      "Average R² of min_temp: 0.7560477518917192\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input, LayerNormalization, MultiHeadAttention, Dropout, GlobalAveragePooling1D, Add\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Reshape\n",
    "\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    # Normalization and Attention\n",
    "    x = LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    x = MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(x, x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    res = Add()([x, inputs])\n",
    "\n",
    "    # Feed Forward Part\n",
    "    x = LayerNormalization(epsilon=1e-6)(res)\n",
    "    x = Dense(ff_dim, activation=\"relu\")(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = Dense(inputs.shape[-1])(x)\n",
    "    return Add()([x, res])\n",
    "\n",
    "# Transformer\n",
    "def build_model2(input_shape, head_size, num_heads, ff_dim, num_transformer_blocks, mlp_units, mlp_dropout=0, dropout=0):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = inputs\n",
    "    for _ in range(num_transformer_blocks):\n",
    "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
    "\n",
    "    x = GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
    "    for dim in mlp_units:\n",
    "        x = Dense(dim, activation=\"relu\")(x)\n",
    "        x = Dropout(mlp_dropout)(x)\n",
    "    x = Dense(output_steps * num_targets)(x)  # output_steps * num_targets\n",
    "    outputs = x\n",
    "    return Model(inputs, outputs)\n",
    "\n",
    "# Time window data creation function\n",
    "def create_time_series10(X, y, input_steps=10, output_steps=3):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - input_steps - output_steps + 1):\n",
    "        Xs.append(X[i:(i + input_steps)])\n",
    "        ys.append(y[(i + input_steps):(i + input_steps + output_steps)])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "# Cross validation configuration\n",
    "n_splits = 5\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "results = {\n",
    "    'loss': [],\n",
    "    'rmse_max': [],\n",
    "    'rmse_min': [],\n",
    "    'mse_max': [],\n",
    "    'mse_min': [],\n",
    "    'mae_max': [],\n",
    "    'mae_min': [],\n",
    "    'r2_max': [],\n",
    "    'r2_min': []\n",
    "}\n",
    "\n",
    "input_steps = 10\n",
    "output_steps = 3\n",
    "\n",
    "X = df_weather[['global_radiation', 'diff_temp', 'mean_temp']].values\n",
    "y = df_weather[['max_temp', 'min_temp']].values\n",
    "\n",
    "num_features = X.shape[1]\n",
    "num_targets = y.shape[1]\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(tscv.split(X)):\n",
    "\n",
    "    X_train, X_val = X[train_index], X[val_index]\n",
    "    y_train, y_val = y[train_index], y[val_index]\n",
    "    \n",
    "\n",
    "    X_train_rnn10, y_train_rnn10 = create_time_series10(X_train, y_train, input_steps, output_steps)\n",
    "    X_val_rnn10, y_val_rnn10 = create_time_series10(X_val, y_val, input_steps, output_steps)\n",
    "\n",
    "    y_train_rnn101 = y_train_rnn10.reshape((y_train_rnn10.shape[0], output_steps * num_targets))\n",
    "    y_val_rnn101 = y_val_rnn10.reshape((y_val_rnn10.shape[0], output_steps * num_targets))\n",
    "    \n",
    "\n",
    "    model2 = build_model2(\n",
    "        (input_steps, num_features),\n",
    "        head_size=256,\n",
    "        num_heads=4,\n",
    "        ff_dim=4,\n",
    "        num_transformer_blocks=4,\n",
    "        mlp_units=[128],\n",
    "        mlp_dropout=0.4,\n",
    "        dropout=0.25,\n",
    "    )\n",
    "\n",
    "    model2.compile(loss=\"mean_squared_error\", optimizer=Adam(learning_rate=1e-4))\n",
    "    model2.fit(X_train_rnn10, y_train_rnn101,\n",
    "               validation_data=(X_val_rnn10, y_val_rnn101),\n",
    "               epochs=50,\n",
    "               batch_size=32,\n",
    "               verbose=1)\n",
    "\n",
    "    val_loss = model2.evaluate(X_val_rnn10, y_val_rnn101, verbose=0)\n",
    "    y_val_pred = model2.predict(X_val_rnn10)\n",
    "    \n",
    "    y_val_rnn_flat = y_val_rnn10.reshape(-1, num_targets)\n",
    "    y_val_pred_flat = y_val_pred.reshape(-1, num_targets)\n",
    "    \n",
    "    mse_max = mean_squared_error(y_val_rnn_flat[:, 0], y_val_pred_flat[:, 0])\n",
    "    mse_min = mean_squared_error(y_val_rnn_flat[:, 1], y_val_pred_flat[:, 1])\n",
    "    rmse_max = np.sqrt(mse_max)\n",
    "    rmse_min = np.sqrt(mse_min)\n",
    "    mae_max = mean_absolute_error(y_val_rnn_flat[:, 0], y_val_pred_flat[:, 0])\n",
    "    mae_min = mean_absolute_error(y_val_rnn_flat[:, 1], y_val_pred_flat[:, 1])\n",
    "    r2_max = r2_score(y_val_rnn_flat[:, 0], y_val_pred_flat[:, 0])\n",
    "    r2_min = r2_score(y_val_rnn_flat[:, 1], y_val_pred_flat[:, 1])\n",
    "    \n",
    "    results['loss'].append(val_loss)\n",
    "    results['rmse_max'].append(rmse_max)\n",
    "    results['rmse_min'].append(rmse_min)\n",
    "    results['mse_max'].append(mse_max)\n",
    "    results['mse_min'].append(mse_min)\n",
    "    results['mae_max'].append(mae_max)\n",
    "    results['mae_min'].append(mae_min)\n",
    "    results['r2_max'].append(r2_max)\n",
    "    results['r2_min'].append(r2_min)\n",
    "    \n",
    "    print(f'Fold {fold + 1}, Loss: {val_loss}, RMSE Max: {rmse_max}, RMSE Min: {rmse_min}')\n",
    "\n",
    "\n",
    "print(f'Average Loss: {np.mean(results[\"loss\"])}')\n",
    "print(f'Average RMSE of max_temp: {np.mean(results[\"rmse_max\"])}')\n",
    "print(f'Average RMSE of min_temp: {np.mean(results[\"rmse_min\"])}')\n",
    "print(f'Average MSE of max_temp: {np.mean(results[\"mse_max\"])}')\n",
    "print(f'Average MSE of min_temp: {np.mean(results[\"mse_min\"])}')\n",
    "print(f'Average MAE of max_temp: {np.mean(results[\"mae_max\"])}')\n",
    "print(f'Average MAE of min_temp: {np.mean(results[\"mae_min\"])}')\n",
    "print(f'Average R² of max_temp: {np.mean(results[\"r2_max\"])}')\n",
    "print(f'Average R² of min_temp: {np.mean(results[\"r2_min\"])}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    Ⅲ.Based on the weather of the past thirty days, predict the highest and lowest temperatures for the thirty-first  day\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Baseline model - Moving average\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "window_size30 = 30\n",
    "\n",
    "X = df_weather[['date', 'global_radiation','diff_temp','mean_temp']]  # 特征\n",
    "y = df_weather[['max_temp', 'min_temp']]       # 目标变量\n",
    "\n",
    "X_train_val, X_test, y_train_val, y_test = split_data(X, y, test_size=0.2)\n",
    "\n",
    "X_train, X_val, y_train, y_val = split_data(X_train_val, y_train_val, test_size=0.25)\n",
    "\n",
    "X_train_ma30 = np.zeros((X_train.shape[0] - window_size30 + 1, X_train.shape[1]))\n",
    "for i in range(X_train.shape[1]):\n",
    "    X_train_ma30[:, i] = moving_average(X_train.iloc[:, i], window_size30)\n",
    "\n",
    "y_train_max_ma30 = moving_average(y_train['max_temp'].values, window_size30)\n",
    "y_train_min_ma30 = moving_average(y_train['min_temp'].values, window_size30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and train\n",
    "model_max_temp30 = MovingAverageModel()\n",
    "model_min_temp30 = MovingAverageModel()\n",
    "model_max_temp30.fit(X_train_ma30, y_train_max_ma30)\n",
    "model_min_temp30.fit(X_train_ma30, y_train_min_ma30)\n",
    "\n",
    "# evaluate in val\n",
    "X_val_ma30 = np.zeros((X_val.shape[0] - window_size30 + 1, X_val.shape[1]))\n",
    "for i in range(X_val.shape[1]):\n",
    "    X_val_ma30[:, i] = moving_average(X_val.iloc[:, i], window_size30)\n",
    "\n",
    "y_val_pred_max30 = model_max_temp30.predict(X_val_ma30)\n",
    "y_val_pred_min30 = model_min_temp30.predict(X_val_ma30)\n",
    "\n",
    "# pred in test\n",
    "X_test_ma30 = np.zeros((X_test.shape[0] - window_size30 + 1, X_test.shape[1]))\n",
    "for i in range(X_test.shape[1]):\n",
    "    X_test_ma30[:, i] = moving_average(X_test.iloc[:, i], window_size30)\n",
    "\n",
    "y_test_pred_max_ma30 = model_max_temp30.predict(X_test_ma30)\n",
    "y_test_pred_min_ma30 = model_min_temp30.predict(X_test_ma30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE of 'max_temp' in test set: 0.53\n",
      "RMSE of 'min_temp' in test set: 0.59\n",
      "MSE of 'max_temp' in test set: 0.28\n",
      "MSE of 'min_temp' in test set: 0.34\n",
      "MAE of 'max_temp' in test set: 0.42\n",
      "MAE of 'min_temp' in test set: 0.47\n",
      "R² of 'max_temp' in test set: 0.71\n",
      "R² of 'min_temp' in test set: 0.64\n"
     ]
    }
   ],
   "source": [
    "# 评evaluate\n",
    "\n",
    "# RMSE\n",
    "mse_max_ma30 = mean_squared_error(y_test['max_temp'].values[window_size30 - 1:], y_test_pred_max_ma30)\n",
    "mse_min_ma30 = mean_squared_error(y_test['min_temp'].values[window_size30 - 1:], y_test_pred_min_ma30)\n",
    "\n",
    "# MSE\n",
    "mse_max_ma30 = mean_squared_error(y_test['max_temp'].values[window_size30 - 1:], y_test_pred_max_ma30)\n",
    "mse_min_ma30 = mean_squared_error(y_test['min_temp'].values[window_size30 - 1:], y_test_pred_min_ma30)\n",
    "\n",
    "# MAE\n",
    "mae_max_ma30 = mean_absolute_error(y_test['max_temp'].values[window_size30 - 1:], y_test_pred_max_ma30)\n",
    "mae_min_ma30 = mean_absolute_error(y_test['min_temp'].values[window_size30 - 1:], y_test_pred_min_ma30)\n",
    "\n",
    "# R2\n",
    "r2_max_ma30 = r2_score(y_test['max_temp'].values[window_size30 - 1:], y_test_pred_max_ma30)\n",
    "r2_min_ma30 = r2_score(y_test['min_temp'].values[window_size30 - 1:], y_test_pred_min_ma30)\n",
    "\n",
    "print(\"RMSE of 'max_temp' in test set: {:.2f}\".format(np.sqrt(mse_max_ma30)))\n",
    "print(\"RMSE of 'min_temp' in test set: {:.2f}\".format(np.sqrt(mse_min_ma30)))\n",
    "\n",
    "print(\"MSE of 'max_temp' in test set: {:.2f}\".format(mse_max_ma30))\n",
    "print(\"MSE of 'min_temp' in test set: {:.2f}\".format(mse_min_ma30))\n",
    "\n",
    "print(\"MAE of 'max_temp' in test set: {:.2f}\".format(mae_max_ma30))\n",
    "print(\"MAE of 'min_temp' in test set: {:.2f}\".format(mae_min_ma30))\n",
    "\n",
    "print(\"R² of 'max_temp' in test set: {:.2f}\".format(r2_max_ma30))\n",
    "print(\"R² of 'min_temp' in test set: {:.2f}\".format(r2_min_ma30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation results:\n",
      "Average MSE of 'max_temp': 0.00\n",
      "Average MSE of 'min_temp': 0.00\n",
      "Average RMSE of 'max_temp': 0.03\n",
      "Average RMSE of 'min_temp': 0.03\n",
      "Average MAE of 'max_temp': 0.02\n",
      "Average MAE of 'min_temp': 0.03\n",
      "Average R² of 'max_temp': 0.97\n",
      "Average R² of 'min_temp': 0.97\n"
     ]
    }
   ],
   "source": [
    "## cross-val\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "window_size = 30\n",
    "\n",
    "mse_scores_max = []\n",
    "mse_scores_min = []\n",
    "rmse_scores_max = []\n",
    "rmse_scores_min = []\n",
    "mae_scores_max = []\n",
    "mae_scores_min = []\n",
    "r2_scores_max = []\n",
    "r2_scores_min = []\n",
    "\n",
    "X_train = X_train.to_numpy()\n",
    "y_train = y_train.to_numpy()\n",
    "\n",
    "\n",
    "for train_index, val_index in tscv.split(X_train):\n",
    "\n",
    "    X_train_cv, X_val_cv = X_train[train_index], X_train[val_index]\n",
    "    y_train_max_cv, y_val_max_cv = y_train[train_index, 0], y_train[val_index, 0]\n",
    "    y_train_min_cv, y_val_min_cv = y_train[train_index, 1], y_train[val_index, 1]\n",
    "    \n",
    "\n",
    "    X_train_ma = np.zeros((X_train_cv.shape[0] - window_size + 1, X_train_cv.shape[1]))\n",
    "    X_val_ma = np.zeros((X_val_cv.shape[0] - window_size + 1, X_val_cv.shape[1]))\n",
    "    for i in range(X_train_cv.shape[1]):\n",
    "        X_train_ma[:, i] = moving_average(X_train_cv[:, i], window_size)\n",
    "        X_val_ma[:, i] = moving_average(X_val_cv[:, i], window_size)\n",
    "    \n",
    "    y_train_max_ma = moving_average(y_train_max_cv, window_size)\n",
    "    y_train_min_ma = moving_average(y_train_min_cv, window_size)\n",
    "    \n",
    "    # Create the corresponding moving average for the validation set label\n",
    "    y_val_max_ma = moving_average(y_val_max_cv, window_size)\n",
    "    y_val_min_ma = moving_average(y_val_min_cv, window_size)\n",
    "    \n",
    "    model_max_temp = MovingAverageModel()\n",
    "    model_min_temp = MovingAverageModel()\n",
    "    model_max_temp.fit(X_train_ma, y_train_max_ma)\n",
    "    model_min_temp.fit(X_train_ma, y_train_min_ma)\n",
    "    \n",
    "\n",
    "    y_val_pred_max = model_max_temp.predict(X_val_ma)\n",
    "    y_val_pred_min = model_min_temp.predict(X_val_ma)\n",
    "    \n",
    "\n",
    "    mse_max = mean_squared_error(y_val_max_ma, y_val_pred_max)\n",
    "    mse_min = mean_squared_error(y_val_min_ma, y_val_pred_min)\n",
    "    rmse_max = np.sqrt(mse_max)\n",
    "    rmse_min = np.sqrt(mse_min)\n",
    "    mae_max = mean_absolute_error(y_val_max_ma, y_val_pred_max)\n",
    "    mae_min = mean_absolute_error(y_val_min_ma, y_val_pred_min)\n",
    "    r2_max = r2_score(y_val_max_ma, y_val_pred_max)\n",
    "    r2_min = r2_score(y_val_min_ma, y_val_pred_min)\n",
    "    \n",
    "    mse_scores_max.append(mse_max)\n",
    "    mse_scores_min.append(mse_min)\n",
    "    rmse_scores_max.append(rmse_max)\n",
    "    rmse_scores_min.append(rmse_min)\n",
    "    mae_scores_max.append(mae_max)\n",
    "    mae_scores_min.append(mae_min)\n",
    "    r2_scores_max.append(r2_max)\n",
    "    r2_scores_min.append(r2_min)\n",
    "\n",
    "\n",
    "print(\"Cross-validation results:\")\n",
    "print(f\"Average MSE of 'max_temp': {np.mean(mse_scores_max):.2f}\")\n",
    "print(f\"Average MSE of 'min_temp': {np.mean(mse_scores_min):.2f}\")\n",
    "print(f\"Average RMSE of 'max_temp': {np.mean(rmse_scores_max):.2f}\")\n",
    "print(f\"Average RMSE of 'min_temp': {np.mean(rmse_scores_min):.2f}\")\n",
    "print(f\"Average MAE of 'max_temp': {np.mean(mae_scores_max):.2f}\")\n",
    "print(f\"Average MAE of 'min_temp': {np.mean(mae_scores_min):.2f}\")\n",
    "print(f\"Average R² of 'max_temp': {np.mean(r2_scores_max):.2f}\")\n",
    "print(f\"Average R² of 'min_temp': {np.mean(r2_scores_min):.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features_and_target30(df, n_days=30):\n",
    "    df = df.copy()\n",
    "    for i in range(1, n_days + 1):\n",
    "        df[f'global_radiation_{i}d'] = df['global_radiation'].shift(i)\n",
    "        df[f'diff_temp_{i}d'] = df['diff_temp'].shift(i)\n",
    "        df[f'mean_temp_{i}d'] = df['mean_temp'].shift(i)\n",
    "    \n",
    "    df = df.dropna().reset_index(drop=True)\n",
    "    X30 = df[[f'global_radiation_{i}d' for i in range(1, n_days + 1)] +\n",
    "           [f'diff_temp_{i}d' for i in range(1, n_days + 1)] +\n",
    "           [f'mean_temp_{i}d' for i in range(1, n_days + 1)]]\n",
    "    y_max_rf30 = df['max_temp']\n",
    "    y_min_rf30 = df['min_temp']\n",
    "    return X30, y_max_rf30, y_min_rf30\n",
    "\n",
    "X_train_rf30, y_train_max_rf30, y_train_min_rf30 = create_features_and_target30(train_df_weather)\n",
    "X_val_rf30, y_val_max_rf30, y_val_min_rf30 = create_features_and_target30(val_df_weather)\n",
    "X_test_rf30, y_test_max_rf30, y_test_min_rf30 = create_features_and_target30(test_df_weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and train\n",
    "rf_max30 = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_max30.fit(X_train_rf30, y_train_max_rf30)\n",
    "\n",
    "rf_min30 = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_min30.fit(X_train_rf30, y_train_min_rf30)\n",
    "\n",
    "y_val_pred_max_rf30 = rf_max30.predict(X_val_rf30)\n",
    "y_val_pred_min_rf30 = rf_min30.predict(X_val_rf30)\n",
    "\n",
    "y_test_pred_max_rf30 = rf_max30.predict(X_test_rf30)\n",
    "y_test_pred_min_rf30 = rf_min30.predict(X_test_rf30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE of 'max_temp' in test set: 0.4036425938153924\n",
      "RMSE of 'min_temp' in test set: 0.39299474804080636\n",
      "MSE of 'max_temp' in test set: 0.16292734354201788\n",
      "MSE of 'min_temp' in test set: 0.15444487198765686\n",
      "MAE of 'max_temp' in test set: 0.3101061335994656\n",
      "MAE of 'min_temp' in test set: 0.31663187447449526\n",
      "R² of 'max_temp' in test set: 0.8354561484255199\n",
      "R² of 'min_temp' in test set: 0.8366807075635517\n"
     ]
    }
   ],
   "source": [
    "# evaluate\n",
    "\n",
    "# RMSE)\n",
    "rmse_max_rf30 = np.sqrt(mean_squared_error(y_test_max_rf30, y_test_pred_max_rf30))\n",
    "rmse_min_rf30 = np.sqrt(mean_squared_error(y_test_min_rf30, y_test_pred_min_rf30))\n",
    "\n",
    "# MSE\n",
    "mse_max_rf30 = mean_squared_error(y_test_max_rf30, y_test_pred_max_rf30)\n",
    "mse_min_rf30 = mean_squared_error(y_test_min_rf30, y_test_pred_min_rf30)\n",
    "\n",
    "# MAE\n",
    "mae_max_rf30 = mean_absolute_error(y_test_max_rf30, y_test_pred_max_rf30)\n",
    "mae_min_rf30 = mean_absolute_error(y_test_min_rf30, y_test_pred_min_rf30)\n",
    "\n",
    "# R²\n",
    "r2_max_rf30 = r2_score(y_test_max_rf30, y_test_pred_max_rf30)\n",
    "r2_min_rf30 = r2_score(y_test_min_rf30, y_test_pred_min_rf30)\n",
    "\n",
    "print(f\"RMSE of 'max_temp' in test set: {rmse_max_rf30}\")\n",
    "print(f\"RMSE of 'min_temp' in test set: {rmse_min_rf30}\")\n",
    "\n",
    "print(f\"MSE of 'max_temp' in test set: {mse_max_rf30}\")\n",
    "print(f\"MSE of 'min_temp' in test set: {mse_min_rf30}\")\n",
    "\n",
    "print(f\"MAE of 'max_temp' in test set: {mae_max_rf30}\")\n",
    "print(f\"MAE of 'min_temp' in test set: {mae_min_rf30}\")\n",
    "\n",
    "print(f\"R² of 'max_temp' in test set: {r2_max_rf30}\")\n",
    "print(f\"R² of 'min_temp' in test set: {r2_min_rf30}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation results:\n",
      "Average MSE of 'max_temp': 0.15\n",
      "Average MSE of 'min_temp': 0.14\n",
      "Average RMSE of 'max_temp': 0.39\n",
      "Average RMSE of 'min_temp': 0.38\n",
      "Average MAE of 'max_temp': 0.30\n",
      "Average MAE of 'min_temp': 0.31\n",
      "Average R² of 'max_temp': 0.85\n",
      "Average R² of 'min_temp': 0.85\n"
     ]
    }
   ],
   "source": [
    "# cross-val\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "\n",
    "X, y_max, y_min = create_features_and_target30(df_weather)\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "mse_scores_max = []\n",
    "mse_scores_min = []\n",
    "rmse_scores_max = []\n",
    "rmse_scores_min = []\n",
    "mae_scores_max = []\n",
    "mae_scores_min = []\n",
    "r2_scores_max = []\n",
    "r2_scores_min = []\n",
    "\n",
    "\n",
    "for train_index, val_index in tscv.split(X):\n",
    "\n",
    "    X_train_cv, X_val_cv = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_train_max_cv, y_val_max_cv = y_max[train_index], y_max[val_index]\n",
    "    y_train_min_cv, y_val_min_cv = y_min[train_index], y_min[val_index]\n",
    "    \n",
    "\n",
    "    rf_max = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    rf_max.fit(X_train_cv, y_train_max_cv)\n",
    "    \n",
    "    rf_min = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    rf_min.fit(X_train_cv, y_train_min_cv)\n",
    "    \n",
    "    y_val_pred_max_rf = rf_max.predict(X_val_cv)\n",
    "    y_val_pred_min_rf = rf_min.predict(X_val_cv)\n",
    "    \n",
    "    mse_max = mean_squared_error(y_val_max_cv, y_val_pred_max_rf)\n",
    "    mse_min = mean_squared_error(y_val_min_cv, y_val_pred_min_rf)\n",
    "    rmse_max = np.sqrt(mse_max)\n",
    "    rmse_min = np.sqrt(mse_min)\n",
    "    mae_max = mean_absolute_error(y_val_max_cv, y_val_pred_max_rf)\n",
    "    mae_min = mean_absolute_error(y_val_min_cv, y_val_pred_min_rf)\n",
    "    r2_max = r2_score(y_val_max_cv, y_val_pred_max_rf)\n",
    "    r2_min = r2_score(y_val_min_cv, y_val_pred_min_rf)\n",
    "    \n",
    "    mse_scores_max.append(mse_max)\n",
    "    mse_scores_min.append(mse_min)\n",
    "    rmse_scores_max.append(rmse_max)\n",
    "    rmse_scores_min.append(rmse_min)\n",
    "    mae_scores_max.append(mae_max)\n",
    "    mae_scores_min.append(mae_min)\n",
    "    r2_scores_max.append(r2_max)\n",
    "    r2_scores_min.append(r2_min)\n",
    "\n",
    "\n",
    "print(\"Cross-validation results:\")\n",
    "print(f\"Average MSE of 'max_temp': {np.mean(mse_scores_max):.2f}\")\n",
    "print(f\"Average MSE of 'min_temp': {np.mean(mse_scores_min):.2f}\")\n",
    "print(f\"Average RMSE of 'max_temp': {np.mean(rmse_scores_max):.2f}\")\n",
    "print(f\"Average RMSE of 'min_temp': {np.mean(rmse_scores_min):.2f}\")\n",
    "print(f\"Average MAE of 'max_temp': {np.mean(mae_scores_max):.2f}\")\n",
    "print(f\"Average MAE of 'min_temp': {np.mean(mae_scores_min):.2f}\")\n",
    "print(f\"Average R² of 'max_temp': {np.mean(r2_scores_max):.2f}\")\n",
    "print(f\"Average R² of 'min_temp': {np.mean(r2_scores_min):.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the training set: (9175, 30, 3) (9175, 2)\n",
      "Size of the validation set: (3038, 30, 3) (3038, 2)\n",
      "Size of the testing set: (3038, 30, 3) (3038, 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def create_time_series30(X, y, time_steps=30):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        Xs.append(X[i:(i + time_steps)])\n",
    "        ys.append(y[i + time_steps])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "\n",
    "time_steps30 = 30\n",
    "X_train_rnn30, y_train_rnn30 = create_time_series(train_X_rnn, train_y_rnn, time_steps30)\n",
    "X_val_rnn30, y_val_rnn30 = create_time_series(val_X_rnn, val_y_rnn, time_steps30)\n",
    "X_test_rnn30, y_test_rnn30 = create_time_series(test_X_rnn, test_y_rnn, time_steps30)\n",
    "\n",
    "print(\"Size of the training set:\", X_train_rnn30.shape, y_train_rnn30.shape)\n",
    "print(\"Size of the validation set:\", X_val_rnn30.shape, y_val_rnn30.shape)\n",
    "print(\"Size of the testing set:\", X_test_rnn30.shape, y_test_rnn30.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_23\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_23\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">10,800</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_143 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">102</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_23 (\u001b[38;5;33mLSTM\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │        \u001b[38;5;34m10,800\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_143 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │           \u001b[38;5;34m102\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,902</span> (42.59 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m10,902\u001b[0m (42.59 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,902</span> (42.59 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m10,902\u001b[0m (42.59 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create model\n",
    "model30 = Sequential()\n",
    "model30.add(LSTM(50, activation='relu', input_shape=(time_steps30, len(features))))\n",
    "model30.add(Dense(2))  \n",
    "\n",
    "model30.compile(optimizer='adam', loss='mse', run_eagerly=True)\n",
    "model30.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 71ms/step - loss: 0.3557 - val_loss: 0.1544\n",
      "Epoch 2/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 71ms/step - loss: 0.1489 - val_loss: 0.1365\n",
      "Epoch 3/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 71ms/step - loss: 0.1417 - val_loss: 0.1416\n",
      "Epoch 4/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 71ms/step - loss: 0.1384 - val_loss: 0.1397\n",
      "Epoch 5/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 71ms/step - loss: 0.1358 - val_loss: 0.1315\n",
      "Epoch 6/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 71ms/step - loss: 0.1334 - val_loss: 0.1297\n",
      "Epoch 7/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 71ms/step - loss: 0.1334 - val_loss: 0.1379\n",
      "Epoch 8/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 71ms/step - loss: 0.1335 - val_loss: 0.1368\n",
      "Epoch 9/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 75ms/step - loss: 0.1322 - val_loss: 0.1305\n",
      "Epoch 10/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 71ms/step - loss: 0.1312 - val_loss: 0.1294\n",
      "Epoch 11/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 71ms/step - loss: 0.1296 - val_loss: 0.1338\n",
      "Epoch 12/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 71ms/step - loss: 0.1329 - val_loss: 0.1300\n",
      "Epoch 13/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 71ms/step - loss: 0.1275 - val_loss: 0.1304\n",
      "Epoch 14/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 71ms/step - loss: 0.1324 - val_loss: 0.1321\n",
      "Epoch 15/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 71ms/step - loss: 0.1317 - val_loss: 0.1304\n",
      "Epoch 16/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 72ms/step - loss: 0.1283 - val_loss: 0.1328\n",
      "Epoch 17/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 71ms/step - loss: 0.1269 - val_loss: 0.1310\n",
      "Epoch 18/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 72ms/step - loss: 0.1282 - val_loss: 0.1334\n",
      "Epoch 19/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 71ms/step - loss: 0.1269 - val_loss: 0.1333\n",
      "Epoch 20/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 71ms/step - loss: 0.1266 - val_loss: 0.1359\n",
      "Epoch 21/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 74ms/step - loss: 0.1276 - val_loss: 0.1358\n",
      "Epoch 22/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 71ms/step - loss: 0.1270 - val_loss: 0.1388\n",
      "Epoch 23/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 71ms/step - loss: 0.1273 - val_loss: 0.1365\n",
      "Epoch 24/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 71ms/step - loss: 0.1245 - val_loss: 0.1376\n",
      "Epoch 25/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 71ms/step - loss: 0.1262 - val_loss: 0.1401\n",
      "Epoch 26/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 70ms/step - loss: 0.1248 - val_loss: 0.1400\n",
      "Epoch 27/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 70ms/step - loss: 0.1251 - val_loss: 0.1439\n",
      "Epoch 28/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 70ms/step - loss: 0.1264 - val_loss: 0.1483\n",
      "Epoch 29/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 70ms/step - loss: 0.1245 - val_loss: 0.1489\n",
      "Epoch 30/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 70ms/step - loss: 0.1247 - val_loss: 0.1453\n",
      "Epoch 31/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 70ms/step - loss: 0.1240 - val_loss: 0.1484\n",
      "Epoch 32/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 70ms/step - loss: 0.1240 - val_loss: 0.1529\n",
      "Epoch 33/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 73ms/step - loss: 0.1250 - val_loss: 0.1433\n",
      "Epoch 34/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 71ms/step - loss: 0.1243 - val_loss: 0.1466\n",
      "Epoch 35/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 70ms/step - loss: 0.1235 - val_loss: 0.1459\n",
      "Epoch 36/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 70ms/step - loss: 0.1213 - val_loss: 0.1487\n",
      "Epoch 37/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 71ms/step - loss: 0.1244 - val_loss: 0.1585\n",
      "Epoch 38/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 71ms/step - loss: 0.1235 - val_loss: 0.1529\n",
      "Epoch 39/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 71ms/step - loss: 0.1234 - val_loss: 0.1492\n",
      "Epoch 40/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 70ms/step - loss: 0.1219 - val_loss: 0.1474\n",
      "Epoch 41/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 70ms/step - loss: 0.1213 - val_loss: 0.1503\n",
      "Epoch 42/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 70ms/step - loss: 0.1219 - val_loss: 0.1538\n",
      "Epoch 43/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 70ms/step - loss: 0.1193 - val_loss: 0.1552\n",
      "Epoch 44/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 70ms/step - loss: 0.1211 - val_loss: 0.1566\n",
      "Epoch 45/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 74ms/step - loss: 0.1200 - val_loss: 0.1708\n",
      "Epoch 46/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 70ms/step - loss: 0.1202 - val_loss: 0.1555\n",
      "Epoch 47/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 70ms/step - loss: 0.1204 - val_loss: 0.1536\n",
      "Epoch 48/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 71ms/step - loss: 0.1191 - val_loss: 0.1537\n",
      "Epoch 49/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 71ms/step - loss: 0.1221 - val_loss: 0.1602\n",
      "Epoch 50/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 71ms/step - loss: 0.1182 - val_loss: 0.1622\n"
     ]
    }
   ],
   "source": [
    "\n",
    "history30 = model30.fit(X_train_rnn30, y_train_rnn30, epochs=50, batch_size=32, validation_data=(X_val_rnn30, y_val_rnn30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "\n",
    "model30.save('rnn_weather_model30.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - loss: 0.1368\n",
      "The loss in testing set: 0.16712136566638947\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 评evaluate\n",
    "loss30 = model30.evaluate(X_test_rnn30, y_test_rnn30)\n",
    "print('The loss in testing set:', loss30)\n",
    "\n",
    "plt.plot(history30.history['loss'], label='Training Loss')\n",
    "plt.plot(history30.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step\n",
      "RMSE of 'max_temp' in test set: 0.4138786075924413\n",
      "RMSE of 'min_temp' in test set: 0.4036671996773904\n",
      "MSE of 'max_temp' in test set: 0.17129550182265801\n",
      "MSE of 'min_temp' in test set: 0.16294720809538615\n",
      "MAE of 'max_temp' in test set: 0.31527826968793493\n",
      "MAE of 'min_temp' in test set: 0.31962341491062546\n",
      "R² of 'max_temp' in test set: 0.8270049642096162\n",
      "R² of 'min_temp' in test set: 0.827689826226409\n"
     ]
    }
   ],
   "source": [
    "# evaluate\n",
    "y_val_pred_rnn30 = model30.predict(X_val_rnn30)\n",
    "\n",
    "y_test_pred_rnn30 = model30.predict(X_test_rnn30)\n",
    "\n",
    "# RMSE\n",
    "rmse_max_rnn30 = np.sqrt(mean_squared_error(y_test_rnn30[:, 0], y_test_pred_rnn30[:, 0]))\n",
    "rmse_min_rnn30 = np.sqrt(mean_squared_error(y_test_rnn30[:, 1], y_test_pred_rnn30[:, 1]))\n",
    "\n",
    "# MSE\n",
    "mse_max_rnn30 = mean_squared_error(y_test_rnn30[:, 0], y_test_pred_rnn30[:, 0])\n",
    "mse_min_rnn30 = mean_squared_error(y_test_rnn30[:, 1], y_test_pred_rnn30[:, 1])\n",
    "\n",
    "# MAE\n",
    "mae_max_rnn30 = mean_absolute_error(y_test_rnn30[:, 0], y_test_pred_rnn30[:, 0])\n",
    "mae_min_rnn30 = mean_absolute_error(y_test_rnn30[:, 1], y_test_pred_rnn30[:, 1])\n",
    "\n",
    "# R²\n",
    "r2_max_rnn30 = r2_score(y_test_rnn30[:, 0], y_test_pred_rnn30[:, 0])\n",
    "r2_min_rnn30 = r2_score(y_test_rnn30[:, 1], y_test_pred_rnn30[:, 1])\n",
    "\n",
    "print(f\"RMSE of 'max_temp' in test set: {rmse_max_rnn30}\")\n",
    "print(f\"RMSE of 'min_temp' in test set: {rmse_min_rnn30}\")\n",
    "\n",
    "print(f\"MSE of 'max_temp' in test set: {mse_max_rnn30}\")\n",
    "print(f\"MSE of 'min_temp' in test set: {mse_min_rnn30}\")\n",
    "\n",
    "print(f\"MAE of 'max_temp' in test set: {mae_max_rnn30}\")\n",
    "print(f\"MAE of 'min_temp' in test set: {mae_min_rnn30}\")\n",
    "\n",
    "print(f\"R² of 'max_temp' in test set: {r2_max_rnn30}\")\n",
    "print(f\"R² of 'min_temp' in test set: {r2_min_rnn30}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/opt/anaconda3/lib/python3.9/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/opt/anaconda3/lib/python3.9/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/opt/anaconda3/lib/python3.9/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/opt/anaconda3/lib/python3.9/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/opt/anaconda3/lib/python3.9/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/opt/anaconda3/lib/python3.9/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/opt/anaconda3/lib/python3.9/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/opt/anaconda3/lib/python3.9/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "/opt/anaconda3/lib/python3.9/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "/opt/anaconda3/lib/python3.9/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "/opt/anaconda3/lib/python3.9/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "/opt/anaconda3/lib/python3.9/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "/opt/anaconda3/lib/python3.9/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "/opt/anaconda3/lib/python3.9/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "/opt/anaconda3/lib/python3.9/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [156]\u001b[0m, in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39mmean((y_pred30 \u001b[38;5;241m-\u001b[39m y) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# 使用 scikit-learn 的 permutation_importance\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m result_rnn30 \u001b[38;5;241m=\u001b[39m \u001b[43mpermutation_importance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_test_rnn30_reshaped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_test_rnn30\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_repeats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_scoring\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# 显示特征重要性\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(features)):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/sklearn/inspection/_permutation_importance.py:259\u001b[0m, in \u001b[0;36mpermutation_importance\u001b[0;34m(estimator, X, y, scoring, n_repeats, n_jobs, random_state, sample_weight, max_samples)\u001b[0m\n\u001b[1;32m    255\u001b[0m     scorer \u001b[38;5;241m=\u001b[39m _MultimetricScorer(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mscorers_dict)\n\u001b[1;32m    257\u001b[0m baseline_score \u001b[38;5;241m=\u001b[39m _weights_scorer(scorer, estimator, X, y, sample_weight)\n\u001b[0;32m--> 259\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_calculate_permutation_scores\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcol_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_repeats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcol_idx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(baseline_score, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    276\u001b[0m         name: _create_importances_bunch(\n\u001b[1;32m    277\u001b[0m             baseline_score[name],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m baseline_score\n\u001b[1;32m    282\u001b[0m     }\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py:1056\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1053\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1056\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;66;03m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[1;32m   1058\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py:935\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    933\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    934\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupports_timeout\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 935\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(\u001b[43mjob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    936\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    937\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget())\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py:542\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;124;03mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 542\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CfTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    544\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/concurrent/futures/_base.py:441\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 441\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Permutation Feature Importance\n",
    "\n",
    "# Flatten data into a two-dimensional array (number of samples, number of time steps * number of features)\n",
    "X_test_rnn30_reshaped = X_test_rnn30.reshape(X_test_rnn30.shape[0], -1)\n",
    "\n",
    "def custom_scoring(estimator, X, y):\n",
    "    X_reshaped30 = X.reshape(-1, time_steps30, len(features))\n",
    "    y_pred30 = estimator.predict(X_reshaped30)\n",
    "    return -np.mean((y_pred30 - y) ** 2)\n",
    "\n",
    "# scikit-learn permutation_importance\n",
    "result_rnn30 = permutation_importance(estimator=model, X=X_test_rnn30_reshaped, y=y_test_rnn30, n_repeats=10, random_state=42, scoring=custom_scoring, n_jobs=-1)\n",
    "\n",
    "for i in range(len(features)):\n",
    "    print(f\"Feature: {features[i]}, Importance: {result_rnn30.importances_mean[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.6586 - val_loss: 0.2615\n",
      "Epoch 2/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.2652 - val_loss: 0.2032\n",
      "Epoch 3/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.2012 - val_loss: 0.1826\n",
      "Epoch 4/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1757 - val_loss: 0.1529\n",
      "Epoch 5/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1598 - val_loss: 0.1425\n",
      "Epoch 6/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1554 - val_loss: 0.1524\n",
      "Epoch 7/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.1489 - val_loss: 0.1412\n",
      "Epoch 8/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1427 - val_loss: 0.1405\n",
      "Epoch 9/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1451 - val_loss: 0.1406\n",
      "Epoch 10/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1401 - val_loss: 0.1388\n",
      "Epoch 11/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1405 - val_loss: 0.1423\n",
      "Epoch 12/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1411 - val_loss: 0.1370\n",
      "Epoch 13/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1378 - val_loss: 0.1371\n",
      "Epoch 14/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1383 - val_loss: 0.1363\n",
      "Epoch 15/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1389 - val_loss: 0.1362\n",
      "Epoch 16/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1339 - val_loss: 0.1411\n",
      "Epoch 17/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1377 - val_loss: 0.1366\n",
      "Epoch 18/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.1326 - val_loss: 0.1344\n",
      "Epoch 19/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.1357 - val_loss: 0.1352\n",
      "Epoch 20/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1436 - val_loss: 0.1352\n",
      "Epoch 21/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1323 - val_loss: 0.1348\n",
      "Epoch 22/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1338 - val_loss: 0.1375\n",
      "Epoch 23/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1374 - val_loss: 0.1364\n",
      "Epoch 24/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.1328 - val_loss: 0.1393\n",
      "Epoch 25/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1397 - val_loss: 0.1357\n",
      "Epoch 26/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1384 - val_loss: 0.1342\n",
      "Epoch 27/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1371 - val_loss: 0.1384\n",
      "Epoch 28/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1353 - val_loss: 0.1343\n",
      "Epoch 29/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.1330 - val_loss: 0.1374\n",
      "Epoch 30/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1356 - val_loss: 0.1337\n",
      "Epoch 31/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1317 - val_loss: 0.1700\n",
      "Epoch 32/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1416 - val_loss: 0.1369\n",
      "Epoch 33/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1342 - val_loss: 0.1332\n",
      "Epoch 34/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.1290 - val_loss: 0.1418\n",
      "Epoch 35/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1364 - val_loss: 0.1331\n",
      "Epoch 36/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1299 - val_loss: 0.1386\n",
      "Epoch 37/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1359 - val_loss: 0.1331\n",
      "Epoch 38/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1338 - val_loss: 0.1381\n",
      "Epoch 39/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1365 - val_loss: 0.1321\n",
      "Epoch 40/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.1301 - val_loss: 0.1339\n",
      "Epoch 41/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1287 - val_loss: 0.1382\n",
      "Epoch 42/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1347 - val_loss: 0.1352\n",
      "Epoch 43/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1319 - val_loss: 0.1361\n",
      "Epoch 44/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1283 - val_loss: 0.1338\n",
      "Epoch 45/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.1251 - val_loss: 0.1358\n",
      "Epoch 46/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1299 - val_loss: 0.1347\n",
      "Epoch 47/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1319 - val_loss: 0.1349\n",
      "Epoch 48/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1296 - val_loss: 0.1343\n",
      "Epoch 49/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1256 - val_loss: 0.1347\n",
      "Epoch 50/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.1229 - val_loss: 0.1368\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "Fold 1, Validation Loss: 0.13677136600017548, RMSE Max: 0.37300900339918636, RMSE Min: 0.36661569483954803\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.5385 - val_loss: 0.2035\n",
      "Epoch 2/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1864 - val_loss: 0.1476\n",
      "Epoch 3/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1522 - val_loss: 0.1393\n",
      "Epoch 4/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1472 - val_loss: 0.1341\n",
      "Epoch 5/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1420 - val_loss: 0.1443\n",
      "Epoch 6/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1431 - val_loss: 0.1327\n",
      "Epoch 7/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1376 - val_loss: 0.1381\n",
      "Epoch 8/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1400 - val_loss: 0.1376\n",
      "Epoch 9/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1348 - val_loss: 0.1323\n",
      "Epoch 10/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1363 - val_loss: 0.1353\n",
      "Epoch 11/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1348 - val_loss: 0.1297\n",
      "Epoch 12/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1324 - val_loss: 0.1307\n",
      "Epoch 13/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1323 - val_loss: 0.1293\n",
      "Epoch 14/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1357 - val_loss: 0.1304\n",
      "Epoch 15/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1329 - val_loss: 0.1273\n",
      "Epoch 16/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1303 - val_loss: 0.1336\n",
      "Epoch 17/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1347 - val_loss: 0.1317\n",
      "Epoch 18/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1344 - val_loss: 0.1290\n",
      "Epoch 19/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1314 - val_loss: 0.1287\n",
      "Epoch 20/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1343 - val_loss: 0.1285\n",
      "Epoch 21/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1305 - val_loss: 0.1304\n",
      "Epoch 22/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1327 - val_loss: 0.1312\n",
      "Epoch 23/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1331 - val_loss: 0.1281\n",
      "Epoch 24/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1338 - val_loss: 0.1298\n",
      "Epoch 25/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1291 - val_loss: 0.1300\n",
      "Epoch 26/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1307 - val_loss: 0.1290\n",
      "Epoch 27/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1317 - val_loss: 0.1278\n",
      "Epoch 28/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1324 - val_loss: 0.1288\n",
      "Epoch 29/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1321 - val_loss: 0.1306\n",
      "Epoch 30/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1320 - val_loss: 0.1293\n",
      "Epoch 31/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1303 - val_loss: 0.1299\n",
      "Epoch 32/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1245 - val_loss: 0.1266\n",
      "Epoch 33/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1236 - val_loss: 0.1285\n",
      "Epoch 34/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1292 - val_loss: 0.1289\n",
      "Epoch 35/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1318 - val_loss: 0.1293\n",
      "Epoch 36/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1287 - val_loss: 0.1287\n",
      "Epoch 37/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1259 - val_loss: 0.1306\n",
      "Epoch 38/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1273 - val_loss: 0.1285\n",
      "Epoch 39/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1244 - val_loss: 0.1323\n",
      "Epoch 40/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1276 - val_loss: 0.1273\n",
      "Epoch 41/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1254 - val_loss: 0.1318\n",
      "Epoch 42/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1248 - val_loss: 0.1306\n",
      "Epoch 43/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1276 - val_loss: 0.1286\n",
      "Epoch 44/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1261 - val_loss: 0.1286\n",
      "Epoch 45/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1283 - val_loss: 0.1288\n",
      "Epoch 46/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1255 - val_loss: 0.1283\n",
      "Epoch 47/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1223 - val_loss: 0.1300\n",
      "Epoch 48/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1260 - val_loss: 0.1303\n",
      "Epoch 49/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1249 - val_loss: 0.1293\n",
      "Epoch 50/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1251 - val_loss: 0.1304\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "Fold 2, Validation Loss: 0.130404531955719, RMSE Max: 0.37262775067934417, RMSE Min: 0.34922431374503304\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.3914 - val_loss: 0.1918\n",
      "Epoch 2/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1710 - val_loss: 0.1521\n",
      "Epoch 3/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1476 - val_loss: 0.1362\n",
      "Epoch 4/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1396 - val_loss: 0.1336\n",
      "Epoch 5/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1369 - val_loss: 0.1328\n",
      "Epoch 6/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1400 - val_loss: 0.1335\n",
      "Epoch 7/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1380 - val_loss: 0.1328\n",
      "Epoch 8/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.1325 - val_loss: 0.1294\n",
      "Epoch 9/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1352 - val_loss: 0.1304\n",
      "Epoch 10/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1317 - val_loss: 0.1279\n",
      "Epoch 11/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1317 - val_loss: 0.1352\n",
      "Epoch 12/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1370 - val_loss: 0.1289\n",
      "Epoch 13/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1312 - val_loss: 0.1325\n",
      "Epoch 14/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1290 - val_loss: 0.1307\n",
      "Epoch 15/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1316 - val_loss: 0.1348\n",
      "Epoch 16/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1312 - val_loss: 0.1286\n",
      "Epoch 17/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1324 - val_loss: 0.1295\n",
      "Epoch 18/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1309 - val_loss: 0.1339\n",
      "Epoch 19/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1291 - val_loss: 0.1289\n",
      "Epoch 20/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1304 - val_loss: 0.1349\n",
      "Epoch 21/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1283 - val_loss: 0.1292\n",
      "Epoch 22/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1300 - val_loss: 0.1275\n",
      "Epoch 23/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1283 - val_loss: 0.1287\n",
      "Epoch 24/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1310 - val_loss: 0.1285\n",
      "Epoch 25/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1290 - val_loss: 0.1324\n",
      "Epoch 26/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1281 - val_loss: 0.1305\n",
      "Epoch 27/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1253 - val_loss: 0.1337\n",
      "Epoch 28/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1286 - val_loss: 0.1312\n",
      "Epoch 29/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.1246 - val_loss: 0.1320\n",
      "Epoch 30/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1278 - val_loss: 0.1341\n",
      "Epoch 31/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.1267 - val_loss: 0.1347\n",
      "Epoch 32/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1270 - val_loss: 0.1335\n",
      "Epoch 33/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1262 - val_loss: 0.1334\n",
      "Epoch 34/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1253 - val_loss: 0.1321\n",
      "Epoch 35/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1260 - val_loss: 0.1323\n",
      "Epoch 36/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1261 - val_loss: 0.1331\n",
      "Epoch 37/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1239 - val_loss: 0.1360\n",
      "Epoch 38/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1261 - val_loss: 0.1365\n",
      "Epoch 39/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1270 - val_loss: 0.1427\n",
      "Epoch 40/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1238 - val_loss: 0.1378\n",
      "Epoch 41/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1244 - val_loss: 0.1358\n",
      "Epoch 42/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1228 - val_loss: 0.1352\n",
      "Epoch 43/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1226 - val_loss: 0.1393\n",
      "Epoch 44/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1249 - val_loss: 0.1391\n",
      "Epoch 45/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1247 - val_loss: 0.1427\n",
      "Epoch 46/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1231 - val_loss: 0.1465\n",
      "Epoch 47/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1232 - val_loss: 0.1400\n",
      "Epoch 48/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1245 - val_loss: 0.1406\n",
      "Epoch 49/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1226 - val_loss: 0.1415\n",
      "Epoch 50/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.1214 - val_loss: 0.1427\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "Fold 3, Validation Loss: 0.1426781564950943, RMSE Max: 0.39498395845460593, RMSE Min: 0.35964431241674344\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.4335 - val_loss: 0.1477\n",
      "Epoch 2/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1512 - val_loss: 0.1327\n",
      "Epoch 3/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1410 - val_loss: 0.1330\n",
      "Epoch 4/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1371 - val_loss: 0.1284\n",
      "Epoch 5/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1347 - val_loss: 0.1335\n",
      "Epoch 6/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1373 - val_loss: 0.1284\n",
      "Epoch 7/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1340 - val_loss: 0.1276\n",
      "Epoch 8/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1317 - val_loss: 0.1277\n",
      "Epoch 9/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1320 - val_loss: 0.1280\n",
      "Epoch 10/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1322 - val_loss: 0.1271\n",
      "Epoch 11/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1312 - val_loss: 0.1305\n",
      "Epoch 12/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1322 - val_loss: 0.1274\n",
      "Epoch 13/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1312 - val_loss: 0.1295\n",
      "Epoch 14/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1301 - val_loss: 0.1272\n",
      "Epoch 15/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1315 - val_loss: 0.1268\n",
      "Epoch 16/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1281 - val_loss: 0.1271\n",
      "Epoch 17/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1288 - val_loss: 0.1282\n",
      "Epoch 18/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1269 - val_loss: 0.1257\n",
      "Epoch 19/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1304 - val_loss: 0.1270\n",
      "Epoch 20/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1281 - val_loss: 0.1278\n",
      "Epoch 21/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1277 - val_loss: 0.1264\n",
      "Epoch 22/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1281 - val_loss: 0.1264\n",
      "Epoch 23/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1283 - val_loss: 0.1270\n",
      "Epoch 24/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1284 - val_loss: 0.1259\n",
      "Epoch 25/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1275 - val_loss: 0.1262\n",
      "Epoch 26/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1267 - val_loss: 0.1270\n",
      "Epoch 27/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1266 - val_loss: 0.1262\n",
      "Epoch 28/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1269 - val_loss: 0.1257\n",
      "Epoch 29/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1281 - val_loss: 0.1277\n",
      "Epoch 30/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1275 - val_loss: 0.1258\n",
      "Epoch 31/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1244 - val_loss: 0.1283\n",
      "Epoch 32/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1260 - val_loss: 0.1264\n",
      "Epoch 33/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1254 - val_loss: 0.1255\n",
      "Epoch 34/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1252 - val_loss: 0.1268\n",
      "Epoch 35/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1234 - val_loss: 0.1254\n",
      "Epoch 36/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1239 - val_loss: 0.1260\n",
      "Epoch 37/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1233 - val_loss: 0.1276\n",
      "Epoch 38/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1245 - val_loss: 0.1281\n",
      "Epoch 39/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1238 - val_loss: 0.1296\n",
      "Epoch 40/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1235 - val_loss: 0.1262\n",
      "Epoch 41/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1226 - val_loss: 0.1269\n",
      "Epoch 42/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1204 - val_loss: 0.1266\n",
      "Epoch 43/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1219 - val_loss: 0.1267\n",
      "Epoch 44/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1205 - val_loss: 0.1271\n",
      "Epoch 45/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1227 - val_loss: 0.1277\n",
      "Epoch 46/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1227 - val_loss: 0.1270\n",
      "Epoch 47/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1231 - val_loss: 0.1274\n",
      "Epoch 48/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1217 - val_loss: 0.1297\n",
      "Epoch 49/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1202 - val_loss: 0.1281\n",
      "Epoch 50/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1213 - val_loss: 0.1293\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "Fold 4, Validation Loss: 0.12931817770004272, RMSE Max: 0.360322915636812, RMSE Min: 0.3588924382622561\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.2864 - val_loss: 0.1739\n",
      "Epoch 2/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1430 - val_loss: 0.1602\n",
      "Epoch 3/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1366 - val_loss: 0.1703\n",
      "Epoch 4/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1351 - val_loss: 0.1596\n",
      "Epoch 5/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1319 - val_loss: 0.1558\n",
      "Epoch 6/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1318 - val_loss: 0.1607\n",
      "Epoch 7/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1311 - val_loss: 0.1606\n",
      "Epoch 8/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1300 - val_loss: 0.1598\n",
      "Epoch 9/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1259 - val_loss: 0.1592\n",
      "Epoch 10/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1313 - val_loss: 0.1592\n",
      "Epoch 11/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1287 - val_loss: 0.1653\n",
      "Epoch 12/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1270 - val_loss: 0.1551\n",
      "Epoch 13/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1295 - val_loss: 0.1597\n",
      "Epoch 14/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1289 - val_loss: 0.1602\n",
      "Epoch 15/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1287 - val_loss: 0.1593\n",
      "Epoch 16/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1296 - val_loss: 0.1624\n",
      "Epoch 17/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1266 - val_loss: 0.1645\n",
      "Epoch 18/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1258 - val_loss: 0.1584\n",
      "Epoch 19/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1248 - val_loss: 0.1593\n",
      "Epoch 20/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1263 - val_loss: 0.1589\n",
      "Epoch 21/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1261 - val_loss: 0.1638\n",
      "Epoch 22/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1246 - val_loss: 0.1584\n",
      "Epoch 23/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1261 - val_loss: 0.1586\n",
      "Epoch 24/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1247 - val_loss: 0.1628\n",
      "Epoch 25/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1236 - val_loss: 0.1613\n",
      "Epoch 26/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1265 - val_loss: 0.1665\n",
      "Epoch 27/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1256 - val_loss: 0.1678\n",
      "Epoch 28/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1245 - val_loss: 0.1684\n",
      "Epoch 29/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1244 - val_loss: 0.1660\n",
      "Epoch 30/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1250 - val_loss: 0.1756\n",
      "Epoch 31/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1230 - val_loss: 2.5593\n",
      "Epoch 32/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1256 - val_loss: 5.3748\n",
      "Epoch 33/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1209 - val_loss: 0.1861\n",
      "Epoch 34/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1229 - val_loss: 1.1966\n",
      "Epoch 35/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1217 - val_loss: 1.5990\n",
      "Epoch 36/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1227 - val_loss: 0.5439\n",
      "Epoch 37/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1221 - val_loss: 0.1654\n",
      "Epoch 38/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1216 - val_loss: 0.1573\n",
      "Epoch 39/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1241 - val_loss: 0.1572\n",
      "Epoch 40/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1232 - val_loss: 0.1599\n",
      "Epoch 41/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1202 - val_loss: 0.1638\n",
      "Epoch 42/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1205 - val_loss: 0.1609\n",
      "Epoch 43/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1221 - val_loss: 0.1601\n",
      "Epoch 44/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1206 - val_loss: 0.1609\n",
      "Epoch 45/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1227 - val_loss: 0.1637\n",
      "Epoch 46/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1216 - val_loss: 0.1556\n",
      "Epoch 47/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1203 - val_loss: 0.1584\n",
      "Epoch 48/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1209 - val_loss: 0.1608\n",
      "Epoch 49/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1189 - val_loss: 0.1588\n",
      "Epoch 50/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1205 - val_loss: 0.1567\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "Fold 5, Validation Loss: 0.15674617886543274, RMSE Max: 0.3927397724283291, RMSE Min: 0.39905862917276697\n",
      "Average Validation Loss: 0.13918368220329286\n",
      "Average RMSE of max_temp: 0.3787366801196555\n",
      "Average RMSE of min_temp: 0.3666870776872695\n",
      "Average MSE of max_temp: 0.14361532340194808\n",
      "Average MSE of min_temp: 0.13475205844523225\n",
      "Average MAE of max_temp: 0.2939800486335444\n",
      "Average MAE of min_temp: 0.2963503516078341\n",
      "Average R² of max_temp: 0.8528262728998642\n",
      "Average R² of min_temp: 0.8603184555556525\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def create_time_series(X, y, time_steps):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        Xs.append(X[i:(i + time_steps)])\n",
    "        ys.append(y[i + time_steps])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "\n",
    "def create_lstm_model(input_shape, num_targets):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, activation='relu', input_shape=input_shape, return_sequences=False))\n",
    "    model.add(Dense(num_targets))  \n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# cross-val\n",
    "n_splits = 5\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "time_steps = 30\n",
    "num_targets = 2  \n",
    "\n",
    "\n",
    "\n",
    "results = {\n",
    "    'val_loss': [],\n",
    "    'rmse_max': [],\n",
    "    'rmse_min': [],\n",
    "    'mse_max': [],\n",
    "    'mse_min': [],\n",
    "    'mae_max': [],\n",
    "    'mae_min': [],\n",
    "    'r2_max': [],\n",
    "    'r2_min': []\n",
    "}\n",
    "\n",
    "# Converts feature and target variables to numpy arrays\n",
    "X = df_weather[['global_radiation', 'diff_temp', 'mean_temp']].values\n",
    "y = df_weather[['max_temp', 'min_temp']].values\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(tscv.split(X)):\n",
    "    X_train, X_val = X[train_index], X[val_index]\n",
    "    y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "\n",
    "    X_train_rnn, y_train_rnn = create_time_series(X_train, y_train, time_steps)\n",
    "    X_val_rnn, y_val_rnn = create_time_series(X_val, y_val, time_steps)\n",
    "\n",
    "    model = create_lstm_model((time_steps, X_train_rnn.shape[2]), num_targets)\n",
    "    history = model.fit(X_train_rnn, y_train_rnn, epochs=50, batch_size=32, validation_data=(X_val_rnn, y_val_rnn), verbose=1)\n",
    "\n",
    "    val_loss = model.evaluate(X_val_rnn, y_val_rnn, verbose=0)\n",
    "    y_val_pred = model.predict(X_val_rnn)\n",
    "\n",
    "    assert y_val_rnn.shape[0] == y_val_pred.shape[0], \"预测值和实际值的长度不一致！\"\n",
    "\n",
    "    mse_max = mean_squared_error(y_val_rnn[:, 0], y_val_pred[:, 0])\n",
    "    mse_min = mean_squared_error(y_val_rnn[:, 1], y_val_pred[:, 1])\n",
    "    rmse_max = np.sqrt(mse_max)\n",
    "    rmse_min = np.sqrt(mse_min)\n",
    "    mae_max = mean_absolute_error(y_val_rnn[:, 0], y_val_pred[:, 0])\n",
    "    mae_min = mean_absolute_error(y_val_rnn[:, 1], y_val_pred[:, 1])\n",
    "    r2_max = r2_score(y_val_rnn[:, 0], y_val_pred[:, 0])\n",
    "    r2_min = r2_score(y_val_rnn[:, 1], y_val_pred[:, 1])\n",
    "\n",
    "    results['val_loss'].append(val_loss)\n",
    "    results['rmse_max'].append(rmse_max)\n",
    "    results['rmse_min'].append(rmse_min)\n",
    "    results['mse_max'].append(mse_max)\n",
    "    results['mse_min'].append(mse_min)\n",
    "    results['mae_max'].append(mae_max)\n",
    "    results['mae_min'].append(mae_min)\n",
    "    results['r2_max'].append(r2_max)\n",
    "    results['r2_min'].append(r2_min)\n",
    "    \n",
    "    print(f'Fold {fold + 1}, Validation Loss: {val_loss}, RMSE Max: {rmse_max}, RMSE Min: {rmse_min}')\n",
    "\n",
    "print(f'Average Validation Loss: {np.mean(results[\"val_loss\"])}')\n",
    "print(f'Average RMSE of max_temp: {np.mean(results[\"rmse_max\"])}')\n",
    "print(f'Average RMSE of min_temp: {np.mean(results[\"rmse_min\"])}')\n",
    "print(f'Average MSE of max_temp: {np.mean(results[\"mse_max\"])}')\n",
    "print(f'Average MSE of min_temp: {np.mean(results[\"mse_min\"])}')\n",
    "print(f'Average MAE of max_temp: {np.mean(results[\"mae_max\"])}')\n",
    "print(f'Average MAE of min_temp: {np.mean(results[\"mae_min\"])}')\n",
    "print(f'Average R² of max_temp: {np.mean(results[\"r2_max\"])}')\n",
    "print(f'Average R² of min_temp: {np.mean(results[\"r2_min\"])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape3 = (time_steps30, len(features))\n",
    "model3 = build_model(\n",
    "    input_shape3,\n",
    "    head_size=256,\n",
    "    num_heads=4,\n",
    "    ff_dim=4,\n",
    "    num_transformer_blocks=4,\n",
    "    mlp_units=[128],\n",
    "    mlp_dropout=0.4,\n",
    "    dropout=0.25,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_76\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_76\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_46      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span> │ input_layer_46[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">15,363</span> │ layer_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_209         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_atten… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_128 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_209[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                     │                   │            │ input_layer_46[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span> │ add_128[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_190 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span> │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_210         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_190[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_191 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span> │ dropout_210[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_129 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_191[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│                     │                   │            │ add_128[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span> │ add_129[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">15,363</span> │ layer_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_212         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_atten… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_130 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_212[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                     │                   │            │ add_129[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span> │ add_130[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_192 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span> │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_213         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_192[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_193 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span> │ dropout_213[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_131 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_193[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│                     │                   │            │ add_130[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span> │ add_131[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">15,363</span> │ layer_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_215         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_atten… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_132 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_215[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                     │                   │            │ add_131[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span> │ add_132[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_194 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span> │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_216         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_194[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_195 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span> │ dropout_216[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_133 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_195[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│                     │                   │            │ add_132[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span> │ add_133[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">15,363</span> │ layer_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_218         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_atten… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_134 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_218[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                     │                   │            │ add_133[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span> │ add_134[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_196 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span> │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_219         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_196[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_197 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span> │ dropout_219[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_135 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_197[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│                     │                   │            │ add_134[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_135[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_198 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">3,968</span> │ global_average_p… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_220         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_198[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_199 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">258</span> │ dropout_220[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_46      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │          \u001b[38;5;34m6\u001b[0m │ input_layer_46[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │     \u001b[38;5;34m15,363\u001b[0m │ layer_normalizat… │\n",
       "│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_209         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ multi_head_atten… │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_128 (\u001b[38;5;33mAdd\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ dropout_209[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
       "│                     │                   │            │ input_layer_46[\u001b[38;5;34m0\u001b[0m… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │          \u001b[38;5;34m6\u001b[0m │ add_128[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_190 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m4\u001b[0m)     │         \u001b[38;5;34m16\u001b[0m │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_210         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m4\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ dense_190[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_191 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │         \u001b[38;5;34m15\u001b[0m │ dropout_210[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_129 (\u001b[38;5;33mAdd\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ dense_191[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│                     │                   │            │ add_128[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │          \u001b[38;5;34m6\u001b[0m │ add_129[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │     \u001b[38;5;34m15,363\u001b[0m │ layer_normalizat… │\n",
       "│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_212         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ multi_head_atten… │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_130 (\u001b[38;5;33mAdd\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ dropout_212[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
       "│                     │                   │            │ add_129[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │          \u001b[38;5;34m6\u001b[0m │ add_130[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_192 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m4\u001b[0m)     │         \u001b[38;5;34m16\u001b[0m │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_213         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m4\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ dense_192[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_193 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │         \u001b[38;5;34m15\u001b[0m │ dropout_213[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_131 (\u001b[38;5;33mAdd\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ dense_193[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│                     │                   │            │ add_130[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │          \u001b[38;5;34m6\u001b[0m │ add_131[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │     \u001b[38;5;34m15,363\u001b[0m │ layer_normalizat… │\n",
       "│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_215         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ multi_head_atten… │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_132 (\u001b[38;5;33mAdd\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ dropout_215[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
       "│                     │                   │            │ add_131[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │          \u001b[38;5;34m6\u001b[0m │ add_132[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_194 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m4\u001b[0m)     │         \u001b[38;5;34m16\u001b[0m │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_216         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m4\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ dense_194[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_195 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │         \u001b[38;5;34m15\u001b[0m │ dropout_216[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_133 (\u001b[38;5;33mAdd\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ dense_195[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│                     │                   │            │ add_132[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │          \u001b[38;5;34m6\u001b[0m │ add_133[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │     \u001b[38;5;34m15,363\u001b[0m │ layer_normalizat… │\n",
       "│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_218         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ multi_head_atten… │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_134 (\u001b[38;5;33mAdd\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ dropout_218[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
       "│                     │                   │            │ add_133[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │          \u001b[38;5;34m6\u001b[0m │ add_134[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_196 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m4\u001b[0m)     │         \u001b[38;5;34m16\u001b[0m │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_219         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m4\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ dense_196[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_197 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │         \u001b[38;5;34m15\u001b[0m │ dropout_219[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_135 (\u001b[38;5;33mAdd\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ dense_197[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│                     │                   │            │ add_134[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ add_135[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_198 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │      \u001b[38;5;34m3,968\u001b[0m │ global_average_p… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_220         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_198[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_199 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)         │        \u001b[38;5;34m258\u001b[0m │ dropout_220[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">65,850</span> (257.23 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m65,850\u001b[0m (257.23 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">65,850</span> (257.23 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m65,850\u001b[0m (257.23 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model3.compile(\n",
    "    loss=\"mean_squared_error\",\n",
    "    optimizer=Adam(learning_rate=1e-4),\n",
    ")\n",
    "\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 100ms/step - loss: 1.5408 - val_loss: 0.2963\n",
      "Epoch 2/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 97ms/step - loss: 0.5929 - val_loss: 0.2717\n",
      "Epoch 3/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 97ms/step - loss: 0.4962 - val_loss: 0.2600\n",
      "Epoch 4/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 98ms/step - loss: 0.4403 - val_loss: 0.2560\n",
      "Epoch 5/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 99ms/step - loss: 0.3989 - val_loss: 0.2380\n",
      "Epoch 6/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 98ms/step - loss: 0.3659 - val_loss: 0.2366\n",
      "Epoch 7/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 98ms/step - loss: 0.3460 - val_loss: 0.2221\n",
      "Epoch 8/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 98ms/step - loss: 0.3326 - val_loss: 0.2169\n",
      "Epoch 9/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 98ms/step - loss: 0.3070 - val_loss: 0.2193\n",
      "Epoch 10/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 99ms/step - loss: 0.2980 - val_loss: 0.2109\n",
      "Epoch 11/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 100ms/step - loss: 0.2850 - val_loss: 0.2048\n",
      "Epoch 12/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 101ms/step - loss: 0.2750 - val_loss: 0.2057\n",
      "Epoch 13/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 100ms/step - loss: 0.2708 - val_loss: 0.1972\n",
      "Epoch 14/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 101ms/step - loss: 0.2691 - val_loss: 0.1967\n",
      "Epoch 15/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 99ms/step - loss: 0.2599 - val_loss: 0.1916\n",
      "Epoch 16/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 101ms/step - loss: 0.2553 - val_loss: 0.1900\n",
      "Epoch 17/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 101ms/step - loss: 0.2483 - val_loss: 0.1865\n",
      "Epoch 18/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 102ms/step - loss: 0.2395 - val_loss: 0.1874\n",
      "Epoch 19/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 100ms/step - loss: 0.2365 - val_loss: 0.1848\n",
      "Epoch 20/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 101ms/step - loss: 0.2323 - val_loss: 0.1817\n",
      "Epoch 21/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 100ms/step - loss: 0.2237 - val_loss: 0.1800\n",
      "Epoch 22/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 101ms/step - loss: 0.2216 - val_loss: 0.1818\n",
      "Epoch 23/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 100ms/step - loss: 0.2228 - val_loss: 0.1818\n",
      "Epoch 24/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 101ms/step - loss: 0.2151 - val_loss: 0.1779\n",
      "Epoch 25/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 100ms/step - loss: 0.2131 - val_loss: 0.1758\n",
      "Epoch 26/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 100ms/step - loss: 0.2103 - val_loss: 0.1781\n",
      "Epoch 27/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 101ms/step - loss: 0.2142 - val_loss: 0.1742\n",
      "Epoch 28/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 101ms/step - loss: 0.2130 - val_loss: 0.1744\n",
      "Epoch 29/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 105ms/step - loss: 0.2038 - val_loss: 0.1735\n",
      "Epoch 30/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 102ms/step - loss: 0.2073 - val_loss: 0.1703\n",
      "Epoch 31/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 99ms/step - loss: 0.2081 - val_loss: 0.1735\n",
      "Epoch 32/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 101ms/step - loss: 0.2023 - val_loss: 0.1727\n",
      "Epoch 33/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 101ms/step - loss: 0.2079 - val_loss: 0.1711\n",
      "Epoch 34/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 100ms/step - loss: 0.1980 - val_loss: 0.1692\n",
      "Epoch 35/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 102ms/step - loss: 0.1989 - val_loss: 0.1680\n",
      "Epoch 36/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 100ms/step - loss: 0.2001 - val_loss: 0.1709\n",
      "Epoch 37/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 102ms/step - loss: 0.1970 - val_loss: 0.1666\n",
      "Epoch 38/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 101ms/step - loss: 0.1971 - val_loss: 0.1663\n",
      "Epoch 39/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 102ms/step - loss: 0.1980 - val_loss: 0.1677\n",
      "Epoch 40/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 101ms/step - loss: 0.1983 - val_loss: 0.1661\n",
      "Epoch 41/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 102ms/step - loss: 0.1961 - val_loss: 0.1716\n",
      "Epoch 42/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 101ms/step - loss: 0.1956 - val_loss: 0.1654\n",
      "Epoch 43/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 102ms/step - loss: 0.1913 - val_loss: 0.1640\n",
      "Epoch 44/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 101ms/step - loss: 0.1907 - val_loss: 0.1628\n",
      "Epoch 45/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 102ms/step - loss: 0.1967 - val_loss: 0.1647\n",
      "Epoch 46/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 103ms/step - loss: 0.1879 - val_loss: 0.1661\n",
      "Epoch 47/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 101ms/step - loss: 0.1943 - val_loss: 0.1616\n",
      "Epoch 48/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 102ms/step - loss: 0.1913 - val_loss: 0.1661\n",
      "Epoch 49/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 103ms/step - loss: 0.1969 - val_loss: 0.1635\n",
      "Epoch 50/50\n",
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 104ms/step - loss: 0.1912 - val_loss: 0.1653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "history3 = model3.fit(\n",
    "    X_train_rnn30, y_train_rnn30,\n",
    "    validation_data=(X_val_rnn30, y_val_rnn30),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    ")\n",
    "\n",
    "model3.save('transformer_weather_model30.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 36ms/step\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step\n",
      "RMSE of 'max_temp' in test set: 0.4138786075924413\n",
      "RMSE of 'min_temp' in test set: 0.4036671996773904\n",
      "MSE of 'max_temp' in test set: 0.17129550182265801\n",
      "MSE of 'min_temp' in test set: 0.16294720809538615\n",
      "MAE of 'max_temp' in test set: 0.31527826968793493\n",
      "MAE of 'min_temp' in test set: 0.31962341491062546\n",
      "R² of 'max_temp' in test set: 0.8270049642096162\n",
      "R² of 'min_temp' in test set: 0.827689826226409\n"
     ]
    }
   ],
   "source": [
    "# evaluate\n",
    "y_val_pred_t30 = model3.predict(X_val_rnn30)\n",
    "y_test_pred_t30 = model3.predict(X_test_rnn30)\n",
    "\n",
    "# RMSE\n",
    "rmse_max_t30 = np.sqrt(mean_squared_error(y_test_rnn30[:, 0], y_test_pred_rnn30[:, 0]))\n",
    "rmse_min_t30 = np.sqrt(mean_squared_error(y_test_rnn30[:, 1], y_test_pred_rnn30[:, 1]))\n",
    "\n",
    "# MSE\n",
    "mse_max_t30 = mean_squared_error(y_test_rnn30[:, 0], y_test_pred_rnn30[:, 0])\n",
    "mse_min_t30 = mean_squared_error(y_test_rnn30[:, 1], y_test_pred_rnn30[:, 1])\n",
    "\n",
    "# MAE\n",
    "mae_max_t30 = mean_absolute_error(y_test_rnn30[:, 0], y_test_pred_rnn30[:, 0])\n",
    "mae_min_t30 = mean_absolute_error(y_test_rnn30[:, 1], y_test_pred_rnn30[:, 1])\n",
    "\n",
    "# R²\n",
    "r2_max_t30 = r2_score(y_test_rnn30[:, 0], y_test_pred_rnn30[:, 0])\n",
    "r2_min_t30 = r2_score(y_test_rnn30[:, 1], y_test_pred_rnn30[:, 1])\n",
    "\n",
    "print(f\"RMSE of 'max_temp' in test set: {rmse_max_t30}\")\n",
    "print(f\"RMSE of 'min_temp' in test set: {rmse_min_t30}\")\n",
    "\n",
    "print(f\"MSE of 'max_temp' in test set: {mse_max_t30}\")\n",
    "print(f\"MSE of 'min_temp' in test set: {mse_min_t30}\")\n",
    "\n",
    "print(f\"MAE of 'max_temp' in test set: {mae_max_t30}\")\n",
    "print(f\"MAE of 'min_temp' in test set: {mae_min_t30}\")\n",
    "\n",
    "print(f\"R² of 'max_temp' in test set: {r2_max_t30}\")\n",
    "print(f\"R² of 'min_temp' in test set: {r2_min_t30}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 101ms/step - loss: 1.0465 - val_loss: 0.9442\n",
      "Epoch 2/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 107ms/step - loss: 1.0176 - val_loss: 0.8246\n",
      "Epoch 3/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 117ms/step - loss: 0.8844 - val_loss: 0.6687\n",
      "Epoch 4/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 122ms/step - loss: 0.6610 - val_loss: 0.5132\n",
      "Epoch 5/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 139ms/step - loss: 0.5137 - val_loss: 0.4261\n",
      "Epoch 6/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 122ms/step - loss: 0.4473 - val_loss: 0.3839\n",
      "Epoch 7/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 125ms/step - loss: 0.4212 - val_loss: 0.3721\n",
      "Epoch 8/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 125ms/step - loss: 0.3895 - val_loss: 0.3629\n",
      "Epoch 9/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 121ms/step - loss: 0.3950 - val_loss: 0.3536\n",
      "Epoch 10/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 125ms/step - loss: 0.3827 - val_loss: 0.3490\n",
      "Epoch 11/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 125ms/step - loss: 0.3707 - val_loss: 0.3480\n",
      "Epoch 12/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 125ms/step - loss: 0.3604 - val_loss: 0.3404\n",
      "Epoch 13/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 125ms/step - loss: 0.3583 - val_loss: 0.3338\n",
      "Epoch 14/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 125ms/step - loss: 0.3660 - val_loss: 0.3485\n",
      "Epoch 15/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 124ms/step - loss: 0.3664 - val_loss: 0.3247\n",
      "Epoch 16/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 122ms/step - loss: 0.3706 - val_loss: 0.3393\n",
      "Epoch 17/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 128ms/step - loss: 0.3466 - val_loss: 0.3167\n",
      "Epoch 18/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 126ms/step - loss: 0.3523 - val_loss: 0.3133\n",
      "Epoch 19/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 121ms/step - loss: 0.3443 - val_loss: 0.3162\n",
      "Epoch 20/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 127ms/step - loss: 0.3467 - val_loss: 0.3088\n",
      "Epoch 21/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 126ms/step - loss: 0.3436 - val_loss: 0.3036\n",
      "Epoch 22/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 121ms/step - loss: 0.3543 - val_loss: 0.3265\n",
      "Epoch 23/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 126ms/step - loss: 0.3408 - val_loss: 0.3151\n",
      "Epoch 24/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 120ms/step - loss: 0.3312 - val_loss: 0.3193\n",
      "Epoch 25/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 123ms/step - loss: 0.3223 - val_loss: 0.3023\n",
      "Epoch 26/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 125ms/step - loss: 0.3182 - val_loss: 0.3048\n",
      "Epoch 27/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 120ms/step - loss: 0.3228 - val_loss: 0.3119\n",
      "Epoch 28/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 126ms/step - loss: 0.3225 - val_loss: 0.3000\n",
      "Epoch 29/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 121ms/step - loss: 0.3139 - val_loss: 0.3076\n",
      "Epoch 30/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 123ms/step - loss: 0.3164 - val_loss: 0.3028\n",
      "Epoch 31/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 125ms/step - loss: 0.3185 - val_loss: 0.2891\n",
      "Epoch 32/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 121ms/step - loss: 0.3203 - val_loss: 0.3174\n",
      "Epoch 33/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 125ms/step - loss: 0.3149 - val_loss: 0.2879\n",
      "Epoch 34/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 126ms/step - loss: 0.3159 - val_loss: 0.3065\n",
      "Epoch 35/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 120ms/step - loss: 0.3090 - val_loss: 0.2862\n",
      "Epoch 36/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 124ms/step - loss: 0.3068 - val_loss: 0.2989\n",
      "Epoch 37/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 121ms/step - loss: 0.3113 - val_loss: 0.2888\n",
      "Epoch 38/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 123ms/step - loss: 0.3040 - val_loss: 0.2845\n",
      "Epoch 39/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 123ms/step - loss: 0.2891 - val_loss: 0.2911\n",
      "Epoch 40/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 120ms/step - loss: 0.2993 - val_loss: 0.2906\n",
      "Epoch 41/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 123ms/step - loss: 0.2916 - val_loss: 0.2841\n",
      "Epoch 42/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 122ms/step - loss: 0.2933 - val_loss: 0.2835\n",
      "Epoch 43/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 123ms/step - loss: 0.2903 - val_loss: 0.2900\n",
      "Epoch 44/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 125ms/step - loss: 0.2905 - val_loss: 0.2818\n",
      "Epoch 45/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 120ms/step - loss: 0.2944 - val_loss: 0.2774\n",
      "Epoch 46/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 124ms/step - loss: 0.2886 - val_loss: 0.2851\n",
      "Epoch 47/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 124ms/step - loss: 0.2918 - val_loss: 0.2891\n",
      "Epoch 48/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 120ms/step - loss: 0.2870 - val_loss: 0.2913\n",
      "Epoch 49/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 125ms/step - loss: 0.2959 - val_loss: 0.2841\n",
      "Epoch 50/50\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 126ms/step - loss: 0.2821 - val_loss: 0.3264\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step\n",
      "Fold 1, Validation Loss: 0.3264096975326538, RMSE Max: 0.5451004089773079, RMSE Min: 0.5963932604140211\n",
      "Epoch 1/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 99ms/step - loss: 1.0122 - val_loss: 0.8329\n",
      "Epoch 2/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 109ms/step - loss: 0.7993 - val_loss: 0.5217\n",
      "Epoch 3/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 106ms/step - loss: 0.5135 - val_loss: 0.4010\n",
      "Epoch 4/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 107ms/step - loss: 0.4023 - val_loss: 0.3624\n",
      "Epoch 5/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 110ms/step - loss: 0.3739 - val_loss: 0.3361\n",
      "Epoch 6/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 107ms/step - loss: 0.3635 - val_loss: 0.3354\n",
      "Epoch 7/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 109ms/step - loss: 0.3506 - val_loss: 0.3451\n",
      "Epoch 8/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 111ms/step - loss: 0.3591 - val_loss: 0.3300\n",
      "Epoch 9/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 106ms/step - loss: 0.3534 - val_loss: 0.3268\n",
      "Epoch 10/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 105ms/step - loss: 0.3442 - val_loss: 0.3181\n",
      "Epoch 11/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 107ms/step - loss: 0.3496 - val_loss: 0.3323\n",
      "Epoch 12/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 108ms/step - loss: 0.3482 - val_loss: 0.3250\n",
      "Epoch 13/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 108ms/step - loss: 0.3442 - val_loss: 0.3209\n",
      "Epoch 14/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 109ms/step - loss: 0.3460 - val_loss: 0.3196\n",
      "Epoch 15/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 107ms/step - loss: 0.3479 - val_loss: 0.3338\n",
      "Epoch 16/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 106ms/step - loss: 0.3549 - val_loss: 0.3231\n",
      "Epoch 17/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 107ms/step - loss: 0.3517 - val_loss: 0.3234\n",
      "Epoch 18/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 109ms/step - loss: 0.3403 - val_loss: 0.3271\n",
      "Epoch 19/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 109ms/step - loss: 0.3413 - val_loss: 0.3248\n",
      "Epoch 20/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 106ms/step - loss: 0.3368 - val_loss: 0.3326\n",
      "Epoch 21/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 108ms/step - loss: 0.3371 - val_loss: 0.3221\n",
      "Epoch 22/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 110ms/step - loss: 0.3262 - val_loss: 0.3228\n",
      "Epoch 23/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 106ms/step - loss: 0.3277 - val_loss: 0.3247\n",
      "Epoch 24/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 109ms/step - loss: 0.3414 - val_loss: 0.3133\n",
      "Epoch 25/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 107ms/step - loss: 0.3407 - val_loss: 0.3173\n",
      "Epoch 26/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 108ms/step - loss: 0.3390 - val_loss: 0.3193\n",
      "Epoch 27/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 108ms/step - loss: 0.3296 - val_loss: 0.3162\n",
      "Epoch 28/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 108ms/step - loss: 0.3246 - val_loss: 0.3241\n",
      "Epoch 29/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 108ms/step - loss: 0.3338 - val_loss: 0.3213\n",
      "Epoch 30/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 106ms/step - loss: 0.3274 - val_loss: 0.3199\n",
      "Epoch 31/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 107ms/step - loss: 0.3246 - val_loss: 0.3188\n",
      "Epoch 32/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 108ms/step - loss: 0.3393 - val_loss: 0.3122\n",
      "Epoch 33/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 109ms/step - loss: 0.3226 - val_loss: 0.3113\n",
      "Epoch 34/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 109ms/step - loss: 0.3223 - val_loss: 0.3148\n",
      "Epoch 35/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 109ms/step - loss: 0.3269 - val_loss: 0.3122\n",
      "Epoch 36/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 109ms/step - loss: 0.3211 - val_loss: 0.3109\n",
      "Epoch 37/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 110ms/step - loss: 0.3286 - val_loss: 0.3062\n",
      "Epoch 38/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 108ms/step - loss: 0.3235 - val_loss: 0.3103\n",
      "Epoch 39/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 106ms/step - loss: 0.3211 - val_loss: 0.3065\n",
      "Epoch 40/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 108ms/step - loss: 0.3289 - val_loss: 0.3039\n",
      "Epoch 41/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 107ms/step - loss: 0.3165 - val_loss: 0.3083\n",
      "Epoch 42/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 107ms/step - loss: 0.3334 - val_loss: 0.3145\n",
      "Epoch 43/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 107ms/step - loss: 0.3296 - val_loss: 0.3157\n",
      "Epoch 44/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 106ms/step - loss: 0.3223 - val_loss: 0.3120\n",
      "Epoch 45/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 118ms/step - loss: 0.3255 - val_loss: 0.3027\n",
      "Epoch 46/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 109ms/step - loss: 0.3205 - val_loss: 0.3037\n",
      "Epoch 47/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 109ms/step - loss: 0.3339 - val_loss: 0.3124\n",
      "Epoch 48/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 110ms/step - loss: 0.3171 - val_loss: 0.3293\n",
      "Epoch 49/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 112ms/step - loss: 0.3258 - val_loss: 0.3153\n",
      "Epoch 50/50\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 108ms/step - loss: 0.3119 - val_loss: 0.3225\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step\n",
      "Fold 2, Validation Loss: 0.322460800409317, RMSE Max: 0.549298367780158, RMSE Min: 0.5858267149980225\n",
      "Epoch 1/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 97ms/step - loss: 0.9526 - val_loss: 0.6607\n",
      "Epoch 2/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 105ms/step - loss: 0.5822 - val_loss: 0.3800\n",
      "Epoch 3/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 106ms/step - loss: 0.3878 - val_loss: 0.3255\n",
      "Epoch 4/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 106ms/step - loss: 0.3618 - val_loss: 0.3176\n",
      "Epoch 5/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 102ms/step - loss: 0.3555 - val_loss: 0.3058\n",
      "Epoch 6/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 104ms/step - loss: 0.3438 - val_loss: 0.3091\n",
      "Epoch 7/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 104ms/step - loss: 0.3427 - val_loss: 0.3068\n",
      "Epoch 8/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 106ms/step - loss: 0.3356 - val_loss: 0.3077\n",
      "Epoch 9/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 103ms/step - loss: 0.3336 - val_loss: 0.3143\n",
      "Epoch 10/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 105ms/step - loss: 0.3321 - val_loss: 0.3299\n",
      "Epoch 11/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 117ms/step - loss: 0.3336 - val_loss: 0.3251\n",
      "Epoch 12/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 95ms/step - loss: 0.3341 - val_loss: 0.3190\n",
      "Epoch 13/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 95ms/step - loss: 0.3245 - val_loss: 0.3041\n",
      "Epoch 14/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 118ms/step - loss: 0.3252 - val_loss: 0.3161\n",
      "Epoch 15/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 105ms/step - loss: 0.3264 - val_loss: 0.3207\n",
      "Epoch 16/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 95ms/step - loss: 0.3178 - val_loss: 0.3080\n",
      "Epoch 17/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 100ms/step - loss: 0.3296 - val_loss: 0.3072\n",
      "Epoch 18/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 98ms/step - loss: 0.3318 - val_loss: 0.3127\n",
      "Epoch 19/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 98ms/step - loss: 0.3303 - val_loss: 0.3044\n",
      "Epoch 20/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 106ms/step - loss: 0.3272 - val_loss: 0.3041\n",
      "Epoch 21/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 123ms/step - loss: 0.3273 - val_loss: 0.3111\n",
      "Epoch 22/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 97ms/step - loss: 0.3264 - val_loss: 0.2963\n",
      "Epoch 23/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 120ms/step - loss: 0.3272 - val_loss: 0.2964\n",
      "Epoch 24/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 103ms/step - loss: 0.3165 - val_loss: 0.2989\n",
      "Epoch 25/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 102ms/step - loss: 0.3210 - val_loss: 0.2986\n",
      "Epoch 26/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 104ms/step - loss: 0.3230 - val_loss: 0.2976\n",
      "Epoch 27/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 107ms/step - loss: 0.3279 - val_loss: 0.3056\n",
      "Epoch 28/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 104ms/step - loss: 0.3206 - val_loss: 0.2941\n",
      "Epoch 29/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 106ms/step - loss: 0.3162 - val_loss: 0.2913\n",
      "Epoch 30/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 102ms/step - loss: 0.3131 - val_loss: 0.2992\n",
      "Epoch 31/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 104ms/step - loss: 0.3233 - val_loss: 0.2989\n",
      "Epoch 32/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 104ms/step - loss: 0.3189 - val_loss: 0.2974\n",
      "Epoch 33/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 106ms/step - loss: 0.3214 - val_loss: 0.3030\n",
      "Epoch 34/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 102ms/step - loss: 0.3112 - val_loss: 0.2867\n",
      "Epoch 35/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 105ms/step - loss: 0.3179 - val_loss: 0.2845\n",
      "Epoch 36/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 106ms/step - loss: 0.3174 - val_loss: 0.2986\n",
      "Epoch 37/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 102ms/step - loss: 0.3138 - val_loss: 0.2922\n",
      "Epoch 38/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 106ms/step - loss: 0.3041 - val_loss: 0.2906\n",
      "Epoch 39/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 103ms/step - loss: 0.3118 - val_loss: 0.3117\n",
      "Epoch 40/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 104ms/step - loss: 0.3116 - val_loss: 0.2945\n",
      "Epoch 41/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 104ms/step - loss: 0.3184 - val_loss: 0.2968\n",
      "Epoch 42/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 104ms/step - loss: 0.3178 - val_loss: 0.2931\n",
      "Epoch 43/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 103ms/step - loss: 0.3132 - val_loss: 0.2915\n",
      "Epoch 44/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 103ms/step - loss: 0.3096 - val_loss: 0.2871\n",
      "Epoch 45/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 103ms/step - loss: 0.3143 - val_loss: 0.2951\n",
      "Epoch 46/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 106ms/step - loss: 0.3123 - val_loss: 0.2962\n",
      "Epoch 47/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 107ms/step - loss: 0.3143 - val_loss: 0.3082\n",
      "Epoch 48/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 103ms/step - loss: 0.3143 - val_loss: 0.2802\n",
      "Epoch 49/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 103ms/step - loss: 0.3051 - val_loss: 0.2758\n",
      "Epoch 50/50\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 104ms/step - loss: 0.3042 - val_loss: 0.2861\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 44ms/step\n",
      "Fold 3, Validation Loss: 0.28607848286628723, RMSE Max: 0.5048025323021211, RMSE Min: 0.5633217106913658\n",
      "Epoch 1/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 95ms/step - loss: 0.9530 - val_loss: 0.5269\n",
      "Epoch 2/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 99ms/step - loss: 0.4870 - val_loss: 0.3554\n",
      "Epoch 3/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 100ms/step - loss: 0.3742 - val_loss: 0.3547\n",
      "Epoch 4/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 104ms/step - loss: 0.3634 - val_loss: 0.3271\n",
      "Epoch 5/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 101ms/step - loss: 0.3518 - val_loss: 0.3334\n",
      "Epoch 6/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 102ms/step - loss: 0.3402 - val_loss: 0.3200\n",
      "Epoch 7/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 102ms/step - loss: 0.3415 - val_loss: 0.3244\n",
      "Epoch 8/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 102ms/step - loss: 0.3430 - val_loss: 0.3298\n",
      "Epoch 9/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 103ms/step - loss: 0.3398 - val_loss: 0.3243\n",
      "Epoch 10/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 100ms/step - loss: 0.3348 - val_loss: 0.3091\n",
      "Epoch 11/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 102ms/step - loss: 0.3283 - val_loss: 0.3303\n",
      "Epoch 12/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 103ms/step - loss: 0.3269 - val_loss: 0.2958\n",
      "Epoch 13/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 102ms/step - loss: 0.3236 - val_loss: 0.3101\n",
      "Epoch 14/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 102ms/step - loss: 0.3286 - val_loss: 0.3109\n",
      "Epoch 15/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 102ms/step - loss: 0.3267 - val_loss: 0.2994\n",
      "Epoch 16/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 101ms/step - loss: 0.3146 - val_loss: 0.2999\n",
      "Epoch 17/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 103ms/step - loss: 0.3175 - val_loss: 0.3046\n",
      "Epoch 18/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 104ms/step - loss: 0.3225 - val_loss: 0.2924\n",
      "Epoch 19/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 100ms/step - loss: 0.3155 - val_loss: 0.2809\n",
      "Epoch 20/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 100ms/step - loss: 0.3058 - val_loss: 0.3153\n",
      "Epoch 21/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 102ms/step - loss: 0.3036 - val_loss: 0.3008\n",
      "Epoch 22/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 100ms/step - loss: 0.2977 - val_loss: 0.2935\n",
      "Epoch 23/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 101ms/step - loss: 0.2964 - val_loss: 0.2700\n",
      "Epoch 24/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 100ms/step - loss: 0.2909 - val_loss: 0.2783\n",
      "Epoch 25/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 100ms/step - loss: 0.2933 - val_loss: 0.2720\n",
      "Epoch 26/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 100ms/step - loss: 0.2849 - val_loss: 0.2622\n",
      "Epoch 27/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 102ms/step - loss: 0.2859 - val_loss: 0.2728\n",
      "Epoch 28/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 99ms/step - loss: 0.2800 - val_loss: 0.2622\n",
      "Epoch 29/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 100ms/step - loss: 0.2806 - val_loss: 0.2584\n",
      "Epoch 30/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 101ms/step - loss: 0.2779 - val_loss: 0.2543\n",
      "Epoch 31/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 100ms/step - loss: 0.2768 - val_loss: 0.2474\n",
      "Epoch 32/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 104ms/step - loss: 0.2760 - val_loss: 0.2442\n",
      "Epoch 33/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 101ms/step - loss: 0.2719 - val_loss: 0.2550\n",
      "Epoch 34/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 100ms/step - loss: 0.2792 - val_loss: 0.2349\n",
      "Epoch 35/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 102ms/step - loss: 0.2754 - val_loss: 0.2540\n",
      "Epoch 36/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 101ms/step - loss: 0.2721 - val_loss: 0.2425\n",
      "Epoch 37/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 101ms/step - loss: 0.2719 - val_loss: 0.2556\n",
      "Epoch 38/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 99ms/step - loss: 0.2764 - val_loss: 0.2461\n",
      "Epoch 39/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 100ms/step - loss: 0.2701 - val_loss: 0.2485\n",
      "Epoch 40/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 100ms/step - loss: 0.2675 - val_loss: 0.2405\n",
      "Epoch 41/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 100ms/step - loss: 0.2624 - val_loss: 0.2530\n",
      "Epoch 42/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 100ms/step - loss: 0.2628 - val_loss: 0.2380\n",
      "Epoch 43/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 100ms/step - loss: 0.2613 - val_loss: 0.2355\n",
      "Epoch 44/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 101ms/step - loss: 0.2622 - val_loss: 0.2287\n",
      "Epoch 45/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 100ms/step - loss: 0.2629 - val_loss: 0.2260\n",
      "Epoch 46/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 101ms/step - loss: 0.2610 - val_loss: 0.2378\n",
      "Epoch 47/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 101ms/step - loss: 0.2586 - val_loss: 0.2464\n",
      "Epoch 48/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 101ms/step - loss: 0.2599 - val_loss: 0.2286\n",
      "Epoch 49/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 101ms/step - loss: 0.2615 - val_loss: 0.2358\n",
      "Epoch 50/50\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 101ms/step - loss: 0.2627 - val_loss: 0.2385\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 40ms/step\n",
      "Fold 4, Validation Loss: 0.23849272727966309, RMSE Max: 0.4835298569782912, RMSE Min: 0.4931373716586894\n",
      "Epoch 1/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 96ms/step - loss: 0.9166 - val_loss: 0.5005\n",
      "Epoch 2/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 98ms/step - loss: 0.4200 - val_loss: 0.3863\n",
      "Epoch 3/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 99ms/step - loss: 0.3504 - val_loss: 0.3656\n",
      "Epoch 4/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 99ms/step - loss: 0.3390 - val_loss: 0.3908\n",
      "Epoch 5/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 100ms/step - loss: 0.3255 - val_loss: 0.3538\n",
      "Epoch 6/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 99ms/step - loss: 0.3086 - val_loss: 0.3445\n",
      "Epoch 7/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 100ms/step - loss: 0.3006 - val_loss: 0.3356\n",
      "Epoch 8/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 99ms/step - loss: 0.2941 - val_loss: 0.3336\n",
      "Epoch 9/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 102ms/step - loss: 0.2903 - val_loss: 0.3239\n",
      "Epoch 10/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 97ms/step - loss: 0.2860 - val_loss: 0.3348\n",
      "Epoch 11/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 100ms/step - loss: 0.2809 - val_loss: 0.3439\n",
      "Epoch 12/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 99ms/step - loss: 0.2830 - val_loss: 0.3118\n",
      "Epoch 13/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 101ms/step - loss: 0.2753 - val_loss: 0.3145\n",
      "Epoch 14/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 98ms/step - loss: 0.2763 - val_loss: 0.3170\n",
      "Epoch 15/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 100ms/step - loss: 0.2717 - val_loss: 0.2977\n",
      "Epoch 16/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 98ms/step - loss: 0.2763 - val_loss: 0.2929\n",
      "Epoch 17/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 98ms/step - loss: 0.2686 - val_loss: 0.2968\n",
      "Epoch 18/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 101ms/step - loss: 0.2696 - val_loss: 0.2901\n",
      "Epoch 19/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 99ms/step - loss: 0.2688 - val_loss: 0.2881\n",
      "Epoch 20/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 100ms/step - loss: 0.2685 - val_loss: 0.2825\n",
      "Epoch 21/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 100ms/step - loss: 0.2623 - val_loss: 0.2962\n",
      "Epoch 22/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 100ms/step - loss: 0.2579 - val_loss: 0.2751\n",
      "Epoch 23/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 100ms/step - loss: 0.2544 - val_loss: 0.2621\n",
      "Epoch 24/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 101ms/step - loss: 0.2517 - val_loss: 0.2723\n",
      "Epoch 25/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 99ms/step - loss: 0.2537 - val_loss: 0.2736\n",
      "Epoch 26/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 100ms/step - loss: 0.2503 - val_loss: 0.2659\n",
      "Epoch 27/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 99ms/step - loss: 0.2504 - val_loss: 0.2640\n",
      "Epoch 28/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 100ms/step - loss: 0.2470 - val_loss: 0.2561\n",
      "Epoch 29/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 98ms/step - loss: 0.2494 - val_loss: 0.2655\n",
      "Epoch 30/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 100ms/step - loss: 0.2449 - val_loss: 0.2571\n",
      "Epoch 31/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 100ms/step - loss: 0.2429 - val_loss: 0.2617\n",
      "Epoch 32/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 99ms/step - loss: 0.2455 - val_loss: 0.2566\n",
      "Epoch 33/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 100ms/step - loss: 0.2459 - val_loss: 0.2551\n",
      "Epoch 34/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 101ms/step - loss: 0.2385 - val_loss: 0.2580\n",
      "Epoch 35/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 100ms/step - loss: 0.2402 - val_loss: 0.2624\n",
      "Epoch 36/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 100ms/step - loss: 0.2396 - val_loss: 0.2571\n",
      "Epoch 37/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 100ms/step - loss: 0.2436 - val_loss: 0.2525\n",
      "Epoch 38/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 101ms/step - loss: 0.2405 - val_loss: 0.2513\n",
      "Epoch 39/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 100ms/step - loss: 0.2390 - val_loss: 0.2529\n",
      "Epoch 40/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 100ms/step - loss: 0.2361 - val_loss: 0.2555\n",
      "Epoch 41/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 101ms/step - loss: 0.2367 - val_loss: 0.2455\n",
      "Epoch 42/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 101ms/step - loss: 0.2337 - val_loss: 0.2510\n",
      "Epoch 43/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 99ms/step - loss: 0.2402 - val_loss: 0.2466\n",
      "Epoch 44/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 100ms/step - loss: 0.2355 - val_loss: 0.2424\n",
      "Epoch 45/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 102ms/step - loss: 0.2303 - val_loss: 0.2365\n",
      "Epoch 46/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 100ms/step - loss: 0.2339 - val_loss: 0.2428\n",
      "Epoch 47/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 99ms/step - loss: 0.2391 - val_loss: 0.2689\n",
      "Epoch 48/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 101ms/step - loss: 0.2361 - val_loss: 0.2626\n",
      "Epoch 49/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 100ms/step - loss: 0.2353 - val_loss: 0.2470\n",
      "Epoch 50/50\n",
      "\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 99ms/step - loss: 0.2367 - val_loss: 0.2443\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 40ms/step\n",
      "Fold 5, Validation Loss: 0.24432256817817688, RMSE Max: 0.4829385025441759, RMSE Min: 0.5053865057977823\n",
      "Average Validation Loss: 0.2835528552532196\n",
      "Average RMSE of max_temp: 0.5131339337164109\n",
      "Average RMSE of min_temp: 0.5488131127119763\n",
      "Average MSE of max_temp: 0.2641438938321732\n",
      "Average MSE of min_temp: 0.30296183967556395\n",
      "Average MAE of max_temp: 0.4081873994859396\n",
      "Average MAE of min_temp: 0.43914455456229157\n",
      "Average R² of max_temp: 0.7295714564769014\n",
      "Average R² of min_temp: 0.6867582905865643\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input, MultiHeadAttention, Dropout, GlobalAveragePooling1D, LayerNormalization, Add\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "def create_time_series(X, y, time_steps):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        Xs.append(X[i:(i + time_steps)])\n",
    "        ys.append(y[i + time_steps])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "# cross-val\n",
    "n_splits = 5\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "time_steps = 30\n",
    "output_steps = 1\n",
    "\n",
    "X = df_weather[['global_radiation', 'diff_temp', 'mean_temp']].values\n",
    "y = df_weather[['max_temp', 'min_temp']].values\n",
    "\n",
    "# The number of features and target variables\n",
    "num_features = X.shape[1]\n",
    "num_targets = y.shape[1]\n",
    "\n",
    "# create Transformer model\n",
    "def build_transformer_model(input_shape, head_size, num_heads, ff_dim, num_transformer_blocks, mlp_units, dropout=0, mlp_dropout=0):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = inputs\n",
    "    for _ in range(num_transformer_blocks):\n",
    "        # Multi-Head Attention\n",
    "        attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=head_size, dropout=dropout)(x, x)\n",
    "        attn_output = Dropout(dropout)(attn_output)\n",
    "        x = Add()([x, attn_output])\n",
    "        x = LayerNormalization(epsilon=1e-6)(x)\n",
    "        \n",
    "        # Feed Forward Network\n",
    "        ffn_output = Dense(ff_dim, activation=\"relu\")(x)\n",
    "        ffn_output = Dropout(dropout)(ffn_output)\n",
    "        ffn_output = Dense(input_shape[-1])(ffn_output)\n",
    "        x = Add()([x, ffn_output])\n",
    "        x = LayerNormalization(epsilon=1e-6)(x)\n",
    "\n",
    "    x = GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
    "    for dim in mlp_units:\n",
    "        x = Dense(dim, activation=\"relu\")(x)\n",
    "        x = Dropout(mlp_dropout)(x)\n",
    "    outputs = Dense(output_steps * num_targets)(x)\n",
    "    return Model(inputs, outputs)\n",
    "\n",
    "# Store indicators\n",
    "results = {\n",
    "    'val_loss': [],\n",
    "    'rmse_max': [],\n",
    "    'rmse_min': [],\n",
    "    'mse_max': [],\n",
    "    'mse_min': [],\n",
    "    'mae_max': [],\n",
    "    'mae_min': [],\n",
    "    'r2_max': [],\n",
    "    'r2_min': []\n",
    "}\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(tscv.split(X)):\n",
    "\n",
    "    X_train, X_val = X[train_index], X[val_index]\n",
    "    y_train, y_val = y[train_index], y[val_index]\n",
    "    \n",
    "\n",
    "    X_train_rnn, y_train_rnn = create_time_series(X_train, y_train, time_steps)\n",
    "    X_val_rnn, y_val_rnn = create_time_series(X_val, y_val, time_steps)\n",
    "\n",
    "    # Adjust the shape of the target variable\n",
    "    y_train_rnn = y_train_rnn.reshape(y_train_rnn.shape[0], -1)\n",
    "    y_val_rnn = y_val_rnn.reshape(y_val_rnn.shape[0], -1)\n",
    "    \n",
    "    model = build_transformer_model(\n",
    "        (time_steps, num_features),\n",
    "        head_size=256,\n",
    "        num_heads=4,\n",
    "        ff_dim=4,\n",
    "        num_transformer_blocks=4,\n",
    "        mlp_units=[128],\n",
    "        mlp_dropout=0.4,\n",
    "        dropout=0.25,\n",
    "    )\n",
    "\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=Adam(learning_rate=1e-4))\n",
    "    model.fit(X_train_rnn, y_train_rnn,\n",
    "              validation_data=(X_val_rnn, y_val_rnn),\n",
    "              epochs=50,\n",
    "              batch_size=32,\n",
    "              verbose=1)\n",
    "\n",
    "    val_loss = model.evaluate(X_val_rnn, y_val_rnn, verbose=0)\n",
    "    y_val_pred = model.predict(X_val_rnn)\n",
    "\n",
    "    mse_max = mean_squared_error(y_val_rnn[:, 0], y_val_pred[:, 0])\n",
    "    mse_min = mean_squared_error(y_val_rnn[:, 1], y_val_pred[:, 1])\n",
    "    rmse_max = np.sqrt(mse_max)\n",
    "    rmse_min = np.sqrt(mse_min)\n",
    "    mae_max = mean_absolute_error(y_val_rnn[:, 0], y_val_pred[:, 0])\n",
    "    mae_min = mean_absolute_error(y_val_rnn[:, 1], y_val_pred[:, 1])\n",
    "    r2_max = r2_score(y_val_rnn[:, 0], y_val_pred[:, 0])\n",
    "    r2_min = r2_score(y_val_rnn[:, 1], y_val_pred[:, 1])\n",
    "\n",
    "    results['val_loss'].append(val_loss)\n",
    "    results['rmse_max'].append(rmse_max)\n",
    "    results['rmse_min'].append(rmse_min)\n",
    "    results['mse_max'].append(mse_max)\n",
    "    results['mse_min'].append(mse_min)\n",
    "    results['mae_max'].append(mae_max)\n",
    "    results['mae_min'].append(mae_min)\n",
    "    results['r2_max'].append(r2_max)\n",
    "    results['r2_min'].append(r2_min)\n",
    "    \n",
    "    print(f'Fold {fold + 1}, Validation Loss: {val_loss}, RMSE Max: {rmse_max}, RMSE Min: {rmse_min}')\n",
    "\n",
    "print(f'Average Validation Loss: {np.mean(results[\"val_loss\"])}')\n",
    "print(f'Average RMSE of max_temp: {np.mean(results[\"rmse_max\"])}')\n",
    "print(f'Average RMSE of min_temp: {np.mean(results[\"rmse_min\"])}')\n",
    "print(f'Average MSE of max_temp: {np.mean(results[\"mse_max\"])}')\n",
    "print(f'Average MSE of min_temp: {np.mean(results[\"mse_min\"])}')\n",
    "print(f'Average MAE of max_temp: {np.mean(results[\"mae_max\"])}')\n",
    "print(f'Average MAE of min_temp: {np.mean(results[\"mae_min\"])}')\n",
    "print(f'Average R² of max_temp: {np.mean(results[\"r2_max\"])}')\n",
    "print(f'Average R² of min_temp: {np.mean(results[\"r2_min\"])}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
